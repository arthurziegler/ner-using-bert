{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##External\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer, WhitespaceTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from pprint import pprint\n",
    "import os\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Internal\n",
    "import data_cleaning.data_cleaning as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_token_df[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a dataset from the datasets library\n",
    "The easiest way to use our data with HuggingFace is to use the datasets library. It allows us to import our own data and it will format it into a Dataset Object that is ready to be used by the NER model.\n",
    "\n",
    "To understand how our data need to be formatted let's explore a sample dataset that already exists inside the datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c343c8dd40543efb600327be3a27ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fa8740ce3d48f68087110ff9463cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 to C:/Users/arthu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ed93b9ccc44d2982f1557cd52cbc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dfb085c42447718c0638db1c2d01dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a0772ca0a846b2a356c4b7dacc356c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb071bf44d349f2923c27f54650a4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to C:/Users/arthu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3be481500414523b41469838d9ac59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = load_dataset('conll2003')\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of DatasetDict, which we'll want to use, is a list of three arrow Datasets: train, test and validation. \n",
    "Each Dataset is composed of two main object: features and num_rows. We need to make sure our JSON has the features 'tokens' and 'ner_tags'\n",
    "The sample data uses the following dictionary to convert each label to an int:\n",
    "\n",
    "**{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}**\n",
    "\n",
    "Since we are using the exact same labels we can utilize this dictionary as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'3'\n",
      "['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.']\n",
      "[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]\n",
      "[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "pprint(sample_data['train']['id'][3])\n",
    "print(sample_data['train']['tokens'][3])\n",
    "print(sample_data['train']['pos_tags'][3])\n",
    "print(sample_data['train']['ner_tags'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 0, 7, 0, 0, 0, 7, 0, 0], [1, 2], [5, 0]]"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data['train']['ner_tags'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset into train and test\n",
    "We'll split our DataFrame into to lists of lists. One for the input tokens and another for the labels.\n",
    "After that we'll use the scklearn train_test_split method to get both our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide inputs and tokens into X and y lists.\n",
    "sample_X = []\n",
    "sample_y = []\n",
    "for sentence in list_of_token_df:\n",
    "    sample_X.append(list(sentence['token']))\n",
    "    sample_y.append(list(sentence['label']))\n",
    "\n",
    "# Split X and y into train and test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_X, sample_y, test_size=0.33, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_distribution(seq_labels):\n",
    "    label_count = {}\n",
    "    for label in target_labels:\n",
    "        label_count[label] = 0\n",
    "    for seq in seq_labels:\n",
    "        for target_id in seq:\n",
    "            label = id_to_label[target_id]\n",
    "            label_count[label] += 1\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 2495,\n",
       " 'B-PERSON': 52,\n",
       " 'I-PERSON': 138,\n",
       " 'B-ORGANIZATION': 68,\n",
       " 'I-ORGANIZATION': 233,\n",
       " 'B-LOCATION': 7,\n",
       " 'I-LOCATION': 4,\n",
       " 'B-MISCELLANEOUS': 78,\n",
       " 'I-MISCELLANEOUS': 132}"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1089,\n",
       " 'B-PERSON': 22,\n",
       " 'I-PERSON': 53,\n",
       " 'B-ORGANIZATION': 30,\n",
       " 'I-ORGANIZATION': 126,\n",
       " 'B-LOCATION': 2,\n",
       " 'I-LOCATION': 0,\n",
       " 'B-MISCELLANEOUS': 33,\n",
       " 'I-MISCELLANEOUS': 52}"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_distribution(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train and test dictionary\n",
    "train_data = {'inputs': X_train, 'targets': y_train}\n",
    "test_data = {'inputs': X_test, 'targets': y_test}\n",
    "\n",
    "#Convert dictionary into DataFrame\n",
    "#Needed as intermediary step because DataFrames support convertion into the json record format we need.\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "#Convert DataFrame into json\n",
    "train_json = train_df.to_json(orient='records')\n",
    "test_json = test_df.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save train and test jsons in the auto_data directory.\n",
    "#This directory will serve as the repository of our auto labeled data and we'll use it to import the data with the datasets library.\n",
    "import os\n",
    "os.path\n",
    "file_path = \"C:\\\\Users\\\\arthu\\\\Desktop\\\\ner-using-bert\\BERT_Experiment\\\\auto_data\\\\\"\n",
    "\n",
    "with open(file_path+'train.json', 'w') as outfile:\n",
    "    outfile.write(train_json)\n",
    "\n",
    "with open(file_path+'test.json', 'w') as outfile:\n",
    "    outfile.write(test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and validate\n",
    "We load the dataset that we saved previously. We'll use the load_dataset method from the datasets library, which will allow us to easily use hugging face models with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/arthu/.cache/huggingface/datasets/json/default-b4d3e01485de334b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d16d97e89134fb6bea177aab4b49d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfaa6d842ac4d078d15dd4013129d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa816cde17f4334948e1f1137af1658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5baeb266f34ba3bf56c25f96941f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/arthu/.cache/huggingface/datasets/json/default-b4d3e01485de334b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc0035bfa3641c89e84d67619ad25fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ufrgs_data = load_dataset('json', data_dir = file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we get a DatasetDict object with two Datasets, one for train and one for test.\n",
    "ufrgs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Documento', 'gerado', 'sob', 'autenticação', 'Nº', 'QON.500.984.BHA', ',', 'disponível', 'no', 'endereço', 'http', ':', '//www.ufrgs.br/autenticacao', '1/1', 'PORTARIA', 'Nº', '1184', 'de', '18/02/2016', 'O', 'PRÓ-REITOR', 'DE', 'GESTÃO', 'DE', 'PESSOAS', 'DA', 'UNIVERSIDADE', 'FEDERAL', 'DO', 'RIO', 'GRANDE', 'DO', 'SUL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', 'que', 'lhe', 'foram', 'conferidas', 'pela', 'Portaria', 'nº', '5469', ',', 'de', '04', 'de', 'outubro', 'de', '2012', ',', 'do', 'Magnífico', 'Reitor', ',', 'e', 'conforme', 'o', 'Laudo', 'Médico', 'n°37308', ',', 'RESOLVE', ':', 'Designar', ',', 'temporariamente', ',', 'nos', 'termos', 'da', 'Lei', 'nº', '8.112', ',', 'de', '11', 'de', 'dezembro', 'de', '1990', ',', 'com', 'redação', 'dada', 'pela', 'Lei', 'nº', '9.527', ',', 'de', '10', 'de', 'dezembro', 'de', '1997', ',', 'a', 'ocupante', 'do', 'cargo', 'de', 'PORTEIRO', ',', 'do', 'Quadro', 'de', 'Pessoal', 'desta', 'Universidade', ',', 'ELIANE', 'RICARDO', 'IRANCO', '(', 'Siape', ':', '0359359', ')', ',', 'para', 'substituir', 'SULAMAR', 'FIGUEIRA', 'MARCELINO', '(', 'Siape', ':', '0357487', ')', ',', 'Chefe', 'do', 'Setor', 'de', 'Infraestrutura', 'e', 'Patrimônio', 'da', 'Gerência', 'Administrativa', 'do', 'Instituto', 'de', 'Informática', ',', 'em', 'seu', 'afastamento', 'por', 'motivo', 'de', 'Laudo', 'Médico', 'do', 'titular', 'da', 'Função', ',', 'no', 'período', 'de', '18/01/2016', 'a', '10/02/2016', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 8, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(ufrgs_data['train'][12]['inputs'])\n",
    "print(ufrgs_data['train'][12]['targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "We create a tokenizer to convert our inputs into sub-word ids. We need to use a tokenizer that is compatible with the model we'll use.\n",
    "HuggingFace makes that easy through the AutoTokenizer, which allows us to specify which model will be used and it already makes sure that our tokenizer will work with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'neuralmind/bert-base-portuguese-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 19816, 310, 14928, 425, 16782, 874, 100, 5226, 12234, 119, 5047, 119, 15405, 22336, 119, 241, 18394, 117, 5656, 202, 14441, 14305, 131, 120, 120, 2702, 11740, 119, 169, 2527, 6891, 119, 235, 22282, 120, 16782, 232, 304, 22280, 205, 120, 205, 212, 8718, 5118, 21748, 22301, 100, 17263, 22336, 125, 542, 120, 16956, 120, 4284, 231, 11635, 22369, 118, 257, 18469, 15349, 22322, 10836, 278, 3341, 22321, 16484, 10836, 212, 3341, 19715, 4089, 250, 22301, 7281, 9846, 5054, 22308, 6392, 11836, 22309, 263, 12002, 5054, 9369, 15040, 257, 15749, 278, 5650, 22320, 7545, 15040, 200, 18199, 117, 202, 1700, 125, 675, 20215, 179, 2036, 506, 7940, 649, 412, 14120, 322, 100, 11365, 10852, 117, 125, 16720, 125, 1511, 125, 3470, 117, 171, 13128, 3313, 3501, 428, 117, 122, 4762, 146, 11706, 243, 21491, 149, 22359, 9330, 3708, 22330, 117, 257, 3341, 7918, 22339, 22309, 131, 15945, 159, 117, 12885, 117, 538, 3401, 180, 2502, 100, 1015, 119, 21950, 117, 125, 1433, 125, 1512, 125, 5737, 117, 170, 16517, 5180, 412, 2502, 100, 1117, 119, 10596, 22337, 117, 125, 1193, 125, 1512, 125, 6827, 117, 123, 3045, 175, 171, 2466, 125, 212, 8718, 16017, 15710, 22317, 117, 171, 12106, 157, 125, 10304, 22290, 1014, 1582, 117, 192, 22327, 5234, 22320, 22309, 257, 6162, 6765, 18504, 290, 5650, 22320, 5218, 113, 9278, 7904, 22279, 131, 19148, 22334, 21509, 22334, 22315, 114, 117, 221, 8281, 200, 18199, 8314, 6765, 5769, 22328, 22341, 18469, 5650, 18471, 18532, 9008, 7073, 22317, 113, 9278, 7904, 22279, 131, 19148, 21906, 8706, 22337, 114, 117, 11566, 171, 530, 428, 125, 5027, 124, 5706, 122, 12995, 180, 5685, 399, 5862, 8865, 171, 2900, 125, 5027, 19560, 1443, 117, 173, 347, 18529, 240, 5607, 125, 11706, 243, 21491, 171, 5621, 180, 11513, 182, 117, 202, 1254, 125, 542, 120, 13778, 120, 4284, 123, 1193, 120, 16956, 120, 4284, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(ufrgs_data['train'][12]['inputs'], is_split_into_words=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The tokenizer return looks like a dictionary but it is actually an object called BatchEncoding\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Docu',\n",
       " '##mento',\n",
       " 'gerado',\n",
       " 'sob',\n",
       " 'autent',\n",
       " '##icação',\n",
       " '[UNK]',\n",
       " 'Q',\n",
       " '##ON',\n",
       " '.',\n",
       " '500',\n",
       " '.',\n",
       " '98',\n",
       " '##4',\n",
       " '.',\n",
       " 'B',\n",
       " '##HA',\n",
       " ',',\n",
       " 'disponível',\n",
       " 'no',\n",
       " 'endereço',\n",
       " 'http',\n",
       " ':',\n",
       " '/',\n",
       " '/',\n",
       " 'w',\n",
       " '##ww',\n",
       " '.',\n",
       " 'u',\n",
       " '##fr',\n",
       " '##gs',\n",
       " '.',\n",
       " 'b',\n",
       " '##r',\n",
       " '/',\n",
       " 'autent',\n",
       " '##ica',\n",
       " '##ca',\n",
       " '##o',\n",
       " '1',\n",
       " '/',\n",
       " '1',\n",
       " 'P',\n",
       " '##OR',\n",
       " '##TA',\n",
       " '##RI',\n",
       " '##A',\n",
       " '[UNK]',\n",
       " '118',\n",
       " '##4',\n",
       " 'de',\n",
       " '18',\n",
       " '/',\n",
       " '02',\n",
       " '/',\n",
       " '2016',\n",
       " 'O',\n",
       " 'PR',\n",
       " '##Ó',\n",
       " '-',\n",
       " 'R',\n",
       " '##EI',\n",
       " '##TO',\n",
       " '##R',\n",
       " 'DE',\n",
       " 'G',\n",
       " '##ES',\n",
       " '##T',\n",
       " '##ÃO',\n",
       " 'DE',\n",
       " 'P',\n",
       " '##ES',\n",
       " '##SO',\n",
       " '##AS',\n",
       " 'D',\n",
       " '##A',\n",
       " 'UN',\n",
       " '##IV',\n",
       " '##ER',\n",
       " '##S',\n",
       " '##ID',\n",
       " '##AD',\n",
       " '##E',\n",
       " 'F',\n",
       " '##ED',\n",
       " '##ER',\n",
       " '##AL',\n",
       " 'DO',\n",
       " 'R',\n",
       " '##IO',\n",
       " 'G',\n",
       " '##RA',\n",
       " '##N',\n",
       " '##DE',\n",
       " 'DO',\n",
       " 'S',\n",
       " '##UL',\n",
       " ',',\n",
       " 'no',\n",
       " 'uso',\n",
       " 'de',\n",
       " 'suas',\n",
       " 'atribuições',\n",
       " 'que',\n",
       " 'lhe',\n",
       " 'foram',\n",
       " 'confer',\n",
       " '##idas',\n",
       " 'pela',\n",
       " 'Porta',\n",
       " '##ria',\n",
       " '[UNK]',\n",
       " '54',\n",
       " '##69',\n",
       " ',',\n",
       " 'de',\n",
       " '04',\n",
       " 'de',\n",
       " 'outubro',\n",
       " 'de',\n",
       " '2012',\n",
       " ',',\n",
       " 'do',\n",
       " 'Magn',\n",
       " '##ífico',\n",
       " 'Rei',\n",
       " '##tor',\n",
       " ',',\n",
       " 'e',\n",
       " 'conforme',\n",
       " 'o',\n",
       " 'Lau',\n",
       " '##do',\n",
       " 'Médico',\n",
       " 'n',\n",
       " '##°',\n",
       " '##37',\n",
       " '##30',\n",
       " '##8',\n",
       " ',',\n",
       " 'R',\n",
       " '##ES',\n",
       " '##OL',\n",
       " '##V',\n",
       " '##E',\n",
       " ':',\n",
       " 'Design',\n",
       " '##ar',\n",
       " ',',\n",
       " 'temporariamente',\n",
       " ',',\n",
       " 'nos',\n",
       " 'termos',\n",
       " 'da',\n",
       " 'Lei',\n",
       " '[UNK]',\n",
       " '8',\n",
       " '.',\n",
       " '112',\n",
       " ',',\n",
       " 'de',\n",
       " '11',\n",
       " 'de',\n",
       " 'dezembro',\n",
       " 'de',\n",
       " '1990',\n",
       " ',',\n",
       " 'com',\n",
       " 'redação',\n",
       " 'dada',\n",
       " 'pela',\n",
       " 'Lei',\n",
       " '[UNK]',\n",
       " '9',\n",
       " '.',\n",
       " '52',\n",
       " '##7',\n",
       " ',',\n",
       " 'de',\n",
       " '10',\n",
       " 'de',\n",
       " 'dezembro',\n",
       " 'de',\n",
       " '1997',\n",
       " ',',\n",
       " 'a',\n",
       " 'ocupa',\n",
       " '##nte',\n",
       " 'do',\n",
       " 'cargo',\n",
       " 'de',\n",
       " 'P',\n",
       " '##OR',\n",
       " '##TE',\n",
       " '##IR',\n",
       " '##O',\n",
       " ',',\n",
       " 'do',\n",
       " 'Quad',\n",
       " '##ro',\n",
       " 'de',\n",
       " 'Pessoa',\n",
       " '##l',\n",
       " 'desta',\n",
       " 'Universidade',\n",
       " ',',\n",
       " 'E',\n",
       " '##L',\n",
       " '##IA',\n",
       " '##N',\n",
       " '##E',\n",
       " 'R',\n",
       " '##IC',\n",
       " '##AR',\n",
       " '##DO',\n",
       " 'I',\n",
       " '##RA',\n",
       " '##N',\n",
       " '##CO',\n",
       " '(',\n",
       " 'Si',\n",
       " '##ap',\n",
       " '##e',\n",
       " ':',\n",
       " '03',\n",
       " '##5',\n",
       " '##93',\n",
       " '##5',\n",
       " '##9',\n",
       " ')',\n",
       " ',',\n",
       " 'para',\n",
       " 'substituir',\n",
       " 'S',\n",
       " '##UL',\n",
       " '##AM',\n",
       " '##AR',\n",
       " 'FI',\n",
       " '##G',\n",
       " '##U',\n",
       " '##EI',\n",
       " '##RA',\n",
       " 'MA',\n",
       " '##RC',\n",
       " '##EL',\n",
       " '##IN',\n",
       " '##O',\n",
       " '(',\n",
       " 'Si',\n",
       " '##ap',\n",
       " '##e',\n",
       " ':',\n",
       " '03',\n",
       " '##57',\n",
       " '##48',\n",
       " '##7',\n",
       " ')',\n",
       " ',',\n",
       " 'Chefe',\n",
       " 'do',\n",
       " 'Se',\n",
       " '##tor',\n",
       " 'de',\n",
       " 'Inf',\n",
       " '##ra',\n",
       " '##estrutura',\n",
       " 'e',\n",
       " 'Patrimônio',\n",
       " 'da',\n",
       " 'Ger',\n",
       " '##ência',\n",
       " 'Administ',\n",
       " '##rativa',\n",
       " 'do',\n",
       " 'Instituto',\n",
       " 'de',\n",
       " 'Inf',\n",
       " '##orm',\n",
       " '##ática',\n",
       " ',',\n",
       " 'em',\n",
       " 'seu',\n",
       " 'afastamento',\n",
       " 'por',\n",
       " 'motivo',\n",
       " 'de',\n",
       " 'Lau',\n",
       " '##do',\n",
       " 'Médico',\n",
       " 'do',\n",
       " 'titular',\n",
       " 'da',\n",
       " 'Fun',\n",
       " '##ção',\n",
       " ',',\n",
       " 'no',\n",
       " 'período',\n",
       " 'de',\n",
       " '18',\n",
       " '/',\n",
       " '01',\n",
       " '/',\n",
       " '2016',\n",
       " 'a',\n",
       " '10',\n",
       " '/',\n",
       " '02',\n",
       " '/',\n",
       " '2016',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The object has a tokens method that returns the original tokens before transforming them into integers\n",
    "t.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 56,\n",
       " 57,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 143,\n",
       " 144,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 169,\n",
       " None]"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The word_ids methods returns a list with the ID that maps each sub-word to the original word it was tokenized from.\n",
    "t.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Alignment\n",
    "Now that our input is composed of sub-words, we need to make sure that we have one target per sub-word. To do this we will use the align_targets function and map targets from each word to its sub-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define relationship between B and I tags\n",
    "#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "begin2inside = {\n",
    "    1:2,\n",
    "    3:4,\n",
    "    5:6,\n",
    "    7:8\n",
    "}\n",
    "\n",
    "# Function that aligns the labels to be correctly associated with each sub-word.\n",
    "def align_targets(labels, word_ids):\n",
    "    aligned_labels = []\n",
    "    previous_word = None\n",
    "\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            # Tokens like [CLS] and [SEP]\n",
    "            label = -100 #This value is used by Hugging Face to ignore the tokens during training\n",
    "        elif word != previous_word:\n",
    "            # New word in the list\n",
    "            label = labels[word]\n",
    "        else:\n",
    "            #Repeated word (Would be the next sub-word)\n",
    "            if labels[word] in begin2inside:\n",
    "                #Change B- to I-\n",
    "                label = begin2inside[labels[word]]\n",
    "            else:\n",
    "                # Sub-word of a word classified as \"O\" gets the same label \"O\"\n",
    "                label = labels[word]\n",
    "\n",
    "        aligned_labels.append(label)\n",
    "        previous_word = word #update last word\n",
    " \n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Data: {'input_ids': [101, 205, 120, 205, 212, 8718, 5118, 21748, 22301, 100, 8103, 5752, 125, 16720, 120, 16899, 120, 5096, 177, 5427, 9617, 118, 257, 18469, 15349, 5650, 250, 22301, 7281, 9846, 5054, 22308, 6392, 11836, 22309, 263, 12002, 5054, 9369, 15040, 257, 15749, 278, 5650, 22320, 7545, 15040, 200, 18199, 117, 202, 1700, 125, 675, 20215, 117, 9319, 146, 16620, 229, 14120, 322, 100, 16444, 8510, 117, 125, 2939, 125, 1544, 125, 4284, 257, 3341, 7918, 22339, 22309, 3928, 15802, 6939, 150, 8346, 117, 240, 7887, 125, 4576, 117, 202, 12106, 157, 1014, 1582, 117, 320, 8922, 299, 6072, 22309, 17715, 22322, 22055, 22308, 278, 18178, 3341, 15040, 22308, 16288, 22333, 6072, 117, 12600, 14188, 200, 5234, 8214, 149, 22359, 22222, 4649, 22330, 22330, 117, 1340, 487, 122, 173, 8750, 202, 5985, 125, 8153, 1262, 171, 2900, 125, 9682, 122, 5503, 7023, 138, 117, 180, 3383, 250, 125, 8922, 8688, 243, 117, 2270, 19148, 117, 221, 123, 3383, 250, 125, 8922, 8688, 243, 117, 2270, 16720, 117, 15762, 320, 558, 7485, 826, 125, 542, 120, 1242, 120, 3618, 123, 1040, 120, 1242, 120, 4155, 117, 170, 3459, 399, 7500, 123, 1018, 125, 13778, 120, 16394, 120, 4284, 117, 125, 1365, 170, 146, 179, 13058, 123, 2502, 100, 1242, 119, 14332, 22313, 117, 125, 2593, 125, 1512, 125, 3470, 117, 170, 675, 6860, 122, 123, 4534, 758, 100, 889, 120, 3847, 118, 16273, 22308, 11964, 117, 16072, 412, 4534, 758, 100, 3467, 22302, 120, 3757, 118, 16273, 22308, 11964, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Word Tokens: ['1/1', 'PORTARIA', 'Nº', '3812', 'de', '04/05/2017', 'A', 'VICE-REITORA', 'DA', 'UNIVERSIDADE', 'FEDERAL', 'DO', 'RIO', 'GRANDE', 'DO', 'SUL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', ',', 'considerando', 'o', 'disposto', 'na', 'Portaria', 'nº', '7624', ',', 'de', '29', 'de', 'setembro', 'de', '2016', 'RESOLVE', 'Conceder', 'progressão', 'funcional', ',', 'por', 'avaliação', 'de', 'desempenho', ',', 'no', 'Quadro', 'desta', 'Universidade', ',', 'ao', 'Professor', 'JOSE', 'CARLOS', 'GOMES', 'DOS', 'ANJOS', ',', 'matrícula', 'SIAPE', 'n°', '1296088', ',', 'lotado', 'e', 'em', 'exercício', 'no', 'Departamento', 'de', 'Sociologia', 'do', 'Instituto', 'de', 'Filosofia', 'e', 'Ciências', 'Humanas', ',', 'da', 'classe', 'D', 'de', 'Professor', 'Associado', ',', 'nível', '03', ',', 'para', 'a', 'classe', 'D', 'de', 'Professor', 'Associado', ',', 'nível', '04', ',', 'referente', 'ao', 'interstício', 'de', '18/12/2011', 'a', '17/12/2015', ',', 'com', 'vigência', 'financeira', 'a', 'partir', 'de', '01/08/2016', ',', 'de', 'acordo', 'com', 'o', 'que', 'dispõe', 'a', 'Lei', 'nº', '12.772', ',', 'de', '28', 'de', 'dezembro', 'de', '2012', ',', 'com', 'suas', 'alterações', 'e', 'a', 'Decisão', 'nº', '197/2006-CONSUN', ',', 'alterada', 'pela', 'Decisão', 'nº', '401/2013-CONSUN', '.']\n",
      "Word Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 3, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Word IDs: [None, 0, 0, 0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 11, 12, 12, 13, 13, 13, 13, 14, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 27, 28, 29, 29, 30, 31, 32, 33, 34, 35, 36, 37, 37, 37, 37, 37, 38, 38, 39, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 48, 49, 50, 51, 52, 53, 54, 54, 54, 55, 55, 55, 55, 56, 56, 56, 57, 57, 58, 58, 58, 59, 60, 60, 61, 61, 61, 62, 62, 63, 63, 63, 63, 64, 65, 65, 66, 67, 68, 69, 70, 71, 72, 72, 73, 74, 75, 76, 77, 78, 79, 79, 80, 81, 82, 83, 84, 85, 86, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 97, 98, 99, 100, 101, 102, 103, 104, 104, 104, 105, 106, 106, 106, 106, 106, 107, 108, 108, 108, 108, 108, 109, 110, 111, 111, 112, 113, 114, 115, 116, 116, 116, 116, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 127, 127, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 141, 142, 143, 143, 143, 143, 143, 143, 143, 144, 145, 146, 147, 147, 148, 149, 149, 149, 149, 149, 149, 149, 149, 150, None]\n",
      "Sub-Word Labels: [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "Sub-Word Tokens: ['[CLS]', '1', '/', '1', 'P', '##OR', '##TA', '##RI', '##A', '[UNK]', '38', '##12', 'de', '04', '/', '05', '/', '2017', 'A', 'VI', '##CE', '-', 'R', '##EI', '##TO', '##RA', 'D', '##A', 'UN', '##IV', '##ER', '##S', '##ID', '##AD', '##E', 'F', '##ED', '##ER', '##AL', 'DO', 'R', '##IO', 'G', '##RA', '##N', '##DE', 'DO', 'S', '##UL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', ',', 'considerando', 'o', 'disposto', 'na', 'Porta', '##ria', '[UNK]', '76', '##24', ',', 'de', '29', 'de', 'setembro', 'de', '2016', 'R', '##ES', '##OL', '##V', '##E', 'Conc', '##eder', 'progress', '##ão', 'funcional', ',', 'por', 'avaliação', 'de', 'desempenho', ',', 'no', 'Quad', '##ro', 'desta', 'Universidade', ',', 'ao', 'Professor', 'J', '##OS', '##E', 'CA', '##R', '##LO', '##S', 'G', '##OM', '##ES', 'DO', '##S', 'AN', '##J', '##OS', ',', 'matr', '##ícula', 'S', '##IA', '##PE', 'n', '##°', '129', '##60', '##8', '##8', ',', 'lo', '##tado', 'e', 'em', 'exercício', 'no', 'Departamento', 'de', 'Soci', '##ologia', 'do', 'Instituto', 'de', 'Filosofia', 'e', 'Ciências', 'Human', '##as', ',', 'da', 'classe', 'D', 'de', 'Professor', 'Associa', '##do', ',', 'nível', '03', ',', 'para', 'a', 'classe', 'D', 'de', 'Professor', 'Associa', '##do', ',', 'nível', '04', ',', 'referente', 'ao', 'inter', '##st', '##ício', 'de', '18', '/', '12', '/', '2011', 'a', '17', '/', '12', '/', '2015', ',', 'com', 'vig', '##ência', 'financeira', 'a', 'partir', 'de', '01', '/', '08', '/', '2016', ',', 'de', 'acordo', 'com', 'o', 'que', 'dispõe', 'a', 'Lei', '[UNK]', '12', '.', '77', '##2', ',', 'de', '28', 'de', 'dezembro', 'de', '2012', ',', 'com', 'suas', 'alterações', 'e', 'a', 'Dec', '##isão', '[UNK]', '197', '/', '2006', '-', 'CON', '##S', '##UN', ',', 'alterada', 'pela', 'Dec', '##isão', '[UNK]', '40', '##1', '/', '2013', '-', 'CON', '##S', '##UN', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "## Label-Token Alignment Test\n",
    "idx = 6\n",
    "test_data = tokenizer(ufrgs_data['train'][idx]['inputs'], is_split_into_words=True)\n",
    "print(\"Tokenized Data:\", test_data)\n",
    "print(\"Word Tokens:\", ufrgs_data['train'][idx]['inputs'])\n",
    "test_labels = ufrgs_data['train'][idx]['targets']\n",
    "print(\"Word Labels:\", test_labels)\n",
    "print(\"Word IDs:\", test_data.word_ids())\n",
    "aligned_targets = align_targets(test_labels, test_data.word_ids())\n",
    "print(\"Sub-Word Labels:\", aligned_targets)\n",
    "print(\"Sub-Word Tokens:\", test_data.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "1\tO\n",
      "/\tO\n",
      "1\tO\n",
      "P\tO\n",
      "##OR\tO\n",
      "##TA\tO\n",
      "##RI\tO\n",
      "##A\tO\n",
      "[UNK]\tO\n",
      "38\tO\n",
      "##12\tO\n",
      "de\tO\n",
      "04\tO\n",
      "/\tO\n",
      "05\tO\n",
      "/\tO\n",
      "2017\tO\n",
      "A\tO\n",
      "VI\tO\n",
      "##CE\tO\n",
      "-\tO\n",
      "R\tO\n",
      "##EI\tO\n",
      "##TO\tO\n",
      "##RA\tO\n",
      "D\tO\n",
      "##A\tO\n",
      "UN\tB-ORGANIZATION\n",
      "##IV\tI-ORGANIZATION\n",
      "##ER\tI-ORGANIZATION\n",
      "##S\tI-ORGANIZATION\n",
      "##ID\tI-ORGANIZATION\n",
      "##AD\tI-ORGANIZATION\n",
      "##E\tI-ORGANIZATION\n",
      "F\tI-ORGANIZATION\n",
      "##ED\tI-ORGANIZATION\n",
      "##ER\tI-ORGANIZATION\n",
      "##AL\tI-ORGANIZATION\n",
      "DO\tI-ORGANIZATION\n",
      "R\tI-ORGANIZATION\n",
      "##IO\tI-ORGANIZATION\n",
      "G\tI-ORGANIZATION\n",
      "##RA\tI-ORGANIZATION\n",
      "##N\tI-ORGANIZATION\n",
      "##DE\tI-ORGANIZATION\n",
      "DO\tI-ORGANIZATION\n",
      "S\tI-ORGANIZATION\n",
      "##UL\tI-ORGANIZATION\n",
      ",\tO\n",
      "no\tO\n",
      "uso\tO\n",
      "de\tO\n",
      "suas\tO\n",
      "atribuições\tO\n",
      ",\tO\n",
      "considerando\tO\n",
      "o\tO\n",
      "disposto\tO\n",
      "na\tO\n",
      "Porta\tO\n",
      "##ria\tO\n",
      "[UNK]\tO\n",
      "76\tO\n",
      "##24\tO\n",
      ",\tO\n",
      "de\tO\n",
      "29\tO\n",
      "de\tO\n",
      "setembro\tO\n",
      "de\tO\n",
      "2016\tO\n",
      "R\tO\n",
      "##ES\tO\n",
      "##OL\tO\n",
      "##V\tO\n",
      "##E\tO\n",
      "Conc\tO\n",
      "##eder\tO\n",
      "progress\tO\n",
      "##ão\tO\n",
      "funcional\tO\n",
      ",\tO\n",
      "por\tO\n",
      "avaliação\tO\n",
      "de\tO\n",
      "desempenho\tO\n",
      ",\tO\n",
      "no\tO\n",
      "Quad\tO\n",
      "##ro\tO\n",
      "desta\tO\n",
      "Universidade\tO\n",
      ",\tO\n",
      "ao\tO\n",
      "Professor\tB-MISCELLANEOUS\n",
      "J\tB-PERSON\n",
      "##OS\tI-PERSON\n",
      "##E\tI-PERSON\n",
      "CA\tI-PERSON\n",
      "##R\tI-PERSON\n",
      "##LO\tI-PERSON\n",
      "##S\tI-PERSON\n",
      "G\tI-PERSON\n",
      "##OM\tI-PERSON\n",
      "##ES\tI-PERSON\n",
      "DO\tI-PERSON\n",
      "##S\tI-PERSON\n",
      "AN\tI-PERSON\n",
      "##J\tI-PERSON\n",
      "##OS\tI-PERSON\n",
      ",\tO\n",
      "matr\tO\n",
      "##ícula\tO\n",
      "S\tO\n",
      "##IA\tO\n",
      "##PE\tO\n",
      "n\tO\n",
      "##°\tO\n",
      "129\tO\n",
      "##60\tO\n",
      "##8\tO\n",
      "##8\tO\n",
      ",\tO\n",
      "lo\tO\n",
      "##tado\tO\n",
      "e\tO\n",
      "em\tO\n",
      "exercício\tO\n",
      "no\tO\n",
      "Departamento\tB-ORGANIZATION\n",
      "de\tI-ORGANIZATION\n",
      "Soci\tI-ORGANIZATION\n",
      "##ologia\tI-ORGANIZATION\n",
      "do\tO\n",
      "Instituto\tB-ORGANIZATION\n",
      "de\tI-ORGANIZATION\n",
      "Filosofia\tI-ORGANIZATION\n",
      "e\tI-ORGANIZATION\n",
      "Ciências\tI-ORGANIZATION\n",
      "Human\tI-ORGANIZATION\n",
      "##as\tI-ORGANIZATION\n",
      ",\tO\n",
      "da\tO\n",
      "classe\tO\n",
      "D\tO\n",
      "de\tO\n",
      "Professor\tB-MISCELLANEOUS\n",
      "Associa\tI-MISCELLANEOUS\n",
      "##do\tI-MISCELLANEOUS\n",
      ",\tO\n",
      "nível\tO\n",
      "03\tO\n",
      ",\tO\n",
      "para\tO\n",
      "a\tO\n",
      "classe\tO\n",
      "D\tO\n",
      "de\tO\n",
      "Professor\tB-MISCELLANEOUS\n",
      "Associa\tI-MISCELLANEOUS\n",
      "##do\tI-MISCELLANEOUS\n",
      ",\tO\n",
      "nível\tO\n",
      "04\tO\n",
      ",\tO\n",
      "referente\tO\n",
      "ao\tO\n",
      "inter\tO\n",
      "##st\tO\n",
      "##ício\tO\n",
      "de\tO\n",
      "18\tO\n",
      "/\tO\n",
      "12\tO\n",
      "/\tO\n",
      "2011\tO\n",
      "a\tO\n",
      "17\tO\n",
      "/\tO\n",
      "12\tO\n",
      "/\tO\n",
      "2015\tO\n",
      ",\tO\n",
      "com\tO\n",
      "vig\tO\n",
      "##ência\tO\n",
      "financeira\tO\n",
      "a\tO\n",
      "partir\tO\n",
      "de\tO\n",
      "01\tO\n",
      "/\tO\n",
      "08\tO\n",
      "/\tO\n",
      "2016\tO\n",
      ",\tO\n",
      "de\tO\n",
      "acordo\tO\n",
      "com\tO\n",
      "o\tO\n",
      "que\tO\n",
      "dispõe\tO\n",
      "a\tO\n",
      "Lei\tO\n",
      "[UNK]\tO\n",
      "12\tO\n",
      ".\tO\n",
      "77\tO\n",
      "##2\tO\n",
      ",\tO\n",
      "de\tO\n",
      "28\tO\n",
      "de\tO\n",
      "dezembro\tO\n",
      "de\tO\n",
      "2012\tO\n",
      ",\tO\n",
      "com\tO\n",
      "suas\tO\n",
      "alterações\tO\n",
      "e\tO\n",
      "a\tO\n",
      "Dec\tO\n",
      "##isão\tO\n",
      "[UNK]\tO\n",
      "197\tO\n",
      "/\tO\n",
      "2006\tO\n",
      "-\tO\n",
      "CON\tO\n",
      "##S\tO\n",
      "##UN\tO\n",
      ",\tO\n",
      "alterada\tO\n",
      "pela\tO\n",
      "Dec\tO\n",
      "##isão\tO\n",
      "[UNK]\tO\n",
      "40\tO\n",
      "##1\tO\n",
      "/\tO\n",
      "2013\tO\n",
      "-\tO\n",
      "CON\tO\n",
      "##S\tO\n",
      "##UN\tO\n",
      ".\tO\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = [target_labels[t] if t>=0 else None for t in aligned_targets]\n",
    "for x, y in zip(test_data.tokens(), aligned_labels):\n",
    "    print(f\"{x}\\t{y}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize inputs\n",
    "We take the sub-word inputs and labels and pass it to the 'tokenize_fn' function to generate the tokens we'll feed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize both inputs and targets\n",
    "def tokenize_fn(batch):\n",
    "    # Tokenize the input sequence first\n",
    "    tokenized_inputs = tokenizer(batch['inputs'], truncation=True, is_split_into_words=True)\n",
    "    labels_batch = batch['targets'] # The original targets word-by-word\n",
    "    aligned_labels_batch = [] # The aligned targets sub-word by sub-word\n",
    "    # Loop through each label sequence in the batch\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_ids = tokenized_inputs.word_ids(i) # Get word IDs for the sequence\n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids)) # Align sequence labels\n",
    "    \n",
    "    # Save final aligned labels in a column called 'labels' which is the required name for the hugging face models\n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'targets'],\n",
       "    num_rows: 38\n",
       "})"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufrgs_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cd81f375644bea9e3032cf2067d0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea9db79ded44a40864d72ea905c9f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the datasets method 'map' to apply the tokenize function to the train and test datasets.\n",
    "# We'll use the batched parameter to improve the eficiency of the tokenization.\n",
    "tokenized_datasets = ufrgs_data.map(\n",
    "\ttokenize_fn,\n",
    "\tbatched=True,\n",
    "\tremove_columns=ufrgs_data[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the DataCollator\n",
    "There are several required steps before passing the text into the model: padding, truncate, converting to tensors, etc. When we use the tokenizer method we are not doing most of these steps because the ‘data collator’ in the trainer is taking care of it implicitly when we train the model.\n",
    "\n",
    "The Data Collator is built into the trainer and is defined as such:\n",
    "\n",
    "> • **data_collator** (`DataCollator`, *optional*) — The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will default to **[default_data_collator()](https://huggingface.co/docs/transformers/v4.22.1/en/main_classes/data_collator#transformers.default_data_collator)** if no `tokenizer` is provided, an instance of **[DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.22.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding)** otherwise.\n",
    "> \n",
    "\n",
    "#### For Token Classification\n",
    "\n",
    "The Trainer object in Hugging Face is not capable of recognizing which task we are trying to execute and therefore is not able to automatically select the correct Data Collator for us. Since the default DataCollatorWithPadding does not support tasks of the token classification type, we’ll need to manually define the one we want by importing the token classification data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    3,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    7,    8,    8,    8,    8,\n",
       "            8,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    3,    4,    4,    4,    4,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    3,    4,    4,    4,    0,    5,\n",
       "            6,    6,    6,    6,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0, -100],\n",
       "        [-100,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    7,    8,    8,    8,    8,    8,    8,    8,    8,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the data collator. It should return both inputs as tensor of the same size (including the padding).\n",
    "collator_testset = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = data_collator(collator_testset)\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for validation\n",
    "Computing metrics isn’t as straight forward when we deal with multiple targets per sample(sentence). Usually when calculating accuracy with a single target we can do #correct_samples/#total_samples. This is not possible when each sample has several targets.\n",
    "One solution would be flattening all predictions to calculate #correct_targets/#total_targets.\n",
    "#### Seqeval\n",
    "This library is the standard method to calculate metrics in hugging face as its sole purpose is to compute metrics for NLP tasks with sequence targets.\\\n",
    "https://huggingface.co/spaces/evaluate-metric/seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seqeval will not work with a single input single label task. It will only be usable in tasks that require mulitple labels for multiple inputs\n",
    "# Single input single label example:\n",
    "#metric.compute(predictions=[0, 0, 0], references=[0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.8333333333333334}"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multilple inputs multiple labels example:\n",
    "metric.compute(\n",
    "    predictions=[[0, 0, 0], [1, 0, 1]], \n",
    "    references=[[0, 0, 1], [1, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 0.14285714285714285,\n",
       " 'overall_recall': 0.25,\n",
       " 'overall_f1': 0.18181818181818182,\n",
       " 'overall_accuracy': 0.5}"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seqeval supports sequence labeling evalutation with the IOB formats. So to get rid of the warning and correctly compute the metrics we need to follow this formatting standard.\n",
    "metric.compute(\n",
    "    predictions=[['O', 'I-ORG', 'B-ORG', 'B-ORG', 'B-LOC'], ['B-MISC', 'O', 'B-PER', 'I-PER', \"I-MISC\"]], \n",
    "    references=[['O', 'B-LOC', 'B-ORG', 'I-ORG', 'I-ORG'], ['B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'O']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # remove -100, convert the label ids to label names\n",
    "    str_labels = [[target_labels[t] for t in label if t != -100] for label in labels]\n",
    "\n",
    "    # do the same for predictions whenever true label is -100\n",
    "    str_preds = [[target_labels[p] for p, t in zip(pred, targ) if t != -100] for pred, targ in zip(preds, labels)]\n",
    "\n",
    "    the_metrics = metric.compute(predictions=str_preds, references=str_labels)\n",
    "    return {\n",
    "        'precision': the_metrics['overall_precision'],\n",
    "        'recall': the_metrics['overall_recall'],\n",
    "        'f1': the_metrics['overall_f1'],\n",
    "        'accuracy': the_metrics['overall_accuracy']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PERSON',\n",
       " 2: 'I-PERSON',\n",
       " 3: 'B-ORGANIZATION',\n",
       " 4: 'I-ORGANIZATION',\n",
       " 5: 'B-LOCATION',\n",
       " 6: 'I-LOCATION',\n",
       " 7: 'B-MISCELLANEOUS',\n",
       " 8: 'I-MISCELLANEOUS'}"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {k: v for k, v in enumerate(target_labels)} #Get label IDs\n",
    "label2id = {v: k for k, v in id2label.items()} #Get label names from IDs\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pre-trained model\n",
    "\n",
    "We use the AutoModelForTokenClassification.from_pretrained method to load the BERT model from huggingface.\\\n",
    "The model loades will be the one defined by 'checkpoint'.\\\n",
    "We pass the id2label and label2id parameters for the model to understand the targets we are using for the prediction and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PERSON\",\n",
      "    \"2\": \"I-PERSON\",\n",
      "    \"3\": \"B-ORGANIZATION\",\n",
      "    \"4\": \"I-ORGANIZATION\",\n",
      "    \"5\": \"B-LOCATION\",\n",
      "    \"6\": \"I-LOCATION\",\n",
      "    \"7\": \"B-MISCELLANEOUS\",\n",
      "    \"8\": \"I-MISCELLANEOUS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOCATION\": 5,\n",
      "    \"B-MISCELLANEOUS\": 7,\n",
      "    \"B-ORGANIZATION\": 3,\n",
      "    \"B-PERSON\": 1,\n",
      "    \"I-LOCATION\": 6,\n",
      "    \"I-MISCELLANEOUS\": 8,\n",
      "    \"I-ORGANIZATION\": 4,\n",
      "    \"I-PERSON\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Training Arguments\n",
    "The training arguments define several parameters of how the training of the model will happen. Some argument define where the outputs will be save, how often during training we want to compute metric, how many epochs we will use for training, define the learning rate and many others. There are a several arguments which can all be found in the documentation of the function:\n",
    "\n",
    "[https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "\n",
    "The trainer uses AdamW for the backpropagation optimization. We can use a custom one if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"bert-base-portuguese-cased-ner-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "The trainer object is what we’ll use to run the training process. The arguments are fairly simple:\n",
    "\n",
    "- The pre-trained model we will use\n",
    "- The training arguments\n",
    "- The train dataset (already tokenized)\n",
    "- The validation dataset\n",
    "- The data collator\n",
    "- The metrics we will use for validation\n",
    "- The tokenizer\n",
    "\n",
    "The training is done by calling the train method as trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "GPU memory occupied: 8076 MB.\n",
      "Mon Apr  3 04:31:11 2023       "
     ]
    }
   ],
   "source": [
    "print(trainer.args.device)\n",
    "print_gpu_utilization()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 38\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25\n",
      "  Number of trainable parameters = 108339465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.02       Driver Version: 528.02       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   37C    P5    14W / 180W |   7957MiB /  8192MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2900    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      3396    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n",
      "|    0   N/A  N/A      4428    C+G   ...\\app-1.0.9011\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      4472    C+G   ...\\app-1.0.9011\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      5908    C+G   ...3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10652    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     12164    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12316    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     12400    C+G   ... Download Manager\\fdm.exe    N/A      |\n",
      "|    0   N/A  N/A     12700    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     13112    C+G   ...werToys.PowerLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A     13352    C+G   ...ray\\lghub_system_tray.exe    N/A      |\n",
      "|    0   N/A  N/A     14260    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15748    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     16356    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     18336    C+G   ...aming\\Spotify\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     18476    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18684    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     19476    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     19768    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     20296    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     22372      C   ...da3\\envs\\NLP22\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     25012    C+G   ...661.54\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     26012    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55b9b399e4646f981b2d2d0e018cf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6bcabaa2e640709efc383984a6a221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\n",
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4058964252471924, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.7182601880877743, 'eval_runtime': 0.6931, 'eval_samples_per_second': 27.411, 'eval_steps_per_second': 4.328, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e551386a7a844125bde98c040b975935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\n",
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9357545971870422, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.7394200626959248, 'eval_runtime': 0.6373, 'eval_samples_per_second': 29.812, 'eval_steps_per_second': 4.707, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4305b2f6b25e40cdb6b7556c45eee909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\n",
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7065246105194092, 'eval_precision': 0.1, 'eval_recall': 0.04597701149425287, 'eval_f1': 0.06299212598425197, 'eval_accuracy': 0.832680250783699, 'eval_runtime': 0.7271, 'eval_samples_per_second': 26.133, 'eval_steps_per_second': 4.126, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1d7cb6e92445c1977dbf97c9837b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5992717146873474, 'eval_precision': 0.8285714285714286, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.47540983606557374, 'eval_accuracy': 0.850705329153605, 'eval_runtime': 0.6647, 'eval_samples_per_second': 28.584, 'eval_steps_per_second': 4.513, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\\config.json\n",
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23361e7cc0254ed4b62da7efa3dc51ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\n",
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5647721290588379, 'eval_precision': 0.7111111111111111, 'eval_recall': 0.367816091954023, 'eval_f1': 0.48484848484848486, 'eval_accuracy': 0.8557993730407524, 'eval_runtime': 0.6702, 'eval_samples_per_second': 28.351, 'eval_steps_per_second': 4.476, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 131.4418, 'train_samples_per_second': 1.446, 'train_steps_per_second': 0.19, 'train_loss': 1.0373165130615234, 'epoch': 5.0}\n",
      "Time: 131.44\n",
      "Samples/second: 1.45\n",
      "GPU memory occupied: 8079 MB.\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 8082 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainer.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to auto_tagger\n",
      "Configuration saved in auto_tagger\\config.json\n",
      "Model weights saved in auto_tagger\\pytorch_model.bin\n",
      "tokenizer config file saved in auto_tagger\\tokenizer_config.json\n",
      "Special tokens file saved in auto_tagger\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('auto_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file auto_tagger\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"auto_tagger\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PERSON\",\n",
      "    \"2\": \"I-PERSON\",\n",
      "    \"3\": \"B-ORGANIZATION\",\n",
      "    \"4\": \"I-ORGANIZATION\",\n",
      "    \"5\": \"B-LOCATION\",\n",
      "    \"6\": \"I-LOCATION\",\n",
      "    \"7\": \"B-MISCELLANEOUS\",\n",
      "    \"8\": \"I-MISCELLANEOUS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOCATION\": 5,\n",
      "    \"B-MISCELLANEOUS\": 7,\n",
      "    \"B-ORGANIZATION\": 3,\n",
      "    \"B-PERSON\": 1,\n",
      "    \"I-LOCATION\": 6,\n",
      "    \"I-MISCELLANEOUS\": 8,\n",
      "    \"I-ORGANIZATION\": 4,\n",
      "    \"I-PERSON\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading configuration file auto_tagger\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"auto_tagger\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PERSON\",\n",
      "    \"2\": \"I-PERSON\",\n",
      "    \"3\": \"B-ORGANIZATION\",\n",
      "    \"4\": \"I-ORGANIZATION\",\n",
      "    \"5\": \"B-LOCATION\",\n",
      "    \"6\": \"I-LOCATION\",\n",
      "    \"7\": \"B-MISCELLANEOUS\",\n",
      "    \"8\": \"I-MISCELLANEOUS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOCATION\": 5,\n",
      "    \"B-MISCELLANEOUS\": 7,\n",
      "    \"B-ORGANIZATION\": 3,\n",
      "    \"B-PERSON\": 1,\n",
      "    \"I-LOCATION\": 6,\n",
      "    \"I-MISCELLANEOUS\": 8,\n",
      "    \"I-ORGANIZATION\": 4,\n",
      "    \"I-PERSON\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading weights file auto_tagger\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at auto_tagger.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\n",
    "    \"token-classification\",\n",
    "    model='auto_tagger',\n",
    "    aggregation_strategy=\"average\",\n",
    "    ignore_labels=[\"\"],\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento gerado sob autenticação Nº NIK.843.557.VM6, disponível no endereço http://www.ufrgs.br/autenticacao 1/1 PORTARIA Nº 1181 de 18/02/2016 O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no uso de suas atribuições que lhe foram conferidas pela Portaria nº 5469, de 04 de outubro de 2012, do Magnífico Reitor, e conforme o Laudo Médico n°37564, RESOLVE: Designar, temporariamente, nos termos da Lei nº 8.112, de 11 de dezembro de 1990, com redação dada pela Lei nº 9.527, de 10 de dezembro de 1997, a ocupante do cargo de ASSISTENTE EM ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade, DENISE SCHROEDER (Siape: 0358763 ), para substituir MARILDA SANTOS DA ROCHA (Siape: 1044125 ), Secretário do Depto de Plantas Forrageiras e Agrometeorologia da Faculdade de Agronomia, Código FG-7, em seu afastamento por motivo de Laudo Médico do titular da Função, no período de 03/02/2016 a 10/02/2016, com o decorrente pagamento das vantagens por 8 dias. MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas\n",
      "['[CLS]', 'Docu', '##mento', 'gerado', 'sob', 'autent', '##icação', '[UNK]', 'N', '##I', '##K', '.', '84', '##3', '.', '55', '##7', '.', 'V', '##M', '##6', ',', 'disponível', 'no', 'endereço', 'http', ':', '/', '/', 'w', '##ww', '.', 'u', '##fr', '##gs', '.', 'b', '##r', '/', 'autent', '##ica', '##ca', '##o', '1', '/', '1', 'P', '##OR', '##TA', '##RI', '##A', '[UNK]', '118', '##1', 'de', '18', '/', '02', '/', '2016', 'O', 'PR', '##Ó', '-', 'R', '##EI', '##TO', '##R', 'DE', 'G', '##ES', '##T', '##ÃO', 'DE', 'P', '##ES', '##SO', '##AS', 'D', '##A', 'UN', '##IV', '##ER', '##S', '##ID', '##AD', '##E', 'F', '##ED', '##ER', '##AL', 'DO', 'R', '##IO', 'G', '##RA', '##N', '##DE', 'DO', 'S', '##UL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', 'que', 'lhe', 'foram', 'confer', '##idas', 'pela', 'Porta', '##ria', '[UNK]', '54', '##69', ',', 'de', '04', 'de', 'outubro', 'de', '2012', ',', 'do', 'Magn', '##ífico', 'Rei', '##tor', ',', 'e', 'conforme', 'o', 'Lau', '##do', 'Médico', 'n', '##°', '##37', '##56', '##4', ',', 'R', '##ES', '##OL', '##V', '##E', ':', 'Design', '##ar', ',', 'temporariamente', ',', 'nos', 'termos', 'da', 'Lei', '[UNK]', '8', '.', '112', ',', 'de', '11', 'de', 'dezembro', 'de', '1990', ',', 'com', 'redação', 'dada', 'pela', 'Lei', '[UNK]', '9', '.', '52', '##7', ',', 'de', '10', 'de', 'dezembro', 'de', '1997', ',', 'a', 'ocupa', '##nte', 'do', 'cargo', 'de', 'AS', '##S', '##IS', '##TE', '##NT', '##E', 'E', '##M', 'A', '##D', '##MI', '##N', '##IS', '##T', '##RA', '##Ç', '##ÃO', ',', 'do', 'Quad', '##ro', 'de', 'Pessoa', '##l', 'desta', 'Universidade', ',', 'DE', '##N', '##IS', '##E', 'SC', '##H', '##RO', '##ED', '##ER', '(', 'Si', '##ap', '##e', ':', '03', '##58', '##7', '##6', '##3', ')', ',', 'para', 'substituir', 'MA', '##RI', '##L', '##DA', 'SA', '##NT', '##OS', 'D', '##A', 'R', '##OC', '##HA', '(', 'Si', '##ap', '##e', ':', '10', '##44', '##12', '##5', ')', ',', 'Secretário', 'do', 'De', '##pto', 'de', 'Plan', '##tas', 'For', '##rag', '##eiras', 'e', 'Agro', '##met', '##e', '##oro', '##logia', 'da', 'Faculdade', 'de', 'Agro', '##nomia', ',', 'Código', 'F', '##G', '-', '7', ',', 'em', 'seu', 'afastamento', 'por', 'motivo', 'de', 'Lau', '##do', 'Médico', 'do', 'titular', 'da', 'Fun', '##ção', ',', 'no', 'período', 'de', '03', '/', '02', '/', '2016', 'a', '10', '/', '02', '/', '2016', ',', 'com', 'o', 'decorre', '##nte', 'pagamento', 'das', 'vantagens', 'por', '8', 'dias', '.', 'MA', '##UR', '##Í', '##CI', '##O', 'VI', '##É', '##GA', '##S', 'D', '##A', 'S', '##IL', '##VA', 'Pr', '##ó', '-', 'Rei', '##tor', 'de', 'Gestão', 'de', 'Pessoas', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "test_sample = \"\"\"Documento gerado sob autenticação Nº NIK.843.557.VM6, disponível no endereço http://www.ufrgs.br/autenticacao\n",
    "1/1\n",
    "PORTARIA Nº             1181                  de  18/02/2016\n",
    "O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no\n",
    "uso de suas atribuições que lhe foram conferidas pela Portaria nº.5469, de 04 de outubro de 2012, do\n",
    "Magnífico Reitor, e conforme o Laudo Médico n°37564,\n",
    "RESOLVE:\n",
    "Designar, temporariamente, nos termos da Lei nº. 8.112, de 11 de dezembro de 1990, com redação\n",
    "dada  pela  Lei  nº.9.527,  de  10  de  dezembro  de  1997,  a  ocupante  do  cargo  de  ASSISTENTE  EM\n",
    "ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade, DENISE SCHROEDER (Siape: 0358763 ),  para\n",
    "substituir   MARILDA SANTOS DA ROCHA (Siape: 1044125 ), Secretário do Depto de Plantas Forrageiras e\n",
    "Agrometeorologia da Faculdade de Agronomia, Código FG-7, em seu afastamento por motivo de Laudo\n",
    "Médico do titular da Função, no período de 03/02/2016 a 10/02/2016, com o decorrente pagamento das\n",
    "vantagens por 8 dias.\n",
    "MAURÍCIO VIÉGAS DA SILVA\n",
    "Pró-Reitor de Gestão de Pessoas\"\"\" \n",
    "clean_sample = data_processing.clear_text(test_sample)\n",
    "print(clean_sample)\n",
    "token_sample = tokenizer(clean_sample)\n",
    "print(token_sample.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.token_classification.TokenClassificationPipeline"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'O',\n",
       "  'score': 0.82746357,\n",
       "  'word': 'Documento gerado sob autenticação Nº NIK. 843. 557. VM6, disponível no endereço http : / / www. ufrgs. br / autenticacao 1 / 1 PORTARIA Nº 1181 de 18 / 02 / 2016 O PRÓ - REITOR DE',\n",
       "  'start': 0,\n",
       "  'end': 160},\n",
       " {'entity_group': 'MISCELLANEOUS',\n",
       "  'score': 0.3277286,\n",
       "  'word': 'GESTÃO',\n",
       "  'start': 161,\n",
       "  'end': 167},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.4662776,\n",
       "  'word': 'DE PESSOAS DA',\n",
       "  'start': 168,\n",
       "  'end': 181},\n",
       " {'entity_group': 'ORGANIZATION',\n",
       "  'score': 0.669568,\n",
       "  'word': 'UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL',\n",
       "  'start': 182,\n",
       "  'end': 223},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.8813147,\n",
       "  'word': ', no uso de suas atribuições que lhe foram conferidas pela Portaria nº 5469, de 04 de outubro de 2012, do Magnífico Reitor, e conforme o Laudo Médico n°37564, RESOLVE : Designar, temporariamente, nos termos da Lei nº 8. 112, de 11 de dezembro de 1990, com redação dada pela Lei nº 9. 527, de 10 de dezembro de 1997, a ocupante do cargo de ASSISTENTE EM ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade,',\n",
       "  'start': 223,\n",
       "  'end': 628},\n",
       " {'entity_group': 'PERSON',\n",
       "  'score': 0.53349257,\n",
       "  'word': 'DENISE SCHROEDER',\n",
       "  'start': 629,\n",
       "  'end': 645},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.9132752,\n",
       "  'word': '( Siape : 0358763 ), para substituir',\n",
       "  'start': 646,\n",
       "  'end': 680},\n",
       " {'entity_group': 'PERSON',\n",
       "  'score': 0.5948445,\n",
       "  'word': 'MARILDA SANTOS DA ROCHA',\n",
       "  'start': 681,\n",
       "  'end': 704},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.82912296,\n",
       "  'word': '( Siape : 1044125 ), Secretário do Depto de Plantas Forrageiras e Agrometeorologia da Faculdade de Agronomia, Código FG - 7, em seu afastamento por motivo de Laudo Médico do titular da Função, no período de 03 / 02 / 2016 a 10 / 02 / 2016, com o decorrente pagamento das vantagens por 8 dias.',\n",
       "  'start': 705,\n",
       "  'end': 985},\n",
       " {'entity_group': 'PERSON',\n",
       "  'score': 0.63545716,\n",
       "  'word': 'MAURÍCIO VIÉGAS DA SILVA',\n",
       "  'start': 986,\n",
       "  'end': 1010},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.5815567,\n",
       "  'word': 'Pró - Reitor de Gestão de Pessoas',\n",
       "  'start': 1011,\n",
       "  'end': 1042}]"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ner(clean_sample)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_sentence(original_sentence, tagged_results):\n",
    "    og_sentence_pos = 0\n",
    "    tagged_sentence_pos = 0\n",
    "    formatted_results = []\n",
    "    for entity in tagged_results:\n",
    "        formatted_entity = \"\"\n",
    "        #print(entity['word'])\n",
    "        for index_char, char in enumerate(entity['word']):\n",
    "            #print(char, original_sentence[og_sentence_pos])\n",
    "            if char == original_sentence[og_sentence_pos]:\n",
    "                og_sentence_pos += 1\n",
    "                formatted_entity = formatted_entity + char\n",
    "            else:\n",
    "                #print(\"Bad char:\", char)\n",
    "                # #Look ahead and see if next char is equal to original\n",
    "                if entity['word'][index_char+1] == original_sentence[og_sentence_pos]:\n",
    "                    #print(\"Found char in next pos\")\n",
    "                    pass\n",
    "                # Else add the character to the formatted entity\n",
    "                else:\n",
    "                    og_sentence_pos += 2\n",
    "                    formatted_entity = formatted_entity + char\n",
    "\n",
    "        #print(\"Formatted:\", formatted_entity)\n",
    "        formatted_result = entity.copy()\n",
    "        formatted_result['word'] = formatted_entity\n",
    "        formatted_results.append(formatted_result)\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_sentence = reformat_sentence(clean_sample, results)\n",
    "sentence_for_relabelling = \"\"\n",
    "for entity in reformatted_sentence:\n",
    "    sentence_for_relabelling = sentence_for_relabelling + entity['word'] + ' '\n",
    "sentence_for_relabelling = re.sub(' ,', ',', sentence_for_relabelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento gerado sob autenticação Nº NIK.843.557.VM6, disponível no endereço http://www.ufrgs.br/autenticacao 1/1 PORTARIA Nº 1181 de 18/02/2016 O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no uso de suas atribuições que lhe foram conferidas pela Portaria nº 5469, de 04 de outubro de 2012, do Magnífico Reitor, e conforme o Laudo Médico n°37564, RESOLVE: Designar, temporariamente, nos termos da Lei nº 8.112, de 11 de dezembro de 1990, com redação dada pela Lei nº 9.527, de 10 de dezembro de 1997, a ocupante do cargo de ASSISTENTE EM ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade, DENISE SCHROEDER  Siape: 0358763 ), para substituir MARILDA SANTOS DA ROCHA  Siape: 1044125 ), Secretário do Depto de Plantas Forrageiras e Agrometeorologia da Faculdade de Agronomia, Código FG-7, em seu afastamento por motivo de Laudo Médico do titular da Função, no período de 03/02/2016 a 10/02/2016, com o decorrente pagamento das vantagens por 8 dias. MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas \n",
      "Documento gerado sob autenticação Nº NIK.843.557.VM6, disponível no endereço http://www.ufrgs.br/autenticacao 1/1 PORTARIA Nº 1181 de 18/02/2016 O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no uso de suas atribuições que lhe foram conferidas pela Portaria nº 5469, de 04 de outubro de 2012, do Magnífico Reitor, e conforme o Laudo Médico n°37564, RESOLVE: Designar, temporariamente, nos termos da Lei nº 8.112, de 11 de dezembro de 1990, com redação dada pela Lei nº 9.527, de 10 de dezembro de 1997, a ocupante do cargo de ASSISTENTE EM ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade, DENISE SCHROEDER (Siape: 0358763 ), para substituir MARILDA SANTOS DA ROCHA (Siape: 1044125 ), Secretário do Depto de Plantas Forrageiras e Agrometeorologia da Faculdade de Agronomia, Código FG-7, em seu afastamento por motivo de Laudo Médico do titular da Função, no período de 03/02/2016 a 10/02/2016, com o decorrente pagamento das vantagens por 8 dias. MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas\n"
     ]
    }
   ],
   "source": [
    "print(sentence_for_relabelling)\n",
    "print(clean_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sentence(tagged_sentence):\n",
    "    list_of_words = []\n",
    "    for token in tokens:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
