{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#External\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer, WhitespaceTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoTokenizer\n",
    "import textwrap\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "#Internal\n",
    "import data_cleaning.data_cleaning as dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Annotated Sentences\n",
    "We get the sentences from 'prep_data_labeling.ipynb' that we annotated manually with deccano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = pd.read_json('../data/annotated/doccano-extraction-09-12-24.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tagged_entities",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "document_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentence_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "doc__sentence_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "duplicate_ids",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5f547c3e-488d-44f9-9bda-b4dff7a86d83",
       "rows": [
        [
         "0",
         "2227",
         "Conceder à servidora LISIANE RAMOS VILK, ocupante do cargo de Administrador de Edifícios - 701400, lotada na Faculdade de Arquitetura, SIAPE 2325261, o percentual de 25% (vinte e cinco por cento) de Incentivo à Qualificação, a contar de 15/07/2016, tendo em vista a conclusão do curso de Graduação em Administração - Bacharelado, conforme o Processo nº 23078.015333/2016-19.",
         "[{'entity_group': 'O', 'score': 0.7762932777, 'word': 'Conceder à servidora', 'start': 0, 'end': 20}, {'entity_group': 'PERSON', 'score': 0.6477044225, 'word': 'LISIANE RAMOS VILK', 'start': 21, 'end': 39}, {'entity_group': 'O', 'score': 0.7640748620000001, 'word': ', ocupante do cargo de Administrador de Edifícios - 701400, lotada na Faculdade de Arquitetura, SIAPE 2325261, o percentual de 25% (vinte e cinco por cento) de Incentivo à Qualificação, a contar de 15/07/2016, tendo em vista a conclusão do curso de Graduação em Administração - Bacharelado, conforme o Processo nº 23078.015333/2016-19.', 'start': 40, 'end': 375}]",
         "25644",
         "2",
         "25644-2",
         "['25644-2']",
         "[[21, 39, 'PERSON'], [62, 88, 'OCCUPATION'], [109, 133, 'ORGANIZATION'], [237, 247, 'DATE']]"
        ],
        [
         "1",
         "2228",
         "MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas",
         "[{'entity_group': 'PERSON', 'score': 0.6592996120000001, 'word': 'MAURÍCIO VIÉGAS DA SILVA', 'start': 0, 'end': 24}, {'entity_group': 'O', 'score': 0.38087555770000003, 'word': 'Pró-Reitor de Gestão de Pessoas', 'start': 25, 'end': 56}]",
         "105798",
         "3",
         "105798-3",
         "['105798-3', '105799-3', '105801-3', '105802-3', '18001-2', '18003-2', '18004-2', '18005-2', '18006-2', '25644-3', '25675-3', '25702-3', '27011-3', '27012-4', '27035-3', '27036-3', '27037-3', '27038-3', '27040-3', '27042-3', '27043-3', '27044-3', '27045-3', '27052-3', '27053-3', '27054-3', '27055-3', '27057-3', '27058-3', '27059-3', '32075-3', '32090-4', '32091-4', '32093-4', '32094-4', '32097-4', '32099-4', '32100-4', '32101-4', '32102-4', '32104-4', '32105-4', '32106-4', '32107-4', '32108-4', '32109-4', '32110-4', '32111-4', '32112-4', '32113-4', '32114-3', '32115-3', '32116-3', '32117-3', '32118-3', '32119-3', '32120-3', '32121-3', '32122-3', '32123-5', '32124-3', '32125-3', '32126-3', '32127-3', '32128-3', '32129-4', '32130-4', '32131-4', '32133-3', '32135-5', '32136-5', '32150-3', '32151-3', '32152-3', '32153-3', '32154-3', '32155-3', '32156-3', '32157-3', '32158-3', '32159-3', '32160-3', '32161-3', '32162-3', '32163-3', '32164-3', '32165-3', '32166-3', '32167-3', '32168-3', '32169-3', '32170-3', '32171-3', '32172-3', '32173-3', '32174-3', '32175-3', '32176-3', '32177-3', '32178-3', '32179-3', '32180-3', '32181-3', '32182-3', '32186-3', '32195-5', '32198-3', '32201-3', '37694-4', '52918-3', '55504-3', '55506-4', '55510-3', '55521-4', '55529-3', '55602-3', '55613-3', '55622-3', '55633-4', '55635-4', '55649-3', '55650-3', '55651-3', '55657-3', '55659-3', '55662-3', '55665-3', '55666-3', '55669-4', '55671-3', '55672-3', '55676-3', '55690-3', '55697-3', '55698-3', '55700-3', '55701-3', '55715-4', '55716-4', '55719-3', '55720-3', '55721-3', '55723-3', '55725-3', '55727-3', '55732-3', '55735-3', '55737-3', '55738-3', '55740-3', '55741-3', '55745-3', '55746-3', '55747-3', '55752-3', '55753-3', '55754-3', '55756-3', '55757-3', '55758-3', '55759-3', '55760-3', '55761-3', '55762-3', '55774-3', '55778-3', '55779-3', '55781-3', '55782-3', '55783-3', '55785-3', '55787-3', '55794-3', '55795-3', '55796-3', '55797-3', '55799-3', '55800-8', '55802-3', '55805-3', '55806-3', '55807-4', '55808-4', '55810-4', '69949-3', '69950-3', '69951-3', '69952-3', '69953-3', '69954-3', '69955-3', '69956-3', '69957-3', '69958-3', '69959-3', '69961-3', '69962-3', '69963-3', '69965-3', '69966-3', '69967-5', '69968-3', '69969-3', '69970-3', '69971-3', '69972-3', '69974-3', '72967-3', '72971-3', '72974-3', '72975-3', '72980-3', '72986-3', '72996-3', '73006-3', '73008-3', '73010-3', '73011-3', '73013-3', '73015-3', '73016-3', '73017-3', '73021-3', '73024-3', '73027-3', '93653-3', '93654-3', '93657-3', '93663-3', '93664-3', '93669-3', '93670-4', '93671-3', '93673-3']",
         "[[0, 24, 'PERSON'], [25, 56, 'OCCUPATION']]"
        ],
        [
         "2",
         "2229",
         "Autorizar o afastamento do país de CRISTINE MARIA WARMLING, Professor do Magistério Superior, lotada e em exercício no Departamento de Odontologia Preventiva e Social da Faculdade de Odontologia, com a finalidade de participar do \"3ème Congrès de la Societé Internationale d'Ergologie\", em Aix-en-Provence - França, no período compreendido entre 28/08/2016 e 01/09/2016, com ônus limitado.",
         "[{'entity_group': 'O', 'score': 0.6819901466, 'word': 'Autorizar o afastamento do país de', 'start': 0, 'end': 34}, {'entity_group': 'PERSON', 'score': 0.6194185615000001, 'word': 'CRISTINE MARIA WARMLING', 'start': 35, 'end': 58}, {'entity_group': 'O', 'score': 0.6582459807000001, 'word': ', Professor do Magistério Superior, lotada e em exercício no Departamento de Odontologia Preventiva e Social da Faculdade de Odontologia, com a finalidade de participar do \"3ème Congrès de la Societé Internationale d\\'Ergologie\", em Aix-en-Provence - França, no período compreendido entre 28/08/2016 e 01/09/2016, com ônus limitado.', 'start': 59, 'end': 390}]",
         "25645",
         "2",
         "25645-2",
         "['25645-2']",
         "[[35, 58, 'PERSON'], [60, 92, 'OCCUPATION'], [170, 194, 'ORGANIZATION'], [250, 284, 'ORGANIZATION'], [290, 305, 'LOCATION'], [308, 314, 'LOCATION'], [346, 356, 'DATE'], [359, 369, 'DATE']]"
        ],
        [
         "3",
         "2230",
         "CARLOS ALEXANDRE NETTO Reitor",
         "[{'entity_group': 'PERSON', 'score': 0.6323550344000001, 'word': 'CARLOS ALEXANDRE NETTO', 'start': 0, 'end': 22}, {'entity_group': 'O', 'score': 0.41836801170000004, 'word': 'Reitor', 'start': 23, 'end': 29}]",
         "25645",
         "4",
         "25645-4",
         "['25645-4', '25646-4', '25647-4', '25648-4', '25649-4', '25653-4', '25654-4', '25665-4', '25670-4', '25672-4', '25676-4', '25677-4', '25681-4', '25684-4', '25696-4', '25697-4', '25699-4', '25700-4', '25701-4', '25704-4']",
         "[[0, 22, 'PERSON'], [23, 29, 'OCCUPATION']]"
        ],
        [
         "4",
         "2231",
         "Autorizar o afastamento do país de ANDRE DIAS MORTARI, Assistente em Administração, lotado e em exercício na Escola de Administração, com a finalidade de participar do \"IV Congreso Internacional Red Pilares\", em Cartagena - Colombia, no período compreendido entre 29/08/2016 e 02/09/2016, com ônus limitado.",
         "[{'entity_group': 'O', 'score': 0.6808940768, 'word': 'Autorizar o afastamento do país de', 'start': 0, 'end': 34}, {'entity_group': 'PERSON', 'score': 0.6045627594, 'word': 'ANDRE DIAS MORTARI', 'start': 35, 'end': 53}, {'entity_group': 'O', 'score': 0.7239830494, 'word': ', Assistente em Administração, lotado e em exercício na Escola de Administração, com a finalidade de participar do \"IV Congreso Internacional Red Pilares\", em Cartagena - Colombia, no período compreendido entre 29/08/2016 e 02/09/2016, com ônus limitado.', 'start': 54, 'end': 308}]",
         "25646",
         "2",
         "25646-2",
         "['25646-2']",
         "[[35, 53, 'PERSON'], [55, 82, 'OCCUPATION'], [109, 132, 'ORGANIZATION'], [212, 222, 'LOCATION'], [224, 232, 'LOCATION'], [264, 274, 'DATE'], [277, 287, 'DATE']]"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>tagged_entities</th>\n",
       "      <th>document_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc__sentence_id</th>\n",
       "      <th>duplicate_ids</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2227</td>\n",
       "      <td>Conceder à servidora LISIANE RAMOS VILK, ocupa...</td>\n",
       "      <td>[{'entity_group': 'O', 'score': 0.7762932777, ...</td>\n",
       "      <td>25644</td>\n",
       "      <td>2</td>\n",
       "      <td>25644-2</td>\n",
       "      <td>[25644-2]</td>\n",
       "      <td>[[21, 39, PERSON], [62, 88, OCCUPATION], [109,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2228</td>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[{'entity_group': 'PERSON', 'score': 0.6592996...</td>\n",
       "      <td>105798</td>\n",
       "      <td>3</td>\n",
       "      <td>105798-3</td>\n",
       "      <td>[105798-3, 105799-3, 105801-3, 105802-3, 18001...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, OCCUPATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2229</td>\n",
       "      <td>Autorizar o afastamento do país de CRISTINE MA...</td>\n",
       "      <td>[{'entity_group': 'O', 'score': 0.6819901466, ...</td>\n",
       "      <td>25645</td>\n",
       "      <td>2</td>\n",
       "      <td>25645-2</td>\n",
       "      <td>[25645-2]</td>\n",
       "      <td>[[35, 58, PERSON], [60, 92, OCCUPATION], [170,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2230</td>\n",
       "      <td>CARLOS ALEXANDRE NETTO Reitor</td>\n",
       "      <td>[{'entity_group': 'PERSON', 'score': 0.6323550...</td>\n",
       "      <td>25645</td>\n",
       "      <td>4</td>\n",
       "      <td>25645-4</td>\n",
       "      <td>[25645-4, 25646-4, 25647-4, 25648-4, 25649-4, ...</td>\n",
       "      <td>[[0, 22, PERSON], [23, 29, OCCUPATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2231</td>\n",
       "      <td>Autorizar o afastamento do país de ANDRE DIAS ...</td>\n",
       "      <td>[{'entity_group': 'O', 'score': 0.6808940768, ...</td>\n",
       "      <td>25646</td>\n",
       "      <td>2</td>\n",
       "      <td>25646-2</td>\n",
       "      <td>[25646-2]</td>\n",
       "      <td>[[35, 53, PERSON], [55, 82, OCCUPATION], [109,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  2227  Conceder à servidora LISIANE RAMOS VILK, ocupa...   \n",
       "1  2228  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "2  2229  Autorizar o afastamento do país de CRISTINE MA...   \n",
       "3  2230                      CARLOS ALEXANDRE NETTO Reitor   \n",
       "4  2231  Autorizar o afastamento do país de ANDRE DIAS ...   \n",
       "\n",
       "                                     tagged_entities  document_id  \\\n",
       "0  [{'entity_group': 'O', 'score': 0.7762932777, ...        25644   \n",
       "1  [{'entity_group': 'PERSON', 'score': 0.6592996...       105798   \n",
       "2  [{'entity_group': 'O', 'score': 0.6819901466, ...        25645   \n",
       "3  [{'entity_group': 'PERSON', 'score': 0.6323550...        25645   \n",
       "4  [{'entity_group': 'O', 'score': 0.6808940768, ...        25646   \n",
       "\n",
       "   sentence_id doc__sentence_id  \\\n",
       "0            2          25644-2   \n",
       "1            3         105798-3   \n",
       "2            2          25645-2   \n",
       "3            4          25645-4   \n",
       "4            2          25646-2   \n",
       "\n",
       "                                       duplicate_ids  \\\n",
       "0                                          [25644-2]   \n",
       "1  [105798-3, 105799-3, 105801-3, 105802-3, 18001...   \n",
       "2                                          [25645-2]   \n",
       "3  [25645-4, 25646-4, 25647-4, 25648-4, 25649-4, ...   \n",
       "4                                          [25646-2]   \n",
       "\n",
       "                                               label  \n",
       "0  [[21, 39, PERSON], [62, 88, OCCUPATION], [109,...  \n",
       "1            [[0, 24, PERSON], [25, 56, OCCUPATION]]  \n",
       "2  [[35, 58, PERSON], [60, 92, OCCUPATION], [170,...  \n",
       "3            [[0, 22, PERSON], [23, 29, OCCUPATION]]  \n",
       "4  [[35, 53, PERSON], [55, 82, OCCUPATION], [109,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate duplicate sentences [OPTIONAL STEP]\n",
    "(Having repeated sentences might not help training. Ideal solution is using data augmentation if more data is necessary)\n",
    "Next we'll explode our dataframe so that duplicate sentences can become separate rows. We'll also drop a few columns that will have inconsistent information after the explode and will also not be useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotated_df = annotated_df.explode(\"duplicate_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean document so only required data is kept\n",
    "All the document and sentences ids are not needed for training. Their use would be necessary only when trying to map sentences back to the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "beed7af5-22e7-4a94-92c6-04dad938b991",
       "rows": [
        [
         "0",
         "Conceder à servidora LISIANE RAMOS VILK, ocupante do cargo de Administrador de Edifícios - 701400, lotada na Faculdade de Arquitetura, SIAPE 2325261, o percentual de 25% (vinte e cinco por cento) de Incentivo à Qualificação, a contar de 15/07/2016, tendo em vista a conclusão do curso de Graduação em Administração - Bacharelado, conforme o Processo nº 23078.015333/2016-19.",
         "[[21, 39, 'PERSON'], [62, 88, 'OCCUPATION'], [109, 133, 'ORGANIZATION'], [237, 247, 'DATE']]"
        ],
        [
         "1",
         "MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas",
         "[[0, 24, 'PERSON'], [25, 56, 'OCCUPATION']]"
        ],
        [
         "2",
         "Autorizar o afastamento do país de CRISTINE MARIA WARMLING, Professor do Magistério Superior, lotada e em exercício no Departamento de Odontologia Preventiva e Social da Faculdade de Odontologia, com a finalidade de participar do \"3ème Congrès de la Societé Internationale d'Ergologie\", em Aix-en-Provence - França, no período compreendido entre 28/08/2016 e 01/09/2016, com ônus limitado.",
         "[[35, 58, 'PERSON'], [60, 92, 'OCCUPATION'], [170, 194, 'ORGANIZATION'], [250, 284, 'ORGANIZATION'], [290, 305, 'LOCATION'], [308, 314, 'LOCATION'], [346, 356, 'DATE'], [359, 369, 'DATE']]"
        ],
        [
         "3",
         "CARLOS ALEXANDRE NETTO Reitor",
         "[[0, 22, 'PERSON'], [23, 29, 'OCCUPATION']]"
        ],
        [
         "4",
         "Autorizar o afastamento do país de ANDRE DIAS MORTARI, Assistente em Administração, lotado e em exercício na Escola de Administração, com a finalidade de participar do \"IV Congreso Internacional Red Pilares\", em Cartagena - Colombia, no período compreendido entre 29/08/2016 e 02/09/2016, com ônus limitado.",
         "[[35, 53, 'PERSON'], [55, 82, 'OCCUPATION'], [109, 132, 'ORGANIZATION'], [212, 222, 'LOCATION'], [224, 232, 'LOCATION'], [264, 274, 'DATE'], [277, 287, 'DATE']]"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conceder à servidora LISIANE RAMOS VILK, ocupa...</td>\n",
       "      <td>[[21, 39, PERSON], [62, 88, OCCUPATION], [109,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, OCCUPATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autorizar o afastamento do país de CRISTINE MA...</td>\n",
       "      <td>[[35, 58, PERSON], [60, 92, OCCUPATION], [170,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CARLOS ALEXANDRE NETTO Reitor</td>\n",
       "      <td>[[0, 22, PERSON], [23, 29, OCCUPATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Autorizar o afastamento do país de ANDRE DIAS ...</td>\n",
       "      <td>[[35, 53, PERSON], [55, 82, OCCUPATION], [109,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Conceder à servidora LISIANE RAMOS VILK, ocupa...   \n",
       "1  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "2  Autorizar o afastamento do país de CRISTINE MA...   \n",
       "3                      CARLOS ALEXANDRE NETTO Reitor   \n",
       "4  Autorizar o afastamento do país de ANDRE DIAS ...   \n",
       "\n",
       "                                               label  \n",
       "0  [[21, 39, PERSON], [62, 88, OCCUPATION], [109,...  \n",
       "1            [[0, 24, PERSON], [25, 56, OCCUPATION]]  \n",
       "2  [[35, 58, PERSON], [60, 92, OCCUPATION], [170,...  \n",
       "3            [[0, 22, PERSON], [23, 29, OCCUPATION]]  \n",
       "4  [[35, 53, PERSON], [55, 82, OCCUPATION], [109,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = annotated_df[['text', 'label']].copy()\n",
    "print(sentences.shape)\n",
    "sentences.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all sentences that only contain 1 or less labels. This is useful so that we don't fine tune our model with sentences that have almost no information about named entities. [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before filter: (768, 2)\n",
      "Shape after filter: (676, 2)\n"
     ]
    }
   ],
   "source": [
    "#Drop all sentences that contain 1 or less labels\n",
    "print('Shape before filter:', sentences.shape)\n",
    "\n",
    "for idx, row in sentences.iterrows():\n",
    "    if len(row['label']) <= 1 :\n",
    "        sentences.drop(idx, inplace=True)\n",
    "sentences.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Shape after filter:', sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check list of sentences with a specific length\n",
    "#sentences[sentences['label'].map(len) == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset from sentences to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the sentences and tokens using the TreebankWordTokenizer\n",
    "The first method 'tokenize' will split our sentence and return a list of words which we'll organize in a 'tokenized_sentences' list.\n",
    "The second method 'span_tokenize' will return the start and end position of each token resulting from the split. We'll organize them in a 'token_positions' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conceder', 'à', 'servidora', 'LISIANE', 'RAMOS', 'VILK', ',', 'ocupante', 'do', 'cargo', 'de', 'Administrador', 'de', 'Edifícios', '-', '701400', ',', 'lotada', 'na', 'Faculdade', 'de', 'Arquitetura', ',', 'SIAPE', '2325261', ',', 'o', 'percentual', 'de', '25', '%', '(', 'vinte', 'e', 'cinco', 'por', 'cento', ')', 'de', 'Incentivo', 'à', 'Qualificação', ',', 'a', 'contar', 'de', '15/07/2016', ',', 'tendo', 'em', 'vista', 'a', 'conclusão', 'do', 'curso', 'de', 'Graduação', 'em', 'Administração', '-', 'Bacharelado', ',', 'conforme', 'o', 'Processo', 'nº', '23078.015333/2016-19', '.']\n",
      "[(0, 8), (9, 10), (11, 20), (21, 28), (29, 34), (35, 39), (39, 40), (41, 49), (50, 52), (53, 58), (59, 61), (62, 75), (76, 78), (79, 88), (89, 90), (91, 97), (97, 98), (99, 105), (106, 108), (109, 118), (119, 121), (122, 133), (133, 134), (135, 140), (141, 148), (148, 149), (150, 151), (152, 162), (163, 165), (166, 168), (168, 169), (170, 171), (171, 176), (177, 178), (179, 184), (185, 188), (189, 194), (194, 195), (196, 198), (199, 208), (209, 210), (211, 223), (223, 224), (225, 226), (227, 233), (234, 236), (237, 247), (247, 248), (249, 254), (255, 257), (258, 263), (264, 265), (266, 275), (276, 278), (279, 284), (285, 287), (288, 297), (298, 300), (301, 314), (315, 316), (317, 328), (328, 329), (330, 338), (339, 340), (341, 349), (350, 352), (353, 373), (373, 374)]\n"
     ]
    }
   ],
   "source": [
    "# Split sentences into list of words. Tokenized_sentences has all the lists of words and token_positions has the start and end position of each word in the original sentence.\n",
    "tokenized_sentences = []\n",
    "token_positions = []\n",
    "for idx, sentence in sentences['text'].items():\n",
    "    tokenized_sentences.append(TreebankWordTokenizer().tokenize(sentence))\n",
    "    token_positions.append(list(TreebankWordTokenizer().span_tokenize(sentence)))\n",
    "\n",
    "\n",
    "print(tokenized_sentences[0])\n",
    "print(token_positions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 24, 'PERSON'], [25, 56, 'OCCUPATION']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('MAURÍCIO', (0, 8)),\n",
       " ('VIÉGAS', (9, 15)),\n",
       " ('DA', (16, 18)),\n",
       " ('SILVA', (19, 24)),\n",
       " ('Pró-Reitor', (25, 35)),\n",
       " ('de', (36, 38)),\n",
       " ('Gestão', (39, 45)),\n",
       " ('de', (46, 48)),\n",
       " ('Pessoas', (49, 56))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll zip both the words and their positions together in the same list.\n",
    "tokenized_sentence_position = []\n",
    "for sentence, positions in zip(tokenized_sentences, token_positions):\n",
    "    tokenized_sentence_position.append(list(zip(sentence, positions)))\n",
    "    \n",
    "# Check the zip\n",
    "print(sentences['label'].loc[1])\n",
    "tokenized_sentence_position[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "token",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "end",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bece0788-197d-4677-a168-feaafc5b1a70",
       "rows": [
        [
         "0",
         "MAURÍCIO",
         "0",
         "8"
        ],
        [
         "1",
         "VIÉGAS",
         "9",
         "15"
        ],
        [
         "2",
         "DA",
         "16",
         "18"
        ],
        [
         "3",
         "SILVA",
         "19",
         "24"
        ],
        [
         "4",
         "Pró-Reitor",
         "25",
         "35"
        ],
        [
         "5",
         "de",
         "36",
         "38"
        ],
        [
         "6",
         "Gestão",
         "39",
         "45"
        ],
        [
         "7",
         "de",
         "46",
         "48"
        ],
        [
         "8",
         "Pessoas",
         "49",
         "56"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAURÍCIO</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIÉGAS</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DA</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SILVA</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pró-Reitor</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>de</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gestão</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pessoas</td>\n",
       "      <td>49</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  start  end\n",
       "0    MAURÍCIO      0    8\n",
       "1      VIÉGAS      9   15\n",
       "2          DA     16   18\n",
       "3       SILVA     19   24\n",
       "4  Pró-Reitor     25   35\n",
       "5          de     36   38\n",
       "6      Gestão     39   45\n",
       "7          de     46   48\n",
       "8     Pessoas     49   56"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intermediary step to convert the list of words into a dataframe (Used only to facilitate the next steps)\n",
    "list_of_tokenized_sentences = []\n",
    "for list_of_words in tokenized_sentence_position:\n",
    "    tokens_df = pd.DataFrame(list_of_words, columns = ['token', 'pos'])\n",
    "    tokens_df[['start', 'end']] = tokens_df['pos'].to_list()\n",
    "    tokens_df = tokens_df.drop(columns='pos')\n",
    "    list_of_tokenized_sentences.append(tokens_df)\n",
    "\n",
    "# Check the dataframe\n",
    "list_of_tokenized_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning the labels to each word\n",
    "We now have, for each sentences, a dataframe of all the word and a list with all the labels.\n",
    "What needs to be done is assign the correct label to each word of the tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token_df in enumerate(list_of_tokenized_sentences):\n",
    "    labels = []\n",
    "    is_entity = False\n",
    "    for idx, token in token_df.iterrows():\n",
    "        for label in sentences['label'].loc[index]:\n",
    "            if token['start'] == label[0]: #Check if the word starts at the same position as the label. (Start is in position 0)\n",
    "                labels.append('B-' + label[2]) #Puts the label name. (Name is in position 2)\n",
    "                is_entity = True\n",
    "                break\n",
    "            if token['start'] >= label[0] and token['end'] <= label[1]: #Check if the word starts after the label and ends before it. (Start is in position 0 and end is in position 1)\n",
    "                labels.append('I-' + label[2])\n",
    "                is_entity = True\n",
    "                break\n",
    "            is_entity = False\n",
    "        if is_entity == False:\n",
    "            labels.append('O')\n",
    "    token_df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(676, 2) \n",
      " 676\n"
     ]
    }
   ],
   "source": [
    "print(sentences.shape,'\\n',\n",
    "      len(list_of_tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE 0 \n",
      "\n",
      "[[21, 39, 'PERSON'], [62, 88, 'OCCUPATION'], [109, 133, 'ORGANIZATION'], [237, 247, 'DATE']] \n",
      "\n",
      "            token  start  end           label\n",
      "3         LISIANE     21   28        B-PERSON\n",
      "4           RAMOS     29   34        I-PERSON\n",
      "5            VILK     35   39        I-PERSON\n",
      "11  Administrador     62   75    B-OCCUPATION\n",
      "12             de     76   78    I-OCCUPATION\n",
      "13      Edifícios     79   88    I-OCCUPATION\n",
      "19      Faculdade    109  118  B-ORGANIZATION\n",
      "20             de    119  121  I-ORGANIZATION\n",
      "21    Arquitetura    122  133  I-ORGANIZATION\n",
      "46     15/07/2016    237  247          B-DATE \n",
      "\n",
      "O                 58\n",
      "I-PERSON           2\n",
      "I-OCCUPATION       2\n",
      "I-ORGANIZATION     2\n",
      "B-PERSON           1\n",
      "B-OCCUPATION       1\n",
      "B-ORGANIZATION     1\n",
      "B-DATE             1\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "SENTENCE 1 \n",
      "\n",
      "[[0, 24, 'PERSON'], [25, 56, 'OCCUPATION']] \n",
      "\n",
      "        token  start  end         label\n",
      "0    MAURÍCIO      0    8      B-PERSON\n",
      "1      VIÉGAS      9   15      I-PERSON\n",
      "2          DA     16   18      I-PERSON\n",
      "3       SILVA     19   24      I-PERSON\n",
      "4  Pró-Reitor     25   35  B-OCCUPATION\n",
      "5          de     36   38  I-OCCUPATION\n",
      "6      Gestão     39   45  I-OCCUPATION\n",
      "7          de     46   48  I-OCCUPATION\n",
      "8     Pessoas     49   56  I-OCCUPATION \n",
      "\n",
      "I-OCCUPATION    4\n",
      "I-PERSON        3\n",
      "B-PERSON        1\n",
      "B-OCCUPATION    1\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "SENTENCE 2 \n",
      "\n",
      "[[35, 58, 'PERSON'], [60, 92, 'OCCUPATION'], [170, 194, 'ORGANIZATION'], [250, 284, 'ORGANIZATION'], [290, 305, 'LOCATION'], [308, 314, 'LOCATION'], [346, 356, 'DATE'], [359, 369, 'DATE']] \n",
      "\n",
      "              token  start  end           label\n",
      "6          CRISTINE     35   43        B-PERSON\n",
      "7             MARIA     44   49        I-PERSON\n",
      "8          WARMLING     50   58        I-PERSON\n",
      "10        Professor     60   69    B-OCCUPATION\n",
      "11               do     70   72    I-OCCUPATION\n",
      "12       Magistério     73   83    I-OCCUPATION\n",
      "13         Superior     84   92    I-OCCUPATION\n",
      "27        Faculdade    170  179  B-ORGANIZATION\n",
      "28               de    180  182  I-ORGANIZATION\n",
      "29      Odontologia    183  194  I-ORGANIZATION\n",
      "42          Societé    250  257  B-ORGANIZATION\n",
      "43   Internationale    258  272  I-ORGANIZATION\n",
      "44      d'Ergologie    273  284  I-ORGANIZATION\n",
      "48  Aix-en-Provence    290  305      B-LOCATION\n",
      "50           França    308  314      B-LOCATION\n",
      "56       28/08/2016    346  356          B-DATE\n",
      "58       01/09/2016    359  369          B-DATE \n",
      "\n",
      "O                 47\n",
      "I-ORGANIZATION     4\n",
      "I-OCCUPATION       3\n",
      "I-PERSON           2\n",
      "B-ORGANIZATION     2\n",
      "B-LOCATION         2\n",
      "B-DATE             2\n",
      "B-PERSON           1\n",
      "B-OCCUPATION       1\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "SENTENCE 3 \n",
      "\n",
      "[[0, 22, 'PERSON'], [23, 29, 'OCCUPATION']] \n",
      "\n",
      "       token  start  end         label\n",
      "0     CARLOS      0    6      B-PERSON\n",
      "1  ALEXANDRE      7   16      I-PERSON\n",
      "2      NETTO     17   22      I-PERSON\n",
      "3     Reitor     23   29  B-OCCUPATION \n",
      "\n",
      "I-PERSON        2\n",
      "B-PERSON        1\n",
      "B-OCCUPATION    1\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "SENTENCE 4 \n",
      "\n",
      "[[35, 53, 'PERSON'], [55, 82, 'OCCUPATION'], [109, 132, 'ORGANIZATION'], [212, 222, 'LOCATION'], [224, 232, 'LOCATION'], [264, 274, 'DATE'], [277, 287, 'DATE']] \n",
      "\n",
      "            token  start  end           label\n",
      "6           ANDRE     35   40        B-PERSON\n",
      "7            DIAS     41   45        I-PERSON\n",
      "8         MORTARI     46   53        I-PERSON\n",
      "10     Assistente     55   65    B-OCCUPATION\n",
      "11             em     66   68    I-OCCUPATION\n",
      "12  Administração     69   82    I-OCCUPATION\n",
      "19         Escola    109  115  B-ORGANIZATION\n",
      "20             de    116  118  I-ORGANIZATION\n",
      "21  Administração    119  132  I-ORGANIZATION\n",
      "38      Cartagena    212  221      B-LOCATION\n",
      "40       Colombia    224  232      B-LOCATION\n",
      "46     29/08/2016    264  274          B-DATE\n",
      "48     02/09/2016    277  287          B-DATE \n",
      "\n",
      "O                 41\n",
      "I-PERSON           2\n",
      "I-OCCUPATION       2\n",
      "I-ORGANIZATION     2\n",
      "B-LOCATION         2\n",
      "B-DATE             2\n",
      "B-PERSON           1\n",
      "B-OCCUPATION       1\n",
      "B-ORGANIZATION     1\n",
      "Name: label, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate that the tokens are correctly assigned to each label.\n",
    "for index in range(0, 5):\n",
    "    print('SENTENCE',index, '\\n') \n",
    "    print(sentences['label'].loc[index], '\\n')\n",
    "    print(list_of_tokenized_sentences[index][list_of_tokenized_sentences[index]['label'] != 'O'], '\\n')\n",
    "    print(list_of_tokenized_sentences[index]['label'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = ['O', 'B-PERSON', 'I-PERSON', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-LOCATION', 'I-LOCATION', 'B-MISCELLANEOUS', 'I-MISCELLANEOUS']\n",
    "\n",
    "# Convert label from name to id\n",
    "label_to_id = {\n",
    "    'O': 0, \n",
    "    'B-PERSON': 1, \n",
    "    'I-PERSON': 2, \n",
    "    'B-ORGANIZATION': 3, \n",
    "    'I-ORGANIZATION': 4, \n",
    "    'B-LOCATION': 5, \n",
    "    'I-LOCATION': 6, \n",
    "    'B-MISCELLANEOUS': 7, \n",
    "    'I-MISCELLANEOUS': 8\n",
    "}\n",
    "\n",
    "id_to_label = {\n",
    "    0: 'O', \n",
    "    1: 'B-PERSON', \n",
    "    2: 'I-PERSON', \n",
    "    3: 'B-ORGANIZATION', \n",
    "    4: 'I-ORGANIZATION', \n",
    "    5: 'B-LOCATION', \n",
    "    6: 'I-LOCATION', \n",
    "    7: 'B-MISCELLANEOUS', \n",
    "    8: 'I-MISCELLANEOUS'\n",
    "}\n",
    "for sentence in list_of_tokenized_sentences:\n",
    "    sentence['label'] = sentence['label'].map(label_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PERSON',\n",
       " 2: 'I-PERSON',\n",
       " 3: 'B-ORGANIZATION',\n",
       " 4: 'I-ORGANIZATION',\n",
       " 5: 'B-LOCATION',\n",
       " 6: 'I-LOCATION',\n",
       " 7: 'B-MISCELLANEOUS',\n",
       " 8: 'I-MISCELLANEOUS'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of DatasetDict, which we'll want to use, is a list of three arrow Datasets: train, test and validation. \n",
    "Each Dataset is composed of two main object: features and num_rows. We need to make sure our JSON has the features 'tokens' and 'ner_tags'\n",
    "The sample data uses the following dictionary to convert each label to an int:\n",
    "\n",
    "**{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}**\n",
    "\n",
    "Since we are using the exact same labels we can utilize this dictionary as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset into train and test\n",
    "We'll split our DataFrame into to lists of lists. One for the input tokens and another for the labels.\n",
    "After that we'll use the scklearn train_test_split method to get both our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide inputs and tokens into X and y lists.\n",
    "sample_X = []\n",
    "sample_y = []\n",
    "for sentence in list_of_token_df:\n",
    "    sample_X.append(list(sentence['token']))\n",
    "    sample_y.append(list(sentence['label']))\n",
    "\n",
    "# Split X and y into train and test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_X, sample_y, test_size=0.33, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_distribution(seq_labels):\n",
    "    label_count = {}\n",
    "    for label in target_labels:\n",
    "        label_count[label] = 0\n",
    "    for seq in seq_labels:\n",
    "        for target_id in seq:\n",
    "            label = id_to_label[target_id]\n",
    "            label_count[label] += 1\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 2495,\n",
       " 'B-PERSON': 52,\n",
       " 'I-PERSON': 138,\n",
       " 'B-ORGANIZATION': 68,\n",
       " 'I-ORGANIZATION': 233,\n",
       " 'B-LOCATION': 7,\n",
       " 'I-LOCATION': 4,\n",
       " 'B-MISCELLANEOUS': 78,\n",
       " 'I-MISCELLANEOUS': 132}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_label_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1089,\n",
       " 'B-PERSON': 22,\n",
       " 'I-PERSON': 53,\n",
       " 'B-ORGANIZATION': 30,\n",
       " 'I-ORGANIZATION': 126,\n",
       " 'B-LOCATION': 2,\n",
       " 'I-LOCATION': 0,\n",
       " 'B-MISCELLANEOUS': 33,\n",
       " 'I-MISCELLANEOUS': 52}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_label_distribution(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train and test dictionary\n",
    "train_data = {'inputs': X_train, 'targets': y_train}\n",
    "test_data = {'inputs': X_test, 'targets': y_test}\n",
    "\n",
    "#Convert dictionary into DataFrame\n",
    "#Needed as intermediary step because DataFrames support convertion into the json record format we need.\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "#Convert DataFrame into json\n",
    "train_json = train_df.to_json(orient='records')\n",
    "test_json = test_df.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save train and test jsons in the auto_data directory.\n",
    "#This directory will serve as the repository of our auto labeled data and we'll use it to import the data with the datasets library.\n",
    "import os\n",
    "os.path\n",
    "file_path = \"C:\\\\Users\\\\arthu\\\\Desktop\\\\ner-using-bert\\BERT_Experiment\\\\auto_data\\\\\"\n",
    "\n",
    "with open(file_path+'train.json', 'w') as outfile:\n",
    "    outfile.write(train_json)\n",
    "\n",
    "with open(file_path+'test.json', 'w') as outfile:\n",
    "    outfile.write(test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and validate\n",
    "We load the dataset that we saved previously. We'll use the load_dataset method from the datasets library, which will allow us to easily use hugging face models with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/arthu/.cache/huggingface/datasets/json/default-b4d3e01485de334b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d16d97e89134fb6bea177aab4b49d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfaa6d842ac4d078d15dd4013129d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa816cde17f4334948e1f1137af1658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5baeb266f34ba3bf56c25f96941f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/arthu/.cache/huggingface/datasets/json/default-b4d3e01485de334b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc0035bfa3641c89e84d67619ad25fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ufrgs_data = load_dataset('json', data_dir = file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we get a DatasetDict object with two Datasets, one for train and one for test.\n",
    "ufrgs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Documento', 'gerado', 'sob', 'autenticação', 'Nº', 'QON.500.984.BHA', ',', 'disponível', 'no', 'endereço', 'http', ':', '//www.ufrgs.br/autenticacao', '1/1', 'PORTARIA', 'Nº', '1184', 'de', '18/02/2016', 'O', 'PRÓ-REITOR', 'DE', 'GESTÃO', 'DE', 'PESSOAS', 'DA', 'UNIVERSIDADE', 'FEDERAL', 'DO', 'RIO', 'GRANDE', 'DO', 'SUL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', 'que', 'lhe', 'foram', 'conferidas', 'pela', 'Portaria', 'nº', '5469', ',', 'de', '04', 'de', 'outubro', 'de', '2012', ',', 'do', 'Magnífico', 'Reitor', ',', 'e', 'conforme', 'o', 'Laudo', 'Médico', 'n°37308', ',', 'RESOLVE', ':', 'Designar', ',', 'temporariamente', ',', 'nos', 'termos', 'da', 'Lei', 'nº', '8.112', ',', 'de', '11', 'de', 'dezembro', 'de', '1990', ',', 'com', 'redação', 'dada', 'pela', 'Lei', 'nº', '9.527', ',', 'de', '10', 'de', 'dezembro', 'de', '1997', ',', 'a', 'ocupante', 'do', 'cargo', 'de', 'PORTEIRO', ',', 'do', 'Quadro', 'de', 'Pessoal', 'desta', 'Universidade', ',', 'ELIANE', 'RICARDO', 'IRANCO', '(', 'Siape', ':', '0359359', ')', ',', 'para', 'substituir', 'SULAMAR', 'FIGUEIRA', 'MARCELINO', '(', 'Siape', ':', '0357487', ')', ',', 'Chefe', 'do', 'Setor', 'de', 'Infraestrutura', 'e', 'Patrimônio', 'da', 'Gerência', 'Administrativa', 'do', 'Instituto', 'de', 'Informática', ',', 'em', 'seu', 'afastamento', 'por', 'motivo', 'de', 'Laudo', 'Médico', 'do', 'titular', 'da', 'Função', ',', 'no', 'período', 'de', '18/01/2016', 'a', '10/02/2016', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 8, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(ufrgs_data['train'][12]['inputs'])\n",
    "print(ufrgs_data['train'][12]['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK APPENDIX TO TEST TOKENIZATION METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conceder à servidora lisiane ramos vilk, ocupante do cargo de administrador de edifícios - 701400, lotada na faculdade de arquitetura, siape 2325261, o percentual de 25% (vinte e cinco por cento) de incentivo à qualificação, a contar de 15/07/2016, tendo em vista a conclusão do curso de graduação em administração - bacharelado, conforme o processo nº 23078.015333/2016-19.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = annotated_df['text'][0].lower()\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tkn = TreebankWordTokenizer().tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_1 = tokenizer(test_sentence, is_split_into_words=False)\n",
    "\n",
    "checkpoint = 'bert-large-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_2 = tokenizer(test_sentence, is_split_into_words=False)\n",
    "\n",
    "checkpoint = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_3 = tokenizer(test_sentence, is_split_into_words=False)\n",
    "\n",
    "checkpoint = 'bert-large-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_4 = tokenizer(test_sentence, is_split_into_words=False)\n",
    "\n",
    "checkpoint = 'bert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_5 = tokenizer(test_sentence, is_split_into_words=False)\n",
    "\n",
    "checkpoint = 'bert-large-uncased-whole-word-masking'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_6 = tokenizer(test_sentence, is_split_into_words=False)\n",
    "\n",
    "checkpoint = 'bert-large-cased-whole-word-masking'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_7 = tokenizer(test_sentence, is_split_into_words=False)\n",
    "\n",
    "checkpoint = 'neuralmind/bert-base-portuguese-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_8 = tokenizer(test_sentence, is_split_into_words=False)\n",
    "\n",
    "checkpoint = 'neuralmind/bert-large-portuguese-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sw_tkn_9 = tokenizer(test_sentence, is_split_into_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conceder', 'à', 'servidora', 'lisiane', 'ramos', 'vilk', ',', 'ocupante', 'do', 'cargo', 'de', 'administrador', 'de', 'edifícios', '-', '701400', ',', 'lotada', 'na', 'faculdade', 'de', 'arquitetura', ',', 'siape', '2325261', ',', 'o', 'percentual', 'de', '25', '%', '(', 'vinte', 'e', 'cinco', 'por', 'cento', ')', 'de', 'incentivo', 'à', 'qualificação', ',', 'a', 'contar', 'de', '15/07/2016', ',', 'tendo', 'em', 'vista', 'a', 'conclusão', 'do', 'curso', 'de', 'graduação', 'em', 'administração', '-', 'bacharelado', ',', 'conforme', 'o', 'processo', 'nº', '23078.015333/2016-19', '.']\n"
     ]
    }
   ],
   "source": [
    "print(w_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'con', '##cede', '##r', 'a', 'ser', '##vid', '##ora', 'li', '##sian', '##e', 'ramos', 'vi', '##lk', ',', 'o', '##cup', '##ante', 'do', 'cargo', 'de', 'ad', '##mini', '##stra', '##dor', 'de', 'ed', '##ific', '##ios', '-', '70', '##14', '##00', ',', 'lot', '##ada', 'na', 'fa', '##cu', '##lda', '##de', 'de', 'ar', '##qui', '##tet', '##ura', ',', 'si', '##ape', '232', '##52', '##6', '##1', ',', 'o', 'percent', '##ual', 'de', '25', '%', '(', 'vin', '##te', 'e', 'ci', '##nco', 'por', 'cent', '##o', ')', 'de', 'inc', '##ent', '##ivo', 'a', 'qu', '##ali', '##fi', '##ca', '##cao', ',', 'a', 'con', '##tar', 'de', '15', '/', '07', '/', '2016', ',', 'tend', '##o', 'em', 'vista', 'a', 'con', '##cl', '##usa', '##o', 'do', 'cu', '##rso', 'de', 'gr', '##ad', '##ua', '##cao', 'em', 'ad', '##mini', '##stra', '##cao', '-', 'bach', '##are', '##lad', '##o', ',', 'conform', '##e', 'o', 'process', '##o', 'n', '##º', '230', '##7', '##8', '.', '01', '##53', '##33', '/', '2016', '-', '19', '.', '[SEP]']\n",
      "['[CLS]', 'con', '##cede', '##r', 'a', 'ser', '##vid', '##ora', 'li', '##sian', '##e', 'ramos', 'vi', '##lk', ',', 'o', '##cup', '##ante', 'do', 'cargo', 'de', 'ad', '##mini', '##stra', '##dor', 'de', 'ed', '##ific', '##ios', '-', '70', '##14', '##00', ',', 'lot', '##ada', 'na', 'fa', '##cu', '##lda', '##de', 'de', 'ar', '##qui', '##tet', '##ura', ',', 'si', '##ape', '232', '##52', '##6', '##1', ',', 'o', 'percent', '##ual', 'de', '25', '%', '(', 'vin', '##te', 'e', 'ci', '##nco', 'por', 'cent', '##o', ')', 'de', 'inc', '##ent', '##ivo', 'a', 'qu', '##ali', '##fi', '##ca', '##cao', ',', 'a', 'con', '##tar', 'de', '15', '/', '07', '/', '2016', ',', 'tend', '##o', 'em', 'vista', 'a', 'con', '##cl', '##usa', '##o', 'do', 'cu', '##rso', 'de', 'gr', '##ad', '##ua', '##cao', 'em', 'ad', '##mini', '##stra', '##cao', '-', 'bach', '##are', '##lad', '##o', ',', 'conform', '##e', 'o', 'process', '##o', 'n', '##º', '230', '##7', '##8', '.', '01', '##53', '##33', '/', '2016', '-', '19', '.', '[SEP]']\n",
      "['[CLS]', 'con', '##cede', '##r', 'à', 'se', '##r', '##vid', '##ora', 'l', '##isi', '##ane', 'ram', '##os', 'v', '##il', '##k', ',', 'o', '##cup', '##ante', 'do', 'cargo', 'de', 'ad', '##mini', '##stra', '##dor', 'de', 'ed', '##if', '##í', '##cio', '##s', '-', '70', '##14', '##00', ',', 'lot', '##ada', 'na', 'f', '##ac', '##uld', '##ade', 'de', 'a', '##r', '##qui', '##te', '##tura', ',', 'si', '##ap', '##e', '232', '##5', '##26', '##1', ',', 'o', 'percent', '##ual', 'de', '25', '%', '(', 'v', '##int', '##e', 'e', 'c', '##in', '##co', 'p', '##or', 'cent', '##o', ')', 'de', 'in', '##cent', '##ivo', 'à', 'q', '##ual', '##ific', '##a', '##ção', ',', 'a', 'con', '##tar', 'de', '15', '/', '07', '/', '2016', ',', 'tend', '##o', 'em', 'v', '##ista', 'a', 'con', '##c', '##lus', '##ão', 'do', 'cu', '##rs', '##o', 'de', 'g', '##rad', '##ua', '##ção', 'em', 'ad', '##mini', '##stra', '##ção', '-', 'b', '##ach', '##arel', '##ado', ',', 'conform', '##e', 'o', 'process', '##o', 'n', '##º', '230', '##7', '##8', '.', '01', '##53', '##33', '/', '2016', '-', '19', '.', '[SEP]']\n",
      "['[CLS]', 'con', '##cede', '##r', 'à', 'se', '##r', '##vid', '##ora', 'l', '##isi', '##ane', 'ram', '##os', 'v', '##il', '##k', ',', 'o', '##cup', '##ante', 'do', 'cargo', 'de', 'ad', '##mini', '##stra', '##dor', 'de', 'ed', '##if', '##í', '##cio', '##s', '-', '70', '##14', '##00', ',', 'lot', '##ada', 'na', 'f', '##ac', '##uld', '##ade', 'de', 'a', '##r', '##qui', '##te', '##tura', ',', 'si', '##ap', '##e', '232', '##5', '##26', '##1', ',', 'o', 'percent', '##ual', 'de', '25', '%', '(', 'v', '##int', '##e', 'e', 'c', '##in', '##co', 'p', '##or', 'cent', '##o', ')', 'de', 'in', '##cent', '##ivo', 'à', 'q', '##ual', '##ific', '##a', '##ção', ',', 'a', 'con', '##tar', 'de', '15', '/', '07', '/', '2016', ',', 'tend', '##o', 'em', 'v', '##ista', 'a', 'con', '##c', '##lus', '##ão', 'do', 'cu', '##rs', '##o', 'de', 'g', '##rad', '##ua', '##ção', 'em', 'ad', '##mini', '##stra', '##ção', '-', 'b', '##ach', '##arel', '##ado', ',', 'conform', '##e', 'o', 'process', '##o', 'n', '##º', '230', '##7', '##8', '.', '01', '##53', '##33', '/', '2016', '-', '19', '.', '[SEP]']\n",
      "['[CLS]', 'con', '##cede', '##r', 'à', 'servidor', '##a', 'li', '##sian', '##e', 'ramo', '##s', 'vil', '##k', ',', 'ocupa', '##nte', 'do', 'cargo', 'de', 'administrador', 'de', 'edifício', '##s', '-', '701', '##400', ',', 'lot', '##ada', 'na', 'fac', '##uld', '##ade', 'de', 'ar', '##quitetura', ',', 'sia', '##pe', '232', '##52', '##6', '##1', ',', 'o', 'percent', '##ual', 'de', '25', '%', '(', 'vinte', 'e', 'cinco', 'por', 'cento', ')', 'de', 'in', '##cent', '##ivo', 'à', 'quali', '##ficação', ',', 'a', 'contar', 'de', '15', '/', '07', '/', '2016', ',', 'tendo', 'em', 'vista', 'a', 'con', '##clu', '##são', 'do', 'curso', 'de', 'gradu', '##ação', 'em', 'administração', '-', 'ba', '##char', '##ela', '##do', ',', 'conforme', 'o', 'processo', 'nº', '2307', '##8', '.', '015', '##33', '##3', '/', '2016', '-', '19', '.', '[SEP]']\n",
      "['[CLS]', 'con', '##cede', '##r', 'a', 'ser', '##vid', '##ora', 'li', '##sian', '##e', 'ramos', 'vi', '##lk', ',', 'o', '##cup', '##ante', 'do', 'cargo', 'de', 'ad', '##mini', '##stra', '##dor', 'de', 'ed', '##ific', '##ios', '-', '70', '##14', '##00', ',', 'lot', '##ada', 'na', 'fa', '##cu', '##lda', '##de', 'de', 'ar', '##qui', '##tet', '##ura', ',', 'si', '##ape', '232', '##52', '##6', '##1', ',', 'o', 'percent', '##ual', 'de', '25', '%', '(', 'vin', '##te', 'e', 'ci', '##nco', 'por', 'cent', '##o', ')', 'de', 'inc', '##ent', '##ivo', 'a', 'qu', '##ali', '##fi', '##ca', '##cao', ',', 'a', 'con', '##tar', 'de', '15', '/', '07', '/', '2016', ',', 'tend', '##o', 'em', 'vista', 'a', 'con', '##cl', '##usa', '##o', 'do', 'cu', '##rso', 'de', 'gr', '##ad', '##ua', '##cao', 'em', 'ad', '##mini', '##stra', '##cao', '-', 'bach', '##are', '##lad', '##o', ',', 'conform', '##e', 'o', 'process', '##o', 'n', '##º', '230', '##7', '##8', '.', '01', '##53', '##33', '/', '2016', '-', '19', '.', '[SEP]']\n",
      "['[CLS]', 'con', '##cede', '##r', 'à', 'se', '##r', '##vid', '##ora', 'l', '##isi', '##ane', 'ram', '##os', 'v', '##il', '##k', ',', 'o', '##cup', '##ante', 'do', 'cargo', 'de', 'ad', '##mini', '##stra', '##dor', 'de', 'ed', '##if', '##í', '##cio', '##s', '-', '70', '##14', '##00', ',', 'lot', '##ada', 'na', 'f', '##ac', '##uld', '##ade', 'de', 'a', '##r', '##qui', '##te', '##tura', ',', 'si', '##ap', '##e', '232', '##5', '##26', '##1', ',', 'o', 'percent', '##ual', 'de', '25', '%', '(', 'v', '##int', '##e', 'e', 'c', '##in', '##co', 'p', '##or', 'cent', '##o', ')', 'de', 'in', '##cent', '##ivo', 'à', 'q', '##ual', '##ific', '##a', '##ção', ',', 'a', 'con', '##tar', 'de', '15', '/', '07', '/', '2016', ',', 'tend', '##o', 'em', 'v', '##ista', 'a', 'con', '##c', '##lus', '##ão', 'do', 'cu', '##rs', '##o', 'de', 'g', '##rad', '##ua', '##ção', 'em', 'ad', '##mini', '##stra', '##ção', '-', 'b', '##ach', '##arel', '##ado', ',', 'conform', '##e', 'o', 'process', '##o', 'n', '##º', '230', '##7', '##8', '.', '01', '##53', '##33', '/', '2016', '-', '19', '.', '[SEP]']\n",
      "['[CLS]', 'conceder', 'à', 'servidor', '##a', 'lis', '##iane', 'ramos', 'vil', '##k', ',', 'ocupa', '##nte', 'do', 'cargo', 'de', 'administrador', 'de', 'edifícios', '-', '70', '##14', '##00', ',', 'lo', '##tada', 'na', 'faculdade', 'de', 'arquitetura', ',', 'si', '##ap', '##e', '23', '##25', '##26', '##1', ',', 'o', 'percentual', 'de', '25', '%', '(', 'vinte', 'e', 'cinco', 'por', 'cento', ')', 'de', 'incentivo', 'à', 'qualificação', ',', 'a', 'contar', 'de', '15', '/', '07', '/', '2016', ',', 'tendo', 'em', 'vista', 'a', 'conclusão', 'do', 'curso', 'de', 'graduação', 'em', 'administração', '-', 'bacharel', '##ado', ',', 'conforme', 'o', 'processo', '[UNK]', '23', '##0', '##78', '.', '01', '##53', '##33', '/', '2016', '-', '19', '.', '[SEP]']\n",
      "['[CLS]', 'conceder', 'à', 'servidor', '##a', 'lis', '##iane', 'ramos', 'vil', '##k', ',', 'ocupa', '##nte', 'do', 'cargo', 'de', 'administrador', 'de', 'edifícios', '-', '70', '##14', '##00', ',', 'lo', '##tada', 'na', 'faculdade', 'de', 'arquitetura', ',', 'si', '##ap', '##e', '23', '##25', '##26', '##1', ',', 'o', 'percentual', 'de', '25', '%', '(', 'vinte', 'e', 'cinco', 'por', 'cento', ')', 'de', 'incentivo', 'à', 'qualificação', ',', 'a', 'contar', 'de', '15', '/', '07', '/', '2016', ',', 'tendo', 'em', 'vista', 'a', 'conclusão', 'do', 'curso', 'de', 'graduação', 'em', 'administração', '-', 'bacharel', '##ado', ',', 'conforme', 'o', 'processo', '[UNK]', '23', '##0', '##78', '.', '01', '##53', '##33', '/', '2016', '-', '19', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(sw_tkn_1.tokens())\n",
    "print(sw_tkn_2.tokens())\n",
    "print(sw_tkn_3.tokens())\n",
    "print(sw_tkn_4.tokens())\n",
    "print(sw_tkn_5.tokens())\n",
    "print(sw_tkn_6.tokens())\n",
    "print(sw_tkn_7.tokens())\n",
    "print(sw_tkn_8.tokens())\n",
    "print(sw_tkn_9.tokens())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
