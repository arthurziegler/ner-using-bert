{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import external dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from pprint import pprint\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import internal dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# current_dir = os.path.dirname(os.path.abspath('__file__'))\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# parent_dir = os.path.dirname(current_dir)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# sys.path.append(parent_dir)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# from src.data_cleaning import data_cleaning\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_cleaning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_cleaning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clear_text\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "# parent_dir = os.path.dirname(current_dir)\n",
    "# sys.path.append(parent_dir)\n",
    "# from src.data_cleaning import data_cleaning\n",
    "\n",
    "from src.data_cleaning.data_cleaning import clear_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Unannotated Documents\n",
    "We get the unnanotatted texts as key, value pairs. This will make it easy to convert them into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_test = \"../data/test/raw_documents/\"\n",
    "path_to_train = \"../data/train/raw_documents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "\n",
    "# Get train documents that require manual labeling\n",
    "for filename in os.listdir(path_to_train):\n",
    "   if filename.endswith(\".txt\"):\n",
    "      with open(os.path.join(path_to_train, filename), 'r', encoding=\"utf8\") as f:\n",
    "         key = int(filename.replace('.txt', ''))\n",
    "         value = f.read()\n",
    "         documents[key] = value\n",
    "\n",
    "# Transform documents dictionary into a pandas dataframe\n",
    "df_docs = pd.DataFrame.from_dict(documents, orient='index')\n",
    "df_docs.columns = ['text']\n",
    "\n",
    "print(df_docs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "We will use the **data_processing.clear_text** function which was built to clean the specific document we are using here. Since we already know they will be used for NER, the objective of the cleaning is getting the text into a format which will be easy to split into sentences. Each sentence will later be used as an input to our neural network.\n",
    "There are three aspects we are cleaning:\n",
    "1. Removing line breaks as they make it very hard for the sentence tokenizer to correctly recognize the begining and ending of sentences in the text\n",
    "2. Removing repeated whitespaces which are used for the visual formatting of the documents but will generate unnecessary tokens for our neural network.\n",
    "3. There are a lot of law and document numbers in this documents. There isn't a consistent writing of these numbers which can start as \"nº.123\", \"nº. 123\" and \"nº 123\". We will padronize this occurences to appear as \"nº 123\" since the punctuation after \"nº\" makes it harder for the sentence tokenizer to correctly separate the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs['text'] = df_docs['text'].apply(data_cleaning.clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence tokenization\n",
    "Having cleaned our documents we will split them into sentences. The **data_processing.split_text_sentences** function will return a DataFrame with: The sentences, a unique ID for each sentence and the index of the document each sentence is a part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = data_cleaning.split_text_sentences(df_docs['text'])\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "print(df_sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with duplicate sentences\n",
    "Now that we have our dataset of sentences we'll go to the last step which is identifying and categorizing duplicate sentences. Since we need to manually annotate the dataset for training we can group duplicate sentences to facilitate the anottation process. The document we have are fairly standardized so there are a few duplicates we will find.\n",
    "\n",
    "Let's group sentences by their textual content. Each group will be composed of identical sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_group = df_sentences.groupby('sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop each group of duplicated sentences and create a dictionary where the key will be the index of the first sentence of the group and the value is the index of all sentences equal to the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = {}\n",
    "for i in duplicate_group:\n",
    "    dup_list = i[1]['sentence_id'].tolist()\n",
    "    duplicates[dup_list[0]] = dup_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe which will contain only unique sentences. The sentences that have identic pairs will have all of their indexes in a list in the 'duplicates' column. This will allow us to replicate the labels after the anotattion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences = df_sentences.copy()\n",
    "df_unique_sentences['duplicates'] = pd.Series(duplicates, index=df_sentences.index)\n",
    "df_unique_sentences = df_unique_sentences.dropna(subset=['duplicates'])\n",
    "df_unique_sentences['label'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the unique dataframe has less sentences which represen the duplicates we removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(df_sentences.shape)\n",
    "pprint(df_unique_sentences.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Removal of Bad Sentences\n",
    "Some sentences are not going to be helpful on our final train and test data. They either have just one or no entities we are interested in. Because of that we'll remove them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(df_unique_sentences.shape)\n",
    "df_unique_sentences = df_unique_sentences[df_unique_sentences['sentence'].str.len() > 20]\n",
    "df_unique_sentences = df_unique_sentences[~df_unique_sentences['sentence'].str.startswith(\"Documento gerado sob\")]\n",
    "df_unique_sentences = df_unique_sentences[~df_unique_sentences['sentence'].str.startswith(\"Solicitação nº\")]\n",
    "df_unique_sentences = df_unique_sentences[~df_unique_sentences['sentence'].str.startswith(\"Processo\")]\n",
    "df_unique_sentences = df_unique_sentences[~df_unique_sentences['sentence'].str.contains(\"RESOLVE\")]\n",
    "pprint(df_unique_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(df_unique_sentences.to_json(lines=True, orient = 'records'))\n",
    "save_path = \"../data/unannotated/\"\n",
    "df_docs.to_json(os.path.join(save_path, 'clean_raw_documents.jsonl'),lines=True, orient = 'records')\n",
    "df_unique_sentences.to_json(os.path.join(save_path, 'unlabeled_sentences.jsonl'),lines=True, orient = 'records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assistive Auto Labelling\n",
    "Before we import our sentences into doccano for manual labelling, we'll use the auto_tagger to automatically label them. This will make it easier to do the manual labelling as a few entities will already be correctly or almost correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the auto_tagger model\n",
    "from transformers import pipeline\n",
    "assistive_auto_tagger = pipeline(\n",
    "    \"token-classification\",\n",
    "    model='auto_tagger',\n",
    "    aggregation_strategy=\"average\",\n",
    "    ignore_labels=[\"\"],\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sentences through our NER tagging model\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences['sentence'].apply(lambda x: assistive_auto_tagger(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the classified entities text to match the original sentence.\n",
    "df_unique_sentences['tagged_entities'] = df_unique_sentences.apply(lambda x: data_processing.reformat_sentence(x['sentence'], x['tagged_entities']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes a Hugging Face NER Pipeline dictionary output and formats it in a format that can be used by Doccano for manual labelling.\n",
    "# This is useful when using the NER Pipeline as an assistive auto labelling before doing the manual work.\n",
    "def ner_entity_dict_to_doccano_jsonl(tagged_entities):\n",
    "    # Join the entities texts to get the full sentence\n",
    "    full_sentence_text = \"\"\n",
    "    entities_metadata = []\n",
    "    for entity in tagged_entities:\n",
    "        full_sentence_text = full_sentence_text + entity['word'] + ' '\n",
    "        if entity['entity_group'] != 'O':\n",
    "            entities_metadata.append([entity['start'], entity['end'], entity['entity_group']])\n",
    "    full_sentence_text = re.sub(' ,', ',', full_sentence_text)\n",
    "    return full_sentence_text, entities_metadata\n",
    "\n",
    "    # Get a list of the entities in the format [START, END, TYPE] for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences['tagged_entities'].loc[2]\n",
    "print(ner_entity_dict_to_doccano_jsonl(df_unique_sentences['tagged_entities'].loc[2]))\n",
    "df_unique_sentences['label'] = df_unique_sentences.apply(lambda x: ner_entity_dict_to_doccano_jsonl(x['tagged_entities'])[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences.to_json(os.path.join(path_to_train, 'unique_sentences.jsonl'),lines=True, orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences[df_unique_sentences['sentence_id'] == 2507]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
