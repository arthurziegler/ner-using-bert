{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#External\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from pprint import pprint\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "#Internal\n",
    "import data_cleaning.data_cleaning as dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Unannotated Documents\n",
    "We get the unnanotatted texts as key, value pairs. This will make it easy to convert them into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_raw = \"../data/raw_documents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568, 1)\n"
     ]
    }
   ],
   "source": [
    "documents = {}\n",
    "\n",
    "# Get documents that require manual labeling\n",
    "for filename in os.listdir(path_to_raw):\n",
    "   if filename.endswith(\".txt\"):\n",
    "      with open(os.path.join(path_to_raw, filename), 'r', encoding=\"utf8\") as f:\n",
    "         key = int(filename.replace('.txt', ''))\n",
    "         value = f.read()\n",
    "         documents[key] = value\n",
    "\n",
    "# Transform documents dictionary into a pandas dataframe\n",
    "df_docs = pd.DataFrame.from_dict(documents, orient='index')\n",
    "df_docs.columns = ['text']\n",
    "\n",
    "print(df_docs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "We will use the **data_processing.clear_text** function which was built to clean the specific document we are using here. Since we already know they will be used for NER, the objective of the cleaning is getting the text into a format which will be easy to split into sentences. Each sentence will later be used as an input to our neural network.\n",
    "There are three aspects we are cleaning:\n",
    "1. Removing line breaks as they make it very hard for the sentence tokenizer to correctly recognize the begining and ending of sentences in the text\n",
    "2. Removing repeated whitespaces which are used for the visual formatting of the documents but will generate unnecessary tokens for our neural network.\n",
    "3. There are a lot of law and document numbers in this documents. There isn't a consistent writing of these numbers which can start as \"nº.123\", \"nº. 123\" and \"nº 123\". We will padronize this occurences to appear as \"nº 123\" since the punctuation after \"nº\" makes it harder for the sentence tokenizer to correctly separate the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs['text'] = df_docs['text'].apply(dc.clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence tokenization\n",
    "Having cleaned our documents we will split them into sentences. The **data_processing.split_text_sentences** function will return a DataFrame with: The sentences, a unique ID for each sentence and the index of the document each sentence is a part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 4)\n"
     ]
    }
   ],
   "source": [
    "df_sentences = dc.split_text_sentences(df_docs['text'])\n",
    "\n",
    "# We create a new column doc_sentence_id that serves as a unique identifier for each sentence in our dataset\n",
    "df_sentences['doc_sentence_id'] = df_sentences['document'].astype(str) + '-' + df_sentences['sentence_id'].astype(str)\n",
    "\n",
    "print(df_sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with duplicate sentences\n",
    "Now that we have our dataset of sentences we'll go to the last step which is identifying and categorizing duplicate sentences. Since we need to manually annotate the dataset for training we can group duplicate sentences to facilitate the anottation process. The document we have are very standardized so there are multiple duplicate sentences in them.\n",
    "\n",
    "Let's group sentences by their text. Each group will be composed of identical sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_group = df_sentences.groupby('sentence')\n",
    "\n",
    "# Loop each group of duplicated sentences and create a dictionary where the key will be the index of the first sentence of the group and the value is the doc_sentence_id of all sentences equal to the first.\n",
    "duplicates = {}\n",
    "for i in duplicate_group:\n",
    "    dup_list = i[1]['doc_sentence_id'].tolist()\n",
    "    duplicates[dup_list[0]] = dup_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe which will contain only unique sentences. The sentences that have identic pairs will have all of their indexes in a list in the 'duplicates' column. This will allow us to replicate the labels after the anotattion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_sentences = df_sentences.copy()\n",
    "df_unique_sentences['duplicates'] = df_unique_sentences['doc_sentence_id'].map(duplicates)\n",
    "df_unique_sentences = df_unique_sentences.dropna(subset=['duplicates'])\n",
    "df_unique_sentences['label'] = \"\"\n",
    "df_unique_sentences['tagged_entities'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the new dataframe has less sentences since each entry is a unique sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 4)\n",
      "(2070, 7)\n"
     ]
    }
   ],
   "source": [
    "pprint(df_sentences.shape)\n",
    "pprint(df_unique_sentences.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad Sentence Filter\n",
    "Some sentences are not going to be helpful on our final train and test data. They either have just one or no entities we are interested in. Because of that we'll remove them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 7)\n",
      "(768, 7)\n"
     ]
    }
   ],
   "source": [
    "pprint(df_unique_sentences.shape)\n",
    "df_unique_sentences = df_unique_sentences[df_unique_sentences['sentence'].str.len() > 20]\n",
    "df_unique_sentences = df_unique_sentences[~df_unique_sentences['sentence'].str.startswith(\"Documento gerado sob\")]\n",
    "df_unique_sentences = df_unique_sentences[~df_unique_sentences['sentence'].str.startswith(\"Solicitação nº\")]\n",
    "df_unique_sentences = df_unique_sentences[~df_unique_sentences['sentence'].str.startswith(\"Processo\")]\n",
    "df_unique_sentences = df_unique_sentences[~df_unique_sentences['sentence'].str.contains(\"RESOLVE\")]\n",
    "pprint(df_unique_sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a clean_documents file with all formatted documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(df_unique_sentences.to_json(lines=True, orient = 'records'))\n",
    "save_path = \"../data/unannotated/\"\n",
    "df_docs.to_json(os.path.join(save_path, 'clean_documents_06-12-24.jsonl'),lines=True, orient = 'records')\n",
    "df_unique_sentences.to_json(os.path.join(save_path, 'unique_sentences-06-12-24.jsonl'),lines=True, orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['document', 'sentence', 'sentence_id', 'doc_sentence_id', 'duplicates',\n",
       "       'label', 'tagged_entities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique_sentences.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update unannotated dataset\n",
    "The unique_sentences file is in the format required to start manual anotation in doccano. We'll join it with a dataset of all sentences that have already been annotated so that we can update all of them inside docanno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open annotated dataset to merge it with new unnanotated one.\n",
    "path_to_annotated = \"../data/annotated/\"\n",
    "df_annotated = pd.read_json(os.path.join(path_to_annotated, 'doccano-extraction-27-11-24.jsonl'), lines=True)\n",
    "\n",
    "#Drop the id column since we don't use it for anything\n",
    "df_annotated.drop(columns=['id'], inplace=True)\n",
    "\n",
    "#Rename columns in both datasets so they match\n",
    "df_annotated.columns = ['text', 'document_id', 'sentence_id', 'duplicate_ids', 'tagged_entities', 'label']\n",
    "df_unique_sentences.columns = ['document_id', 'text', 'sentence_id', 'doc_sentence_id', 'duplicate_ids', 'label', 'tagged_entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge both datasets\n",
    "We do an outer join to take all rows and columns from both datasets.\n",
    "\n",
    "We'll keep the **text**, **tagged_entities** and **label** columns from the annotated dataset as they have the most complete information on those (have already been tagged).\n",
    "\n",
    "The **document_id**, **sentence_id**, **doc_sentence_id** and **duplicate_ids** are updated when new unnanotated documents are added, so we'll keep the unannotated dataset columns for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_unannotated = df = pd.merge(df_annotated, df_unique_sentences, on='text', how='outer')\n",
    "\n",
    "full_unannotated = full_unannotated[['text', 'tagged_entities_x', 'label_x', 'document_id_y', 'sentence_id_y', 'doc_sentence_id', 'duplicate_ids_y']]\n",
    "full_unannotated.columns = ['text', 'tagged_entities', 'label', 'document_id', 'sentence_id', 'doc__sentence_id', 'duplicate_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tagged_entities</th>\n",
       "      <th>label</th>\n",
       "      <th>document_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc__sentence_id</th>\n",
       "      <th>duplicate_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conceder à servidora LISIANE RAMOS VILK, ocupa...</td>\n",
       "      <td>[{'entity_group': 'O', 'score': 0.7762932777, ...</td>\n",
       "      <td>[[21, 39, PERSON], [62, 88, OCCUPATION], [109,...</td>\n",
       "      <td>25644</td>\n",
       "      <td>2</td>\n",
       "      <td>25644-2</td>\n",
       "      <td>[25644-2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[{'entity_group': 'PERSON', 'score': 0.6592996...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, OCCUPATION]]</td>\n",
       "      <td>105798</td>\n",
       "      <td>3</td>\n",
       "      <td>105798-3</td>\n",
       "      <td>[105798-3, 105799-3, 105801-3, 105802-3, 18001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autorizar o afastamento do país de CRISTINE MA...</td>\n",
       "      <td>[{'entity_group': 'O', 'score': 0.6819901466, ...</td>\n",
       "      <td>[[35, 58, PERSON], [60, 92, OCCUPATION], [119,...</td>\n",
       "      <td>25645</td>\n",
       "      <td>2</td>\n",
       "      <td>25645-2</td>\n",
       "      <td>[25645-2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CARLOS ALEXANDRE NETTO Reitor</td>\n",
       "      <td>[{'entity_group': 'PERSON', 'score': 0.6323550...</td>\n",
       "      <td>[[0, 22, PERSON], [23, 29, OCCUPATION]]</td>\n",
       "      <td>25645</td>\n",
       "      <td>4</td>\n",
       "      <td>25645-4</td>\n",
       "      <td>[25645-4, 25646-4, 25647-4, 25648-4, 25649-4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Autorizar o afastamento do país de ANDRE DIAS ...</td>\n",
       "      <td>[{'entity_group': 'O', 'score': 0.6808940768, ...</td>\n",
       "      <td>[[35, 53, PERSON], [55, 82, OCCUPATION], [109,...</td>\n",
       "      <td>25646</td>\n",
       "      <td>2</td>\n",
       "      <td>25646-2</td>\n",
       "      <td>[25646-2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Conceder à servidora LISIANE RAMOS VILK, ocupa...   \n",
       "1  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "2  Autorizar o afastamento do país de CRISTINE MA...   \n",
       "3                      CARLOS ALEXANDRE NETTO Reitor   \n",
       "4  Autorizar o afastamento do país de ANDRE DIAS ...   \n",
       "\n",
       "                                     tagged_entities  \\\n",
       "0  [{'entity_group': 'O', 'score': 0.7762932777, ...   \n",
       "1  [{'entity_group': 'PERSON', 'score': 0.6592996...   \n",
       "2  [{'entity_group': 'O', 'score': 0.6819901466, ...   \n",
       "3  [{'entity_group': 'PERSON', 'score': 0.6323550...   \n",
       "4  [{'entity_group': 'O', 'score': 0.6808940768, ...   \n",
       "\n",
       "                                               label  document_id  \\\n",
       "0  [[21, 39, PERSON], [62, 88, OCCUPATION], [109,...        25644   \n",
       "1            [[0, 24, PERSON], [25, 56, OCCUPATION]]       105798   \n",
       "2  [[35, 58, PERSON], [60, 92, OCCUPATION], [119,...        25645   \n",
       "3            [[0, 22, PERSON], [23, 29, OCCUPATION]]        25645   \n",
       "4  [[35, 53, PERSON], [55, 82, OCCUPATION], [109,...        25646   \n",
       "\n",
       "   sentence_id doc__sentence_id  \\\n",
       "0            2          25644-2   \n",
       "1            3         105798-3   \n",
       "2            2          25645-2   \n",
       "3            4          25645-4   \n",
       "4            2          25646-2   \n",
       "\n",
       "                                       duplicate_ids  \n",
       "0                                          [25644-2]  \n",
       "1  [105798-3, 105799-3, 105801-3, 105802-3, 18001...  \n",
       "2                                          [25645-2]  \n",
       "3  [25645-4, 25646-4, 25647-4, 25648-4, 25649-4, ...  \n",
       "4                                          [25646-2]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_unannotated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a unique_sentences file with all formatted sentences\n",
    "This file is **ready** to be imported in doccano for manual anotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_unannotated.to_json(os.path.join(save_path, 'unique_sentences-09-12-24.jsonl'),lines=True, orient = 'records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
