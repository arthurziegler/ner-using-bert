{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_processing\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "#import tokenizer from nltk that is able to separate text into sentences and tokens.\n",
    "from nltk import tokenize, download\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\arthu\\\\Desktop\\\\TCC\\\\ner-using-bert\\\\BERT_Experiment'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import SpaCy anottated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"acerpi_dataset/train/annotated/\"\n",
    "test_file_path = \"acerpi_dataset/test/annotated/\"\n",
    "train_data = pd.read_json(train_file_path + \"ner_ufrgs_correct_correct.jsonl\", lines=True)\n",
    "test_data = pd.read_json(test_file_path + \"ner_ufrgs_test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting SpaCy anottations\n",
    "The first thing we'll do is to extract a list of labeled tokens. When importing from the spacy anotattion, the labels are not formatted by token, so we will need to go through the *spans* columns and the *tokens* columns to change the labelling from by document to be by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Documento', 'start': 0, 'end': 9, 'id': 0},\n",
       " {'text': 'gerado', 'start': 10, 'end': 16, 'id': 1}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Documento',\n",
       "  'start': 0,\n",
       "  'end': 9,\n",
       "  'id': 0,\n",
       "  'tag': 'O',\n",
       "  'document': 1},\n",
       " {'text': 'gerado',\n",
       "  'start': 10,\n",
       "  'end': 16,\n",
       "  'id': 1,\n",
       "  'tag': 'O',\n",
       "  'document': 1}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"max_colwidth\", 400)\n",
    "display(train_data['tokens'][1][0:2])\n",
    "train_data, spacy_tokens = data_processing.extract_spacy_tokens(train_data)\n",
    "display(train_data['tokens'][1][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3053, 3)\n",
      "(2910, 3)\n"
     ]
    }
   ],
   "source": [
    "# There are instances of tokens with \\n and a whitespace, we want to remove those because they won't be present in our cleaned text later on.\n",
    "print(spacy_tokens.shape)\n",
    "spacy_tokens = spacy_tokens[spacy_tokens['text'].str.fullmatch(r'([A-zÀ-ÿ]+)')]\n",
    "print(spacy_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find repeated entities in spacy_tokens and remove them to have a more compact dataframe\n",
    "token_groups = spacy_tokens.groupby(by='entity_id')\n",
    "n_of_entities = int(spacy_tokens['entity_id'].max())\n",
    "duplicate_entities = []\n",
    "for entity_id in range(n_of_entities+1):\n",
    "    named_entity = token_groups.get_group(entity_id).reset_index(drop=True)[['text', 'tag']]\n",
    "    for cmp_index in range(entity_id + 1, n_of_entities+1):\n",
    "        other_entity = token_groups.get_group(cmp_index).reset_index(drop=True)[['text', 'tag']]\n",
    "        if named_entity.equals(other_entity):\n",
    "            duplicate_entities += (list(token_groups.get_group(cmp_index).index.values))\n",
    "\n",
    "spacy_tokens = spacy_tokens.drop(index=duplicate_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1680, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>entity_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>GLAISON</td>\n",
       "      <td>PER</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>AUGUSTO</td>\n",
       "      <td>PER</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>GUERRERO</td>\n",
       "      <td>PER</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>RUI</td>\n",
       "      <td>PER</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>VICENTE</td>\n",
       "      <td>PER</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>OPPERMANN</td>\n",
       "      <td>PER</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>LUCIOLA</td>\n",
       "      <td>PER</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>CAMPESTRINI</td>\n",
       "      <td>PER</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>JANE</td>\n",
       "      <td>PER</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>FRAGA</td>\n",
       "      <td>PER</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  tag  entity_id\n",
       "94       GLAISON  PER        0.0\n",
       "95       AUGUSTO  PER        0.0\n",
       "96      GUERRERO  PER        0.0\n",
       "210          RUI  PER        1.0\n",
       "211      VICENTE  PER        1.0\n",
       "212    OPPERMANN  PER        1.0\n",
       "312      LUCIOLA  PER        2.0\n",
       "313  CAMPESTRINI  PER        2.0\n",
       "425         JANE  PER        3.0\n",
       "426        FRAGA  PER        3.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spacy_tokens.shape)\n",
    "spacy_tokens.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre processing\n",
    "Now that we have a list with all tokens that were classified as a **Named Entity** by SpaCy we need to clean our text data to get better inputs and a cleaner list of tokens for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_text = data_processing.clear_text(train_data['text'])\n",
    "type(clean_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = data_processing.split_text_sentences(clean_train_text)\n",
    "\n",
    "# Add a 'tokens' column tokenzing the 'sentence' column\n",
    "train_sentences['tokens'] = train_sentences.apply(lambda row: tokenize.word_tokenize(row['sentence'], language='portuguese'), axis=1)\n",
    "train_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the final dataframe where each row will be a token and the columns will be 'Sentence', 'Word' and 'Tag'.\n",
    "train_tokens = train_sentences.explode('tokens').drop(columns=['document', 'sentence']).reset_index(drop=True)\n",
    "train_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_tokens = data_processing.find_labeled_entitites(train_tokens, spacy_tokens)\n",
    "train_labeled_tokens.columns = ['Sentence #', 'Word', 'Tag']\n",
    "train_labeled_tokens[train_labeled_tokens['Tag'] == 'PER'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_tokens = data_processing.apply_iob_format(train_labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a capitalization for all word that belong to a named entity\n",
    "# This is important since the pre-trained models all have names that follow this format. Thus having all letters being uppercase gives us very bad results using BERT.\n",
    "token_final['Word'] = token_final.apply(lambda row: row['Word'].capitalize() if row['Tag'] != 'O' else row['Word'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_final.to_csv(train_file_path + 'acerpi_ner_ufrgs_train_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('NLP22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "228764ee21566ea5d9101d690a7d226c86c8b64251894d53614cd4c162a99691"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
