{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Pipeline\n",
    "## Training Steps\n",
    "1. Load Dataset\n",
    "2. Data Tokenization\n",
    "3. Model Parametrization\n",
    "4. Model Training\n",
    "5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: c:\\Users\\arthu\\Documents\\GitHub\\ner-using-bert\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "print(\"Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One \n",
    "# Load Dataset\n",
    "We are using datasets annotated in [Doccano](https://github.com/doccano/doccano), which outputs a **.jsonl** file with a list of all the documents and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"BERT_Experiment/acerpi_dataset/train/annotated\"\n",
    "dataset_filename = \"labeled_ufrgs_sentences.jsonl\"\n",
    "\n",
    "ufrgs_documents = pd.read_json(os.path.join(data_file_path, dataset_filename), orient='record', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the dataset to remove all sentences without relevant labels besides 'O'\n",
    "ufrgs_documents = ufrgs_documents[ufrgs_documents['label'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the relevant columns: the sentence text, the labels and the id to link back to the original document later on.\n",
    "ufrgs_labeled_sentences = ufrgs_documents[['text', 'label', 'sentence_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Two\n",
    "# Data Tokenization\n",
    "Let's tokenize the sentences and tokens using the TreebankWordTokenizer.  \n",
    "\n",
    "The first method **tokenize** will split our sentence and return a list of words which we'll organize in a 'tokenized_sentences' list.  \n",
    "\n",
    "The second method **span_tokenize** will return the first and last indices of the characters in each token in reference to the orignal sentence.  \n",
    "This allows us to recreate the sentence later on. We'll organize them in a 'token_positions' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = 'neuralmind/bert-large-portuguese-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer(ufrgs_labeled_sentences['text'][3], is_split_into_words=False)\n",
    "word_ids = t.word_ids()\n",
    "tokens_by_word_id = {}\n",
    "\n",
    "for idx, word in enumerate(word_ids):\n",
    "    if word is not None:\n",
    "        tokens_by_word_id.setdefault(word, []).append(t.tokens()[idx])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CARLOS ALEXANDRE NETTO Reitor'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufrgs_labeled_sentences['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['CA', '##R', '##LO', '##S'],\n",
       " 1: ['AL', '##E', '##X', '##AN', '##DR', '##E'],\n",
       " 2: ['N', '##ET', '##TO'],\n",
       " 3: ['Rei', '##tor']}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_by_word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['car', '##los'],\n",
       " 1: ['ale', '##xa', '##nd', '##re'],\n",
       " 2: ['ne', '##tto'],\n",
       " 3: ['reitor']}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_by_word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "token_positions = []\n",
    "for idx, sentence in annotated_sentences['text'].items():\n",
    "    tokenized_sentences.append(TreebankWordTokenizer().tokenize(sentence))\n",
    "    token_positions.append(list(TreebankWordTokenizer().span_tokenize(sentence)))\n",
    "\n",
    "\n",
    "print(tokenized_sentences[0])\n",
    "print(token_positions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
