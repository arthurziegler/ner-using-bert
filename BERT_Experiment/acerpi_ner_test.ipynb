{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurziegler/ner-using-bert/blob/main/BERT_Experiment/bert_ner_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoPwXN9O3c9E"
      },
      "source": [
        "# First test of the BERT model for NER\n",
        "Using [an implementation by Tobias Sterbak](https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/) as a base for the tests of the application of bert in this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL4p0B_dQUYz",
        "outputId": "b75c795c-3d0d-4976-db19-beca16a758cf"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G-zeW31W02th",
        "outputId": "a0a05349-6f10-4e41-e0fe-2dcf5dcd0739"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os.path\n",
        "\n",
        "torch.__version__\n",
        "\n",
        "pd.options.display.max_colwidth = 300\n",
        "pd.options.display.max_rows = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get ACERPI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #import tokenizer from nltk that is able to separate text into sentences and tokens.\n",
        "# from nltk import tokenize, download\n",
        "# download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#annotated_data = pd.read_json(\"acerpi_dataset/train/annotated/new_file????\", lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting SpaCy anottations\n",
        "The first thing we'll do is to extract a list of labeled tokens. When importing from the spacy anotattion, the labels are not formatted by token, so we will need to go through the *spans* columns and the *tokens* columns to change the labelling from by document to be by token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# entity_id = 0\n",
        "\n",
        "# #Loop through each document\n",
        "# for index, document in annotated_data.iterrows():\n",
        "#     entity_tokens = {}\n",
        "\n",
        "#     #In each entity on the 'spans' column we populate a dictionary with the id of the tokens and the entity they belong to\n",
        "#     for entity in document['spans']:\n",
        "#         for token_id in range(entity['token_start'], entity['token_end'] + 1, 1):\n",
        "#             entity_tokens[token_id] = entity_id\n",
        "#         entity_id += 1\n",
        "\n",
        "#     #For each token in the 'tokens' column we give them the PER tag if they belong to an entity and tag the ID of the entity they belong to. We also tag the document ID.    \n",
        "#     for token in document['tokens']:\n",
        "#         if token['id'] in entity_tokens:\n",
        "#             token['tag'] = 'PER'\n",
        "#             token['entity_id'] = entity_tokens[token['id']]\n",
        "#         else:\n",
        "#             token['tag'] = 'O'\n",
        "\n",
        "#         token['document'] = index\n",
        "\n",
        "# print(entity_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# annotated_data.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flatten array of lists\n",
        "The 'tokens' columns is currently in a format where each row contain a list of dictionaries. Since we want a single list of token we need to flatten it so that we only have a single list of dictionaries that contains the tokens from every single document of the dataset. We will not lose the document information since we added it as a key-value pair of the dictionary before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tagged_tokens = annotated_data['tokens'].to_numpy()\n",
        "\n",
        "# token_dict_list = []\n",
        "# for index, dict_list in enumerate(tagged_tokens):\n",
        "#     token_dict_list += dict_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(type(tagged_tokens))\n",
        "# print(type(token_dict_list))\n",
        "# token_dict_list[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we transform the list of dictionaries into a dataframe and each column will be a single anotated token. We only care about the tokens that were anotatted as 'PER' by the SpaCy library since we still want to clean the rest of the document and the other tokens might change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pd.options.display.max_rows = None\n",
        "# token_df = pd.DataFrame(token_dict_list)\n",
        "\n",
        "# token_df = token_df[token_df['tag'] == 'PER'].drop(columns = ['start', 'end', 'id', 'document'])\n",
        "# token_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # There are two instances of tokens with \\n and a whitespace, we want to remove those because they won't be present in our cleaned text later on.\n",
        "# print(token_df.shape)\n",
        "# token_df = token_df[token_df['text'].str.fullmatch(r'([A-zÀ-ÿ]+)')]\n",
        "# print(token_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Find repeated entities in token_df and remove them to have a more compact dataframe\n",
        "# token_groups = token_df.groupby(by='entity_id')\n",
        "# n_of_entities = int(token_df['entity_id'].max())\n",
        "# duplicate_entities = []\n",
        "# for entity_id in range(n_of_entities+1):\n",
        "#     named_entity = token_groups.get_group(entity_id).reset_index(drop=True)[['text', 'tag']]\n",
        "#     for cmp_index in range(entity_id + 1, n_of_entities+1):\n",
        "#         other_entity = token_groups.get_group(cmp_index).reset_index(drop=True)[['text', 'tag']]\n",
        "#         if named_entity.equals(other_entity):\n",
        "#             duplicate_entities += (list(token_groups.get_group(cmp_index).index.values))\n",
        "\n",
        "# #print(duplicate_entities)\n",
        "# token_df = token_df.drop(index=duplicate_entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(token_df.shape)\n",
        "# token_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean the text\n",
        "The document text contains several things like repeated spaces, new line character and wrong punctuation that could break our sentence split afterwards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def clear_text(text_input_series):\n",
        "#     clean_text = text_input_series.copy()\n",
        "#     #print(\"Document BEFORE pre-processing\" + '\\n--------------------------------\\n' + clean_text[1])\n",
        "#     # #Remove \\n character and repeated whitespaces\n",
        "#     clean_text = clean_text.replace(r'(nº\\.) ?','nº ', regex=True)\n",
        "#     clean_text = clean_text.replace(r'\\n',' ', regex=True)\n",
        "#     clean_text = clean_text.replace(r'[ ]+', ' ', regex = True)\n",
        "#     #print(\"Document AFTER pre-processing\" + '\\n--------------------------------\\n' + clean_text[1])\n",
        "#     return clean_text\n",
        " \n",
        "#clean_text = clear_text(annotated_data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# type(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # def split_text_sentences(text_input_series):\n",
        "# #     # #Split documents into phrases\n",
        "# #     sentence_df = text_input_series.apply(lambda row: tokenize.sent_tokenize(row, language='portuguese'))\n",
        "\n",
        "# #     # #Turn series of list of phrases into series of phrases. Index will indicate which document it belongs to for now.\n",
        "# #     sentence_df = sentence_df.explode()\n",
        "# #     sentence_df = pd.DataFrame(sentence_df)\n",
        "    \n",
        "# #     # Put sentece_df index as a column and the sentence id as another column\n",
        "# #     sentence_df = sentence_df.reset_index()\n",
        "# #     sentence_df['SentenceID'] = sentence_df.index\n",
        "# #     sentence_df.columns = ['document', 'sentence', 'sentence_id']\n",
        "# #     sentence_df.head()\n",
        "\n",
        "# sentence_df = data_cleaning.split_text_sentences(input_text_series????)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Add a 'tokens' column tokenzing the 'sentence' column\n",
        "# sentence_df['tokens'] = sentence_df.apply(lambda row: tokenize.word_tokenize(row['sentence'], language='portuguese'), axis=1)\n",
        "# sentence_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Format the final dataframe where each row will be a token and the columns will be 'Sentence', 'Word' and 'Tag'.\n",
        "# token_input_df = sentence_df.explode('tokens').drop(columns=['document', 'sentence']).reset_index(drop=True)\n",
        "# token_input_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# token_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Anotate token_input_df based on the labels in the token_df dataframe we got from spacy.\n",
        "# def labeled_entities(tokens, labeled_entities):\n",
        "#     tokens['label'] = 'O'\n",
        "#     for entity in range(int(labeled_entities['entity_id'].max())):\n",
        "#         matched_tokens = []\n",
        "#         entity = list(labeled_entities['text'][labeled_entities['entity_id'] == entity])#.iterrows()\n",
        "#         if entity:\n",
        "#             #print('\\nEntitiy Token List:\\n', entity)\n",
        "#             #print(len(entity))\n",
        "#             first_tokens = tokens[tokens['tokens'] == entity[0]]\n",
        "#             #print(first_tokens)\n",
        "#             first_index_list = first_tokens.index.values\n",
        "#             #print(first_index_list)\n",
        "#             #print('\\nPossible entity matches:')\n",
        "#             for index in first_index_list:\n",
        "#                 #print(list(tokens['tokens'].iloc[index:index + len(entity)]))\n",
        "#                 found_token_list = list(tokens['tokens'].iloc[index:index + len(entity)])\n",
        "#                 #print(found_token_list == entity)\n",
        "#                 if found_token_list == entity:\n",
        "#                     tokens['label'].iloc[index:index + len(entity)] = 'PER'\n",
        "#     return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# token_final = labeled_entities(token_input_df, token_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# token_final.columns = ['Sentence #', 'Word', 'Tag']\n",
        "# token_final[token_final['Tag'] == 'PER'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def apply_iob_format(token_df):\n",
        "#     for token_idx in range(token_df.shape[0]):\n",
        "#         if token_df['Tag'].iloc[token_idx] == 'PER':\n",
        "#             if token_df['Tag'].iloc[token_idx-1] == 'O':\n",
        "#                 token_df['Tag'].iloc[token_idx] = 'B-per'\n",
        "#             else:\n",
        "#                 token_df['Tag'].iloc[token_idx] = 'I-per'\n",
        "#     return token_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# token_final = apply_iob_format(token_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# token_final['Word'] = token_final.apply(lambda row: row['Word'].capitalize() if row['Tag'] != 'O' else row['Word'], axis=1)#[row['Tag'] != 'O'].str.capitalize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# token_final[token_final['Tag'] != 'O'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_file_path = \"acerpi_dataset/train/annotated/\"\n",
        "test_file_path = \"acerpi_dataset/test/annotated/\"\n",
        "\n",
        "train_data = pd.read_csv(train_file_path + 'acerpi_ner_ufrgs_train_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Documento</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>gerado</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>sob</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>autenticação</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nº</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentence #          Word Tag\n",
              "0           0     Documento   O\n",
              "1           0        gerado   O\n",
              "2           0           sob   O\n",
              "3           0  autenticação   O\n",
              "4           0            Nº   O"
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(63835, 3)"
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(640,)\n",
            "(1239,)\n"
          ]
        }
      ],
      "source": [
        "print(train_data['Sentence #'][train_data['Tag'] != 'O'].unique().shape)\n",
        "print(train_data['Sentence #'].unique().shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, our dataset has a lot of data from sentences that have no named entities. Using this sentences would mean training our model with data that doesn't contribute to our goal of classifing named entities, since they will only be an example of how to classify word that are not in one.\n",
        "\n",
        "Because of this we will filter our dataset so that it only contains sentences which have at least on named entity in them. This will ensure that during training phase we are tuning the model to find the named entities inside sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner_sentences = train_data['Sentence #'][train_data['Tag'] != 'O'].unique()\n",
        "train_data = train_data[train_data['Sentence #'].isin(ner_sentences)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(48604, 3)"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bert Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "wu6R9mV3Olnj"
      },
      "outputs": [],
      "source": [
        "class SentenceGetter(object):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        print(\"a\")\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           #s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        print(\"h\")\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "6WqZWzHhPj88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a\n"
          ]
        }
      ],
      "source": [
        "getter = SentenceGetter(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('1/1', 'O'),\n",
              " ('PORTARIA', 'O'),\n",
              " ('Nº', 'O'),\n",
              " ('6839', 'O'),\n",
              " ('de', 'O'),\n",
              " ('02/09/2016', 'O'),\n",
              " ('O', 'O'),\n",
              " ('VICE-REITOR', 'O'),\n",
              " (',', 'O'),\n",
              " ('NO', 'O')]"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "getter.sentences[0][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXvAmvAxPmN9",
        "outputId": "3c781f5c-65ae-46b2-8353-894c28ea2bc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1/1',\n",
              " 'PORTARIA',\n",
              " 'Nº',\n",
              " '6839',\n",
              " 'de',\n",
              " '02/09/2016',\n",
              " 'O',\n",
              " 'VICE-REITOR',\n",
              " ',',\n",
              " 'NO']"
            ]
          },
          "execution_count": 212,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
        "sentences[0][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1/1',\n",
              " 'PORTARIA',\n",
              " 'Nº',\n",
              " '6495',\n",
              " 'de',\n",
              " '20/07/2017',\n",
              " 'A',\n",
              " 'VICE-REITORA',\n",
              " ',',\n",
              " 'NO']"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences[2][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AENL2u8YPrHb",
        "outputId": "25c54476-03e9-46ae-e56f-e525337970ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "labels = [[s[1] for s in sentence] for sentence in getter.sentences]\n",
        "print(labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "1x0BdhlePv5C"
      },
      "outputs": [],
      "source": [
        "tag_values = list(set(train_data[\"Tag\"].values))\n",
        "tag_values.append(\"PAD\")\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['O', 'B-per', 'I-per', 'PAD']\n",
            "{'O': 0, 'B-per': 1, 'I-per': 2, 'PAD': 3}\n"
          ]
        }
      ],
      "source": [
        "print(tag_values)\n",
        "print(tag2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "7ZgEmSrdPxuQ"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 75\n",
        "bs = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "G5SfQrBqQnaE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 217,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VEQexw6WQptM",
        "outputId": "32349098-d06e-4478-f7f1-0f3a3ea92cc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA GeForce GTX 1070 Ti'"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "L--802t0QrIt"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "FrNxug4fR9ML"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "640\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "8Uq1QNj5SCpP"
      },
      "outputs": [],
      "source": [
        "tokenized_texts_and_labels = [\n",
        "    tokenize_and_preserve_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences, labels)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "nhW8WM7XSERW"
      },
      "outputs": [],
      "source": [
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
        "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "R7OzSHJESGJF"
      },
      "outputs": [],
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ma', '##ur', '##í', '##cio', 'V', '##i', '##é', '##gas', 'Da', 'Silva', 'P', '##r', '##ó', '-', 'Re', '##itor', 'de', 'G', '##est', '##ão', 'de', 'P', '##ess', '##oa', '##s']\n",
            "[ 7085  2149  6212  8174   159  1182  2744 11305 10136 11211   153  1197\n",
            "  7774   118 11336 15419  1260   144  2556  9290  1260   153  5800 12985\n",
            "  1116     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_texts[600])\n",
        "print(input_ids[600])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "uNuKJgXqSHiX"
      },
      "outputs": [],
      "source": [
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "cK7ZAzyTSI5y"
      },
      "outputs": [],
      "source": [
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "xLjUGUMLSKdz"
      },
      "outputs": [],
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [],
      "source": [
        "tr_txt, val_txt, _, _ = train_test_split(tokenized_texts, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 243,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_tags[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  122,   120,   122,   153,  9565,  9159, 20595,  1592,   151,\n",
              "       28174,  9377, 23124,  1260,  1476,   120,  1367,   120,  1446,\n",
              "         152, 11629, 28191,   118,   155, 27514, 18082,  2069, 18581,\n",
              "         100, 18581,   153,  9919, 23161, 10719,   141,  1592,  7414,\n",
              "       26140,  8900,  9949, 14569,  2036,   143, 10069,  9637, 12507,\n",
              "         141,  2346,   155, 19368,   144,  9664, 16769,  2036,   141,\n",
              "        2346,   156,  2591,  2162,   117,  1185,  1366,  1186,  1260,\n",
              "       28117,  2225,  1120,  2047,  7925,  1182, 28201, 28207,  1279,\n",
              "       15027,   181,  4638])"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_inputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1',\n",
              " '/',\n",
              " '1',\n",
              " 'P',\n",
              " '##OR',\n",
              " '##TA',\n",
              " '##RI',\n",
              " '##A',\n",
              " 'N',\n",
              " '##º',\n",
              " '104',\n",
              " '##34',\n",
              " 'de',\n",
              " '30',\n",
              " '/',\n",
              " '12',\n",
              " '/',\n",
              " '2016',\n",
              " 'O',\n",
              " 'PR',\n",
              " '##Ó',\n",
              " '-',\n",
              " 'R',\n",
              " '##EI',\n",
              " '##TO',\n",
              " '##R',\n",
              " 'DE',\n",
              " '[UNK]',\n",
              " 'DE',\n",
              " 'P',\n",
              " '##ES',\n",
              " '##SO',\n",
              " '##AS',\n",
              " 'D',\n",
              " '##A',\n",
              " 'UN',\n",
              " '##IVE',\n",
              " '##RS',\n",
              " '##ID',\n",
              " '##AD',\n",
              " '##E',\n",
              " 'F',\n",
              " '##ED',\n",
              " '##ER',\n",
              " '##AL',\n",
              " 'D',\n",
              " '##O',\n",
              " 'R',\n",
              " '##IO',\n",
              " 'G',\n",
              " '##RA',\n",
              " '##ND',\n",
              " '##E',\n",
              " 'D',\n",
              " '##O',\n",
              " 'S',\n",
              " '##U',\n",
              " '##L',\n",
              " ',',\n",
              " 'no',\n",
              " 'us',\n",
              " '##o',\n",
              " 'de',\n",
              " 'su',\n",
              " '##as',\n",
              " 'at',\n",
              " '##ri',\n",
              " '##bu',\n",
              " '##i',\n",
              " '##ç',\n",
              " '##õ',\n",
              " '##es',\n",
              " 'que',\n",
              " 'l',\n",
              " '##he',\n",
              " 'for',\n",
              " '##am',\n",
              " 'con',\n",
              " '##fer',\n",
              " '##idas',\n",
              " 'p',\n",
              " '##ela',\n",
              " 'Port',\n",
              " '##aria',\n",
              " 'n',\n",
              " '##º',\n",
              " '76',\n",
              " '##8',\n",
              " '##4',\n",
              " ',',\n",
              " 'de',\n",
              " '03',\n",
              " 'de',\n",
              " 'out',\n",
              " '##ub',\n",
              " '##ro',\n",
              " 'de',\n",
              " '2016',\n",
              " ',',\n",
              " 'do',\n",
              " 'Ma',\n",
              " '##gn',\n",
              " '##í',\n",
              " '##fic',\n",
              " '##o',\n",
              " 'Re',\n",
              " '##itor',\n",
              " ',',\n",
              " 'e',\n",
              " 'conform',\n",
              " '##e',\n",
              " 'a',\n",
              " 'Sol',\n",
              " '##icit',\n",
              " '##a',\n",
              " '##ção',\n",
              " 'de',\n",
              " 'F',\n",
              " '##é',\n",
              " '##ria',\n",
              " '##s',\n",
              " 'n',\n",
              " '##°',\n",
              " '##26',\n",
              " '##4',\n",
              " '##9',\n",
              " '##7',\n",
              " ',',\n",
              " 'R',\n",
              " '##ES',\n",
              " '##OL',\n",
              " '##VE',\n",
              " 'Design',\n",
              " '##ar',\n",
              " ',',\n",
              " 'tempo',\n",
              " '##rar',\n",
              " '##iam',\n",
              " '##ente',\n",
              " ',',\n",
              " 'no',\n",
              " '##s',\n",
              " 'term',\n",
              " '##os',\n",
              " 'da',\n",
              " 'Le',\n",
              " '##i',\n",
              " 'n',\n",
              " '##º',\n",
              " '8',\n",
              " '.',\n",
              " '112',\n",
              " ',',\n",
              " 'de',\n",
              " '11',\n",
              " 'de',\n",
              " 'de',\n",
              " '##ze',\n",
              " '##mb',\n",
              " '##ro',\n",
              " 'de',\n",
              " '1990',\n",
              " ',',\n",
              " 'com',\n",
              " 'red',\n",
              " '##a',\n",
              " '##ção',\n",
              " 'dad',\n",
              " '##a',\n",
              " 'p',\n",
              " '##ela',\n",
              " 'Le',\n",
              " '##i',\n",
              " 'n',\n",
              " '##º',\n",
              " '9',\n",
              " '.',\n",
              " '52',\n",
              " '##7',\n",
              " ',',\n",
              " 'de',\n",
              " '10',\n",
              " 'de',\n",
              " 'de',\n",
              " '##ze',\n",
              " '##mb',\n",
              " '##ro',\n",
              " 'de',\n",
              " '1997',\n",
              " ',',\n",
              " 'a',\n",
              " 'o',\n",
              " '##cup',\n",
              " '##ante',\n",
              " 'do',\n",
              " 'cargo',\n",
              " 'de',\n",
              " 'T',\n",
              " '##É',\n",
              " '##C',\n",
              " '##NI',\n",
              " '##CO',\n",
              " 'E',\n",
              " '##M',\n",
              " 'CO',\n",
              " '##NT',\n",
              " '##AB',\n",
              " '##IL',\n",
              " '##ID',\n",
              " '##AD',\n",
              " '##E',\n",
              " ',',\n",
              " 'do',\n",
              " 'Q',\n",
              " '##uad',\n",
              " '##ro',\n",
              " 'de',\n",
              " 'P',\n",
              " '##ess',\n",
              " '##oa',\n",
              " '##l',\n",
              " 'des',\n",
              " '##ta',\n",
              " 'Universidad',\n",
              " '##e',\n",
              " ',',\n",
              " 'Evan',\n",
              " '##ise',\n",
              " 'Da',\n",
              " 'Silva',\n",
              " 'Ramos',\n",
              " '(',\n",
              " 'Si',\n",
              " '##ap',\n",
              " '##e',\n",
              " ':',\n",
              " '1887',\n",
              " '##11',\n",
              " '##5',\n",
              " ')',\n",
              " ',',\n",
              " 'para',\n",
              " 'sub',\n",
              " '##st',\n",
              " '##it',\n",
              " '##ui',\n",
              " '##r',\n",
              " 'T',\n",
              " '##ia',\n",
              " '##go',\n",
              " 'Carr',\n",
              " '##ard',\n",
              " 'Ce',\n",
              " '##nt',\n",
              " '##uria',\n",
              " '##o',\n",
              " '(',\n",
              " 'Si',\n",
              " '##ap',\n",
              " '##e',\n",
              " ':',\n",
              " '118',\n",
              " '##46',\n",
              " '##26',\n",
              " ')',\n",
              " ',',\n",
              " 'G',\n",
              " '##ere',\n",
              " '##nte',\n",
              " 'Ad',\n",
              " '##mini',\n",
              " '##stra',\n",
              " '##ti',\n",
              " '##vo',\n",
              " 'do',\n",
              " 'Instituto',\n",
              " 'de',\n",
              " 'P',\n",
              " '##es',\n",
              " '##quis',\n",
              " '##as',\n",
              " 'Hi',\n",
              " '##dr',\n",
              " '##á',\n",
              " '##uli',\n",
              " '##cas',\n",
              " ',',\n",
              " 'C',\n",
              " '##ó',\n",
              " '##digo',\n",
              " 'F',\n",
              " '##G',\n",
              " '-',\n",
              " '1',\n",
              " ',',\n",
              " 'em',\n",
              " 'se',\n",
              " '##u',\n",
              " 'a',\n",
              " '##fast',\n",
              " '##ament',\n",
              " '##o',\n",
              " 'p',\n",
              " '##or',\n",
              " 'm',\n",
              " '##ot',\n",
              " '##ivo',\n",
              " 'de',\n",
              " 'f',\n",
              " '##é',\n",
              " '##ria',\n",
              " '##s',\n",
              " ',',\n",
              " 'no',\n",
              " 'per',\n",
              " '##ío',\n",
              " '##do',\n",
              " 'de',\n",
              " '04',\n",
              " '/',\n",
              " '11',\n",
              " '/',\n",
              " '2016',\n",
              " 'a',\n",
              " '13',\n",
              " '/',\n",
              " '11',\n",
              " '/',\n",
              " '2016',\n",
              " ',',\n",
              " 'com',\n",
              " 'o',\n",
              " 'de',\n",
              " '##cor',\n",
              " '##rent',\n",
              " '##e',\n",
              " 'p',\n",
              " '##aga',\n",
              " '##mento',\n",
              " 'das',\n",
              " 'van',\n",
              " '##tage',\n",
              " '##ns',\n",
              " 'p',\n",
              " '##or',\n",
              " '10',\n",
              " 'di',\n",
              " '##as',\n",
              " '.']"
            ]
          },
          "execution_count": 245,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_txt[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "YKga3RAdSMST"
      },
      "outputs": [],
      "source": [
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags, dtype=torch.long)\n",
        "val_tags = torch.tensor(val_tags, dtype=torch.long)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "zr76EftaSOc7"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fyxa6kHxSQhw",
        "outputId": "092c2871-a431-4327-c1f9-2c55ee44e5f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'4.11.3'"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "\n",
        "transformers.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VflvtbitEM6",
        "outputId": "54e79031-8d94-4a96-ec1c-c5c24bc26b0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=len(tag2idx),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "roXX3ekbtJpi"
      },
      "outputs": [],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "model.cuda();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "WLjUPRrW0N8s"
      },
      "outputs": [],
      "source": [
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "g1nMbn1q0OTX"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 3\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiETKbw30R9U",
        "outputId": "e3671850-122a-4483-b2b1-bc3aea775fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval==0.0.12 in c:\\users\\arthu\\anaconda3\\envs\\deep learning\\lib\\site-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in c:\\users\\arthu\\anaconda3\\envs\\deep learning\\lib\\site-packages (from seqeval==0.0.12) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\arthu\\anaconda3\\envs\\deep learning\\lib\\site-packages (from seqeval==0.0.12) (1.19.2)\n",
            "Requirement already satisfied: scipy>=0.14 in c:\\users\\arthu\\anaconda3\\envs\\deep learning\\lib\\site-packages (from Keras>=2.2.4->seqeval==0.0.12) (1.6.2)\n",
            "Requirement already satisfied: h5py in c:\\users\\arthu\\anaconda3\\envs\\deep learning\\lib\\site-packages (from Keras>=2.2.4->seqeval==0.0.12) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\arthu\\anaconda3\\envs\\deep learning\\lib\\site-packages (from Keras>=2.2.4->seqeval==0.0.12) (6.0)\n",
            "Requirement already satisfied: six in c:\\users\\arthu\\anaconda3\\envs\\deep learning\\lib\\site-packages (from h5py->Keras>=2.2.4->seqeval==0.0.12) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval==0.0.12\n",
        "from seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLwcvGV40VzJ",
        "outputId": "515b79d2-41b0-42ce-d99c-e482484a0829"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average train loss: 0.2558127416090833\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:  33%|███▎      | 1/3 [00:06<00:12,  6.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.016292601823806763\n",
            "Validation Accuracy: 0.9966153846153846\n",
            "Validation Precision: 0.8351648351648352\n",
            "Validation Recall: 0.926829268292683\n",
            "Validation F1-Score: 0.8786127167630058\n",
            "\n",
            "Average train loss: 0.012545691169280972\n",
            "Validation loss: 0.0032407520338892937\n",
            "Validation Accuracy: 1.0\n",
            "Validation Precision: 1.0\n",
            "Validation Recall: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:  67%|██████▋   | 2/3 [00:11<00:05,  5.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation F1-Score: 1.0\n",
            "\n",
            "Average train loss: 0.006678424012433324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 3/3 [00:17<00:00,  5.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.0015217746258713305\n",
            "Validation Accuracy: 1.0\n",
            "Validation Precision: 1.0\n",
            "Validation Recall: 1.0\n",
            "Validation F1-Score: 1.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## Store the average loss after each epoch so we can plot them.\n",
        "loss_values, validation_loss_values = [], []\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Always clear any previously calculated gradients before performing a backward pass.\n",
        "        model.zero_grad()\n",
        "        # forward pass\n",
        "        # This will return the loss (rather than the model output)\n",
        "        # because we have provided the `labels`.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        # get the loss\n",
        "        loss = outputs[0]\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    # Put the model into evaluation mode\n",
        "    model.eval()\n",
        "    # Reset the validation loss for this epoch.\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients,\n",
        "        # saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have not provided labels.\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    eval_loss = eval_loss / len(valid_dataloader)\n",
        "    validation_loss_values.append(eval_loss)\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "\n",
        "    # Loop through the list of predicted sentences and their true labels in the validation set\n",
        "    pred_tags = []\n",
        "    valid_tags = []\n",
        "    for prediction, labels in zip(predictions, true_labels):\n",
        "        # For each token in the sentence we verify if the true value is a padding, and if it isn't, then we add it to the list of predicted tags.\n",
        "        # This is done because we don't care about how paddings are predicted, since they're not part of the original data.\n",
        "        for p_i, l_i, in zip(prediction, labels):\n",
        "            if tag_values[l_i] != 'PAD':\n",
        "                pred_tags.append(tag_values[p_i])\n",
        "                valid_tags.append(tag_values[l_i])\n",
        "\n",
        "    # Calculate the metrics for this epoch using our predictions and our true values for each token.\n",
        "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "    print(\"Validation Precision: {}\".format(precision_score(pred_tags, valid_tags)))\n",
        "    print(\"Validation Recall: {}\".format(recall_score(pred_tags, valid_tags)))\n",
        "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "#outputs['logits']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_df['predictions'] = pred_tags\n",
        "pred_df['predictions2'] = valid_tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predictions</th>\n",
              "      <th>predictions2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>787</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>788</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>809</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>810</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>812</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>813</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>815</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>816</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>817</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>818</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>820</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1006</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1008</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010</th>\n",
              "      <td>I-per</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>B-per</td>\n",
              "      <td>B-per</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     predictions predictions2\n",
              "150        B-per        B-per\n",
              "151        B-per        B-per\n",
              "152        B-per        B-per\n",
              "153        I-per        I-per\n",
              "154        I-per        I-per\n",
              "155        I-per        I-per\n",
              "163        B-per        B-per\n",
              "164        I-per        I-per\n",
              "165        I-per        I-per\n",
              "166        I-per        I-per\n",
              "167        I-per        I-per\n",
              "168        I-per        I-per\n",
              "169        I-per        I-per\n",
              "192        B-per        B-per\n",
              "193        I-per        I-per\n",
              "194        I-per        I-per\n",
              "195        I-per        I-per\n",
              "196        I-per        I-per\n",
              "197        I-per        I-per\n",
              "198        I-per        I-per\n",
              "199        I-per        I-per\n",
              "200        I-per        I-per\n",
              "287        B-per        B-per\n",
              "288        B-per        B-per\n",
              "289        B-per        B-per\n",
              "290        B-per        B-per\n",
              "291        I-per        I-per\n",
              "292        I-per        I-per\n",
              "293        I-per        I-per\n",
              "294        I-per        I-per\n",
              "295        I-per        I-per\n",
              "296        I-per        I-per\n",
              "387        B-per        B-per\n",
              "388        B-per        B-per\n",
              "389        B-per        B-per\n",
              "390        I-per        I-per\n",
              "391        I-per        I-per\n",
              "392        I-per        I-per\n",
              "393        I-per        I-per\n",
              "394        I-per        I-per\n",
              "395        I-per        I-per\n",
              "396        I-per        I-per\n",
              "397        I-per        I-per\n",
              "398        I-per        I-per\n",
              "422        B-per        B-per\n",
              "423        I-per        I-per\n",
              "424        I-per        I-per\n",
              "425        I-per        I-per\n",
              "426        I-per        I-per\n",
              "427        I-per        I-per\n",
              "428        I-per        I-per\n",
              "434        B-per        B-per\n",
              "435        B-per        B-per\n",
              "436        B-per        B-per\n",
              "437        B-per        B-per\n",
              "438        I-per        I-per\n",
              "439        I-per        I-per\n",
              "440        I-per        I-per\n",
              "441        I-per        I-per\n",
              "442        I-per        I-per\n",
              "443        I-per        I-per\n",
              "534        B-per        B-per\n",
              "535        B-per        B-per\n",
              "536        B-per        B-per\n",
              "537        B-per        B-per\n",
              "538        I-per        I-per\n",
              "539        I-per        I-per\n",
              "540        I-per        I-per\n",
              "541        I-per        I-per\n",
              "542        I-per        I-per\n",
              "543        I-per        I-per\n",
              "784        B-per        B-per\n",
              "785        B-per        B-per\n",
              "786        B-per        B-per\n",
              "787        B-per        B-per\n",
              "788        I-per        I-per\n",
              "789        I-per        I-per\n",
              "790        I-per        I-per\n",
              "791        I-per        I-per\n",
              "792        I-per        I-per\n",
              "793        I-per        I-per\n",
              "809        B-per        B-per\n",
              "810        B-per        B-per\n",
              "811        B-per        B-per\n",
              "812        I-per        I-per\n",
              "813        I-per        I-per\n",
              "814        I-per        I-per\n",
              "815        I-per        I-per\n",
              "816        I-per        I-per\n",
              "817        I-per        I-per\n",
              "818        I-per        I-per\n",
              "819        I-per        I-per\n",
              "820        I-per        I-per\n",
              "1005       B-per        B-per\n",
              "1006       B-per        B-per\n",
              "1007       B-per        B-per\n",
              "1008       I-per        I-per\n",
              "1009       I-per        I-per\n",
              "1010       I-per        I-per\n",
              "1018       B-per        B-per"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_df[pred_df['predictions'] != 'O'].head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "640\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
        "plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Learning curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sentence = \"\"\"\n",
        "Documento gerado sob autenticação Nº QON.500.984.BHA, disponível no endereço http://www.ufrgs.br/autenticacao\n",
        "1/1\n",
        "PORTARIA Nº             1184                  de  18/02/2016\n",
        "O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no\n",
        "uso de suas atribuições que lhe foram conferidas pela Portaria nº.5469, de 04 de outubro de 2012, do\n",
        "Magnífico Reitor, e conforme o Laudo Médico n°37308,\n",
        "RESOLVE: José Da Silva\n",
        "Designar, temporariamente, nos termos\\ da Lei nº. 8.112, de 11 de dezembro de 1990, com redação\n",
        "dada pela Lei nº.9.527, de 10 de dezembro de 1997, a ocupante do cargo de PORTEIRO, do Quadro de\n",
        "Pessoal  desta  Universidade, \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "#sentence_df['sentence'][21]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test_sentence = sentence_df['sentence'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_sentence = tokenizer.encode(test_sentence)\n",
        "input_ids = torch.tensor([tokenized_sentence]).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 192,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101, 11387, 15447, 10153,   176,  5970,  2572, 20295, 12686,  5208,\n",
              "         11761, 18052,   151, 28174,   154, 11414,   119,  2260,   119,  5103,\n",
              "          1527,   119,   139, 11612,   117,  4267, 20080,  1320,  6212, 12559,\n",
              "          1185,  1322,  9014, 28201,  1186,  8413,   131,   120,   120,  7001,\n",
              "           119,   190,  2087, 10805,  1116,   119,  9304,   120, 12686,  5208,\n",
              "         11761,  2599,  1186,   122,   120,   122,   153,  9565,  9159, 20595,\n",
              "          1592,   151, 28174, 13176,  1527,  1260,  1407,   120,  5507,   120,\n",
              "          1446,   152, 11629, 28191,   118,   155, 27514, 18082,  2069, 18581,\n",
              "           100, 18581,   153,  9919, 23161, 10719,   141,  1592,  7414, 26140,\n",
              "          8900,  9949, 14569,  2036,   143, 10069,  9637, 12507,   141,  2346,\n",
              "           155, 19368,   144,  9664, 16769,  2036,   141,  2346,   156,  2591,\n",
              "          2162,   117,  1185,  1366,  1186,  1260, 28117,  2225,  1120,  2047,\n",
              "          7925,  1182, 28201, 28207,  1279, 15027,   181,  4638,  1111,  2312,\n",
              "         14255,  6732, 23358,   185,  9945,  3905, 11315,   183, 28174,   119,\n",
              "          4335,  1545,  1580,   117,  1260,  5129,  1260,  1149, 10354,  2180,\n",
              "          1260,  1368,   117,  1202,  7085, 17653,  6212, 21361,  1186, 11336,\n",
              "         15419,   117,   174, 26736,  1162,   184, 25070,  2572,   150,  2744,\n",
              "         13328,  1186,   183,  7259, 26303, 13144,  1604,   117,   155,  9919,\n",
              "         13901, 17145,   131,  5180, 10136, 11211,  4800,  1813,   117, 16655,\n",
              "         22190, 18331, 23771,   117,  1185,  1116,  1858,  2155,   165,  5358,\n",
              "          3180,  1182,   183, 28174,   119,   129,   119, 11150,   117,  1260,\n",
              "          1429,  1260,  1260,  3171, 12913,  2180,  1260,  1997,   117,  3254,\n",
              "          1894,  1161, 18052,  4153,  1161,   185,  9945,  3180,  1182,   183,\n",
              "         28174,   119,   130,   119,  3882,  1559,   117,  1260,  1275,  1260,\n",
              "          1260,  3171, 12913,  2180,  1260,  1816,   117,   170,   184, 18637,\n",
              "         14626,  1202,  6527,  1260,   153,  9565, 12880, 18172,  2346,   117,\n",
              "          1202,   154, 18413,  2180,  1260,   153,  5800, 12985,  1233,  3532,\n",
              "          1777, 17572,  1162,   117,   102]], device='cuda:0')"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    output = model(input_ids)\n",
        "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join bpe split tokens\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "new_tokens, new_labels = [], []\n",
        "for token, label_idx in zip(tokens, label_indices[0]):\n",
        "    #print('Token: ', token, ' Label: ', label)\n",
        "    if token.startswith(\"##\"):\n",
        "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "    else:\n",
        "        new_labels.append(tag_values[label_idx])\n",
        "        new_tokens.append(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O\t[CLS]\n",
            "O\tDocumento\n",
            "O\tgerado\n",
            "O\tsob\n",
            "O\tautenticação\n",
            "O\tNº\n",
            "O\tQON\n",
            "O\t.\n",
            "O\t500\n",
            "O\t.\n",
            "O\t984\n",
            "O\t.\n",
            "O\tBHA\n",
            "O\t,\n",
            "O\tdisponível\n",
            "O\tno\n",
            "O\tendereço\n",
            "O\thttp\n",
            "O\t:\n",
            "O\t/\n",
            "O\t/\n",
            "O\twww\n",
            "O\t.\n",
            "O\tufrgs\n",
            "O\t.\n",
            "O\tbr\n",
            "O\t/\n",
            "O\tautenticacao\n",
            "O\t1\n",
            "O\t/\n",
            "O\t1\n",
            "O\tPORTARIA\n",
            "O\tNº\n",
            "O\t1184\n",
            "O\tde\n",
            "O\t18\n",
            "O\t/\n",
            "O\t02\n",
            "O\t/\n",
            "O\t2016\n",
            "O\tO\n",
            "O\tPRÓ\n",
            "O\t-\n",
            "O\tREITOR\n",
            "O\tDE\n",
            "O\t[UNK]\n",
            "O\tDE\n",
            "O\tPESSOAS\n",
            "O\tDA\n",
            "O\tUNIVERSIDADE\n",
            "O\tFEDERAL\n",
            "O\tDO\n",
            "O\tRIO\n",
            "O\tGRANDE\n",
            "O\tDO\n",
            "O\tSUL\n",
            "O\t,\n",
            "O\tno\n",
            "O\tuso\n",
            "O\tde\n",
            "O\tsuas\n",
            "O\tatribuições\n",
            "O\tque\n",
            "O\tlhe\n",
            "O\tforam\n",
            "O\tconferidas\n",
            "O\tpela\n",
            "O\tPortaria\n",
            "O\tnº\n",
            "O\t.\n",
            "O\t5469\n",
            "O\t,\n",
            "O\tde\n",
            "O\t04\n",
            "O\tde\n",
            "O\toutubro\n",
            "O\tde\n",
            "O\t2012\n",
            "O\t,\n",
            "O\tdo\n",
            "O\tMagnífico\n",
            "O\tReitor\n",
            "O\t,\n",
            "O\te\n",
            "O\tconforme\n",
            "O\to\n",
            "O\tLaudo\n",
            "O\tMédico\n",
            "O\tn°37308\n",
            "O\t,\n",
            "O\tRESOLVE\n",
            "O\t:\n",
            "O\tJosé\n",
            "O\tDa\n",
            "O\tSilva\n",
            "O\tDesignar\n",
            "O\t,\n",
            "O\ttemporariamente\n",
            "O\t,\n",
            "O\tnos\n",
            "O\ttermos\n",
            "O\t\\\n",
            "O\tda\n",
            "O\tLei\n",
            "O\tnº\n",
            "O\t.\n",
            "O\t8\n",
            "O\t.\n",
            "O\t112\n",
            "O\t,\n",
            "O\tde\n",
            "O\t11\n",
            "O\tde\n",
            "O\tdezembro\n",
            "O\tde\n",
            "O\t1990\n",
            "O\t,\n",
            "O\tcom\n",
            "O\tredação\n",
            "O\tdada\n",
            "O\tpela\n",
            "O\tLei\n",
            "O\tnº\n",
            "O\t.\n",
            "O\t9\n",
            "O\t.\n",
            "O\t527\n",
            "O\t,\n",
            "O\tde\n",
            "O\t10\n",
            "O\tde\n",
            "O\tdezembro\n",
            "O\tde\n",
            "O\t1997\n",
            "O\t,\n",
            "O\ta\n",
            "O\tocupante\n",
            "O\tdo\n",
            "O\tcargo\n",
            "O\tde\n",
            "O\tPORTEIRO\n",
            "O\t,\n",
            "O\tdo\n",
            "O\tQuadro\n",
            "O\tde\n",
            "O\tPessoal\n",
            "O\tdesta\n",
            "O\tUniversidade\n",
            "O\t,\n",
            "O\t[SEP]\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(new_tokens, new_labels):\n",
        "    print(\"{}\\t{}\".format(label, token))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMlBV6AKns3qBgU+66qJunA",
      "include_colab_link": true,
      "name": "bert-ner-test.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ba5b3f7521f7aea3d786a4bde6a43384960ac350401fd3bc258a0740b79cfc42"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('Deep Learning': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
