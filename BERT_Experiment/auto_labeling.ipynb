{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer, WhitespaceTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from pprint import pprint\n",
    "import os\n",
    "import re\n",
    "import data_processing\n",
    "from datasets import load_dataset\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_test_file = \"acerpi_dataset/test/\"\n",
    "path_to_train_file = \"acerpi_dataset/train/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Unannotated Train Documents\n",
    "We get the unnanotatted train texts and later we'll store each document as a key, value pair in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = {}\n",
    "for filename in os.listdir(path_to_train_file):\n",
    "   if filename.endswith(\".txt\"):\n",
    "      with open(os.path.join(path_to_train_file, filename), 'r', encoding=\"utf8\") as f:\n",
    "         key = int(filename.replace('.txt', ''))\n",
    "         value = f.read()\n",
    "         train_documents[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Annotated Sentences\n",
    "We get the sentences from 'prep_data_labeling.ipynb' that we annotated manually with deccano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sentences = pd.read_json(os.path.join(path_to_test_file, 'annotated_sentences.jsonl'), orient='record', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>duplicates</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>Documento gerado sob autenticação Nº LKB.506.4...</td>\n",
       "      <td>105798</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[233, 238, ORGANIZATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>1/1 PORTARIA Nº 1955 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>105798</td>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [448, 475, MISCELLAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117</td>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>105798</td>\n",
       "      <td>2</td>\n",
       "      <td>[2, 5, 8, 11, 17, 19, 21, 23, 25, 41, 56]</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118</td>\n",
       "      <td>Documento gerado sob autenticação Nº BOA.507.6...</td>\n",
       "      <td>105799</td>\n",
       "      <td>3</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[[233, 238, ORGANIZATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119</td>\n",
       "      <td>1/1 PORTARIA Nº 1956 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>105799</td>\n",
       "      <td>4</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [229, 235, MISCELLAN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  document  \\\n",
       "0  115  Documento gerado sob autenticação Nº LKB.506.4...    105798   \n",
       "1  116  1/1 PORTARIA Nº 1955 de 05/03/2020 O PRÓ-REITO...    105798   \n",
       "2  117  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...    105798   \n",
       "3  118  Documento gerado sob autenticação Nº BOA.507.6...    105799   \n",
       "4  119  1/1 PORTARIA Nº 1956 de 05/03/2020 O PRÓ-REITO...    105799   \n",
       "\n",
       "   sentence_id                                 duplicates  \\\n",
       "0            0                                        [0]   \n",
       "1            1                                        [1]   \n",
       "2            2  [2, 5, 8, 11, 17, 19, 21, 23, 25, 41, 56]   \n",
       "3            3                                        [3]   \n",
       "4            4                                        [4]   \n",
       "\n",
       "                                               label  \n",
       "0                         [[233, 238, ORGANIZATION]]  \n",
       "1  [[72, 113, ORGANIZATION], [448, 475, MISCELLAN...  \n",
       "2         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "3                         [[233, 238, ORGANIZATION]]  \n",
       "4  [[72, 113, ORGANIZATION], [229, 235, MISCELLAN...  "
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(annotated_sentences.shape)\n",
    "annotated_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas'"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_sentences.iloc[2]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll explode our dataframe so that duplicate sentences can become separate rows. We'll also drop a few columns that will have inconsistent information after the explode and will also not be useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6982: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  return Index(sequences[0], name=names)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "annotated_sentences = annotated_sentences.explode(\"duplicates\")\n",
    "annotated_sentences.drop(columns=['id', 'document', 'sentence_id'], inplace=True)\n",
    "annotated_sentences.columns = ['text', 'sentence_id', 'label']\n",
    "annotated_sentences = annotated_sentences.set_index(['sentence_id'], verify_integrity=True)\n",
    "annotated_sentences.index.name = None\n",
    "annotated_sentences = annotated_sentences.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Documento gerado sob autenticação Nº LKB.506.4...</td>\n",
       "      <td>[[233, 238, ORGANIZATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1 PORTARIA Nº 1955 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [448, 475, MISCELLAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Documento gerado sob autenticação Nº BOA.507.6...</td>\n",
       "      <td>[[233, 238, ORGANIZATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1 PORTARIA Nº 1956 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [229, 235, MISCELLAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Documento gerado sob autenticação Nº KMT.508.8...</td>\n",
       "      <td>[[233, 238, ORGANIZATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1/1 PORTARIA Nº 1957 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>[[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Documento gerado sob autenticação Nº DYG.509.9...</td>\n",
       "      <td>[[233, 238, ORGANIZATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1/1 PORTARIA Nº 1958 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [229, 235, MISCELLAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Documento gerado sob autenticação Nº MGM.074.9...</td>\n",
       "      <td>[[233, 238, ORGANIZATION]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1/1 PORTARIA Nº 1706 de 27/02/2020 A VICE-REIT...</td>\n",
       "      <td>[[53, 94, ORGANIZATION], [284, 320, MISCELLANE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Processo nº 23078.503731/2020-66.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora.</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Documento gerado sob autenticação Nº QON.500.9...</td>\n",
       "      <td>[[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Documento gerado sob autenticação Nº QMN.077.7...</td>\n",
       "      <td>[[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   Documento gerado sob autenticação Nº LKB.506.4...   \n",
       "1   1/1 PORTARIA Nº 1955 de 05/03/2020 O PRÓ-REITO...   \n",
       "2   MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "3   Documento gerado sob autenticação Nº BOA.507.6...   \n",
       "4   1/1 PORTARIA Nº 1956 de 05/03/2020 O PRÓ-REITO...   \n",
       "5   MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "6   Documento gerado sob autenticação Nº KMT.508.8...   \n",
       "7   1/1 PORTARIA Nº 1957 de 05/03/2020 O PRÓ-REITO...   \n",
       "8   MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "9   Documento gerado sob autenticação Nº DYG.509.9...   \n",
       "10  1/1 PORTARIA Nº 1958 de 05/03/2020 O PRÓ-REITO...   \n",
       "11  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "12  Documento gerado sob autenticação Nº MGM.074.9...   \n",
       "13  1/1 PORTARIA Nº 1706 de 27/02/2020 A VICE-REIT...   \n",
       "14                  Processo nº 23078.503731/2020-66.   \n",
       "15                  JANE FRAGA TUTIKIAN Vice-Reitora.   \n",
       "16  Documento gerado sob autenticação Nº QON.500.9...   \n",
       "17  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "18  Documento gerado sob autenticação Nº QMN.077.7...   \n",
       "19  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "\n",
       "                                                label  \n",
       "0                          [[233, 238, ORGANIZATION]]  \n",
       "1   [[72, 113, ORGANIZATION], [448, 475, MISCELLAN...  \n",
       "2          [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "3                          [[233, 238, ORGANIZATION]]  \n",
       "4   [[72, 113, ORGANIZATION], [229, 235, MISCELLAN...  \n",
       "5          [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "6                          [[233, 238, ORGANIZATION]]  \n",
       "7   [[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...  \n",
       "8          [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "9                          [[233, 238, ORGANIZATION]]  \n",
       "10  [[72, 113, ORGANIZATION], [229, 235, MISCELLAN...  \n",
       "11         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "12                         [[233, 238, ORGANIZATION]]  \n",
       "13  [[53, 94, ORGANIZATION], [284, 320, MISCELLANE...  \n",
       "14                                                 []  \n",
       "15         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "16  [[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...  \n",
       "17         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "18  [[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...  \n",
       "19         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  "
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(annotated_sentences.shape)\n",
    "annotated_sentences.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[43, 91, 'MISCELLANEOUS']]\n",
      "1 [[12, 53, 'ORGANIZATION']]\n",
      "1 [[39, 55, 'MISCELLANEOUS']]\n",
      "0 []\n",
      "1 [[42, 67, 'MISCELLANEOUS']]\n",
      "0 []\n",
      "0 []\n",
      "1 [[0, 21, 'PERSON']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[0, 21, 'PERSON']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[0, 21, 'PERSON']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "0 []\n",
      "1 [[3, 41, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "1 [[233, 238, 'ORGANIZATION']]\n",
      "0 []\n"
     ]
    }
   ],
   "source": [
    "for idx, row in annotated_sentences.iterrows():\n",
    "    if len(row['label']) == 0 or len(row['label']) == 1:\n",
    "        print(len(row['label']), row['label'])\n",
    "        annotated_sentences.drop(idx, inplace=True)\n",
    "annotated_sentences.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1 PORTARIA Nº 1955 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [448, 475, MISCELLAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1 PORTARIA Nº 1956 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [229, 235, MISCELLAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1 PORTARIA Nº 1957 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>[[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1/1 PORTARIA Nº 1958 de 05/03/2020 O PRÓ-REITO...</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [229, 235, MISCELLAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1/1 PORTARIA Nº 1706 de 27/02/2020 A VICE-REIT...</td>\n",
       "      <td>[[53, 94, ORGANIZATION], [284, 320, MISCELLANE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora.</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Documento gerado sob autenticação Nº QON.500.9...</td>\n",
       "      <td>[[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Documento gerado sob autenticação Nº QMN.077.7...</td>\n",
       "      <td>[[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Documento gerado sob autenticação Nº NIK.843.5...</td>\n",
       "      <td>[[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Documento gerado sob autenticação Nº KJE.136.2...</td>\n",
       "      <td>[[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Documento gerado sob autenticação Nº SBD.408.6...</td>\n",
       "      <td>[[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1/1 PORTARIA Nº 3812 de 04/05/2017 A VICE-REIT...</td>\n",
       "      <td>[[53, 94, ORGANIZATION], [296, 305, MISCELLANE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora.</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1/1 PORTARIA Nº 3813 de 04/05/2017 A VICE-REIT...</td>\n",
       "      <td>[[53, 94, ORGANIZATION], [295, 305, MISCELLANE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora.</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1/1 PORTARIA Nº 3806 de 04/05/2017 A VICE-REIT...</td>\n",
       "      <td>[[53, 94, ORGANIZATION], [295, 305, MISCELLANE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora.</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1/1 PORTARIA Nº 3801 de 03/05/2017 O PRÓ-REITO...</td>\n",
       "      <td>[[72, 113, ORGANIZATION], [371, 390, PERSON], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1º NOMEAR a servidora SIMONE SARMENTO, docente...</td>\n",
       "      <td>[[22, 37, PERSON], [146, 162, MISCELLANEOUS], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2º NOMEAR as servidoras abaixo relacionadas, c...</td>\n",
       "      <td>[[50, 75, MISCELLANEOUS], [92, 97, ORGANIZATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2/2 TOMOKO KIMURA GAUDIOSO, docente, matrícula...</td>\n",
       "      <td>[[4, 27, PERSON], [28, 35, MISCELLANEOUS], [11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1/1 PORTARIA Nº 2565 de 09/04/2018 O PRÓ-REITO...</td>\n",
       "      <td>[[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...</td>\n",
       "      <td>[[0, 24, PERSON], [25, 56, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1/1 PORTARIA Nº 2672 de 10/04/2018 O REITOR DA...</td>\n",
       "      <td>[[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1/1 PORTARIA Nº 2604 de 10/04/2018 O REITOR DA...</td>\n",
       "      <td>[[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RUI VICENTE OPPERMANN Reitor</td>\n",
       "      <td>[[0, 21, PERSON], [22, 28, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1/1 PORTARIA Nº 2671 de 10/04/2018 O REITOR DA...</td>\n",
       "      <td>[[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1/1 PORTARIA Nº 2605 de 10/04/2018 O REITOR DA...</td>\n",
       "      <td>[[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>RUI VICENTE OPPERMANN Reitor</td>\n",
       "      <td>[[0, 21, PERSON], [22, 28, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1/1 PORTARIA Nº 9644 de 29/11/2018 A VICE-REIT...</td>\n",
       "      <td>[[79, 120, ORGANIZATION], [212, 239, MISCELLAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1/1 PORTARIA Nº 9645 de 29/11/2018 A VICE-REIT...</td>\n",
       "      <td>[[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1/1 PORTARIA Nº 9641 de 29/11/2018 A VICE-REIT...</td>\n",
       "      <td>[[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>JANE FRAGA TUTIKIAN VICE-REITORA, NO EXERCÍCIO...</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1/1 PORTARIA Nº 9640 de 29/11/2018 A VICE-REIT...</td>\n",
       "      <td>[[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>JANE FRAGA TUTIKIAN VICE-REITORA, NO EXERCÍCIO...</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1/1 PORTARIA Nº 9646 de 29/11/2018 A VICE-REIT...</td>\n",
       "      <td>[[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1/1 PORTARIA Nº 6629 de 24/07/2019 A VICE-REIT...</td>\n",
       "      <td>[[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1/1 PORTARIA Nº 6635 de 24/07/2019 O PRÓ-REITO...</td>\n",
       "      <td>[[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1/1 PORTARIA Nº 6636 de 24/07/2019 O PRÓ-REITO...</td>\n",
       "      <td>[[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1/1 PORTARIA Nº 6838 de 29/07/2019 O REITOR DA...</td>\n",
       "      <td>[[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>RUI VICENTE OPPERMANN Reitor.</td>\n",
       "      <td>[[0, 21, PERSON], [22, 28, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1/1 PORTARIA Nº 6613 de 24/07/2019 A VICE-REIT...</td>\n",
       "      <td>[[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...</td>\n",
       "      <td>[[0, 19, PERSON], [20, 32, MISCELLANEOUS]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   1/1 PORTARIA Nº 1955 de 05/03/2020 O PRÓ-REITO...   \n",
       "1   MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "2   1/1 PORTARIA Nº 1956 de 05/03/2020 O PRÓ-REITO...   \n",
       "3   MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "4   1/1 PORTARIA Nº 1957 de 05/03/2020 O PRÓ-REITO...   \n",
       "5   MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "6   1/1 PORTARIA Nº 1958 de 05/03/2020 O PRÓ-REITO...   \n",
       "7   MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "8   1/1 PORTARIA Nº 1706 de 27/02/2020 A VICE-REIT...   \n",
       "9                   JANE FRAGA TUTIKIAN Vice-Reitora.   \n",
       "10  Documento gerado sob autenticação Nº QON.500.9...   \n",
       "11  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "12  Documento gerado sob autenticação Nº QMN.077.7...   \n",
       "13  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "14  Documento gerado sob autenticação Nº NIK.843.5...   \n",
       "15  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "16  Documento gerado sob autenticação Nº KJE.136.2...   \n",
       "17  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "18  Documento gerado sob autenticação Nº SBD.408.6...   \n",
       "19  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "20  1/1 PORTARIA Nº 3812 de 04/05/2017 A VICE-REIT...   \n",
       "21                  JANE FRAGA TUTIKIAN Vice-Reitora.   \n",
       "22  1/1 PORTARIA Nº 3813 de 04/05/2017 A VICE-REIT...   \n",
       "23                  JANE FRAGA TUTIKIAN Vice-Reitora.   \n",
       "24  1/1 PORTARIA Nº 3806 de 04/05/2017 A VICE-REIT...   \n",
       "25                  JANE FRAGA TUTIKIAN Vice-Reitora.   \n",
       "26  1/1 PORTARIA Nº 3801 de 03/05/2017 O PRÓ-REITO...   \n",
       "27  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "28  1º NOMEAR a servidora SIMONE SARMENTO, docente...   \n",
       "29  2º NOMEAR as servidoras abaixo relacionadas, c...   \n",
       "30  2/2 TOMOKO KIMURA GAUDIOSO, docente, matrícula...   \n",
       "31  1/1 PORTARIA Nº 2565 de 09/04/2018 O PRÓ-REITO...   \n",
       "32  MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão ...   \n",
       "33  1/1 PORTARIA Nº 2672 de 10/04/2018 O REITOR DA...   \n",
       "34  1/1 PORTARIA Nº 2604 de 10/04/2018 O REITOR DA...   \n",
       "35                       RUI VICENTE OPPERMANN Reitor   \n",
       "36  1/1 PORTARIA Nº 2671 de 10/04/2018 O REITOR DA...   \n",
       "37  1/1 PORTARIA Nº 2605 de 10/04/2018 O REITOR DA...   \n",
       "38                       RUI VICENTE OPPERMANN Reitor   \n",
       "39  1/1 PORTARIA Nº 9644 de 29/11/2018 A VICE-REIT...   \n",
       "40  JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...   \n",
       "41  1/1 PORTARIA Nº 9645 de 29/11/2018 A VICE-REIT...   \n",
       "42  JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...   \n",
       "43  1/1 PORTARIA Nº 9641 de 29/11/2018 A VICE-REIT...   \n",
       "44  JANE FRAGA TUTIKIAN VICE-REITORA, NO EXERCÍCIO...   \n",
       "45  1/1 PORTARIA Nº 9640 de 29/11/2018 A VICE-REIT...   \n",
       "46  JANE FRAGA TUTIKIAN VICE-REITORA, NO EXERCÍCIO...   \n",
       "47  1/1 PORTARIA Nº 9646 de 29/11/2018 A VICE-REIT...   \n",
       "48  JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...   \n",
       "49  1/1 PORTARIA Nº 6629 de 24/07/2019 A VICE-REIT...   \n",
       "50  JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...   \n",
       "51  1/1 PORTARIA Nº 6635 de 24/07/2019 O PRÓ-REITO...   \n",
       "52  1/1 PORTARIA Nº 6636 de 24/07/2019 O PRÓ-REITO...   \n",
       "53  1/1 PORTARIA Nº 6838 de 29/07/2019 O REITOR DA...   \n",
       "54                      RUI VICENTE OPPERMANN Reitor.   \n",
       "55  1/1 PORTARIA Nº 6613 de 24/07/2019 A VICE-REIT...   \n",
       "56  JANE FRAGA TUTIKIAN Vice-Reitora, no exercício...   \n",
       "\n",
       "                                                label  \n",
       "0   [[72, 113, ORGANIZATION], [448, 475, MISCELLAN...  \n",
       "1          [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "2   [[72, 113, ORGANIZATION], [229, 235, MISCELLAN...  \n",
       "3          [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "4   [[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...  \n",
       "5          [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "6   [[72, 113, ORGANIZATION], [229, 235, MISCELLAN...  \n",
       "7          [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "8   [[53, 94, ORGANIZATION], [284, 320, MISCELLANE...  \n",
       "9          [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "10  [[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...  \n",
       "11         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "12  [[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...  \n",
       "13         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "14  [[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...  \n",
       "15         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "16  [[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...  \n",
       "17         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "18  [[147, 178, MISCELLANEOUS], [182, 223, ORGANIZ...  \n",
       "19         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "20  [[53, 94, ORGANIZATION], [296, 305, MISCELLANE...  \n",
       "21         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "22  [[53, 94, ORGANIZATION], [295, 305, MISCELLANE...  \n",
       "23         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "24  [[53, 94, ORGANIZATION], [295, 305, MISCELLANE...  \n",
       "25         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "26  [[72, 113, ORGANIZATION], [371, 390, PERSON], ...  \n",
       "27         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "28  [[22, 37, PERSON], [146, 162, MISCELLANEOUS], ...  \n",
       "29  [[50, 75, MISCELLANEOUS], [92, 97, ORGANIZATIO...  \n",
       "30  [[4, 27, PERSON], [28, 35, MISCELLANEOUS], [11...  \n",
       "31  [[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...  \n",
       "32         [[0, 24, PERSON], [25, 56, MISCELLANEOUS]]  \n",
       "33  [[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...  \n",
       "34  [[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...  \n",
       "35         [[0, 21, PERSON], [22, 28, MISCELLANEOUS]]  \n",
       "36  [[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...  \n",
       "37  [[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...  \n",
       "38         [[0, 21, PERSON], [22, 28, MISCELLANEOUS]]  \n",
       "39  [[79, 120, ORGANIZATION], [212, 239, MISCELLAN...  \n",
       "40         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "41  [[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...  \n",
       "42         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "43  [[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...  \n",
       "44         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "45  [[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...  \n",
       "46         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "47  [[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...  \n",
       "48         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "49  [[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...  \n",
       "50         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  \n",
       "51  [[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...  \n",
       "52  [[37, 68, MISCELLANEOUS], [72, 113, ORGANIZATI...  \n",
       "53  [[37, 43, MISCELLANEOUS], [47, 88, ORGANIZATIO...  \n",
       "54         [[0, 21, PERSON], [22, 28, MISCELLANEOUS]]  \n",
       "55  [[37, 49, MISCELLANEOUS], [79, 120, ORGANIZATI...  \n",
       "56         [[0, 19, PERSON], [20, 32, MISCELLANEOUS]]  "
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the sentences and tokens using the TreebankWordTokenizer\n",
    "The first method 'tokenize' will split our sentence and return a list of words which we'll organize in a 'tokenized_sentences' list.\n",
    "The second method 'span_tokenize' will return the start and end position of each token resulting from the split. We'll organize them in a 'token_positions' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1/1', 'PORTARIA', 'Nº', '1955', 'de', '05/03/2020', 'O', 'PRÓ-REITOR', 'DE', 'GESTÃO', 'DE', 'PESSOAS', 'DA', 'UNIVERSIDADE', 'FEDERAL', 'DO', 'RIO', 'GRANDE', 'DO', 'SUL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', 'que', 'lhe', 'foram', 'conferidas', 'pela', 'Portaria', 'nº', '7684', ',', 'de', '03', 'de', 'outubro', 'de', '2016', ',', 'do', 'Magnífico', 'Reitor', ',', 'e', 'conforme', 'o', 'Laudo', 'Médico', 'n°60131', ',', 'RESOLVE', 'Designar', ',', 'temporariamente', ',', 'nos', 'termos', 'da', 'Lei', 'nº', '8.112', ',', 'de', '11', 'de', 'dezembro', 'de', '1990', ',', 'com', 'redação', 'dada', 'pela', 'Lei', 'nº', '9.527', ',', 'de', '10', 'de', 'dezembro', 'de', '1997', ',', 'a', 'ocupante', 'do', 'cargo', 'de', 'ASSISTENTE', 'EM', 'ADMINISTRAÇÃO', ',', 'do', 'Quadro', 'de', 'Pessoal', 'desta', 'Universidade', ',', 'FRANCIELE', 'MARQUES', 'ZIQUINATTI', '(', 'Siape', ':', '1091092', ')', ',', 'para', 'substituir', 'TURENE', 'ANDRADE', 'E', 'SILVA', 'NETO', '(', 'Siape', ':', '0356721', ')', ',', 'Diretor', 'da', 'Divisão', 'de', 'Preparo', 'da', 'Licitação', 'do', 'Departamento', 'de', 'Aquisição', 'de', 'Bens', 'e', 'Serviços', ',', 'Código', 'FG-4', ',', 'em', 'seu', 'afastamento', 'por', 'motivo', 'de', 'Laudo', 'Médico', 'do', 'titular', 'da', 'Função', ',', 'no', 'período', 'de', '30/01/2020', 'a', '15/03/2020', ',', 'com', 'o', 'decorrente', 'pagamento', 'das', 'vantagens', 'por', '46', 'dias', '.']\n",
      "[(0, 3), (4, 12), (13, 15), (16, 20), (21, 23), (24, 34), (35, 36), (37, 47), (48, 50), (51, 57), (58, 60), (61, 68), (69, 71), (72, 84), (85, 92), (93, 95), (96, 99), (100, 106), (107, 109), (110, 113), (113, 114), (115, 117), (118, 121), (122, 124), (125, 129), (130, 141), (142, 145), (146, 149), (150, 155), (156, 166), (167, 171), (172, 180), (181, 183), (184, 188), (188, 189), (190, 192), (193, 195), (196, 198), (199, 206), (207, 209), (210, 214), (214, 215), (216, 218), (219, 228), (229, 235), (235, 236), (237, 238), (239, 247), (248, 249), (250, 255), (256, 262), (263, 270), (270, 271), (272, 279), (280, 288), (288, 289), (290, 305), (305, 306), (307, 310), (311, 317), (318, 320), (321, 324), (325, 327), (328, 333), (333, 334), (335, 337), (338, 340), (341, 343), (344, 352), (353, 355), (356, 360), (360, 361), (362, 365), (366, 373), (374, 378), (379, 383), (384, 387), (388, 390), (391, 396), (396, 397), (398, 400), (401, 403), (404, 406), (407, 415), (416, 418), (419, 423), (423, 424), (425, 426), (427, 435), (436, 438), (439, 444), (445, 447), (448, 458), (459, 461), (462, 475), (475, 476), (477, 479), (480, 486), (487, 489), (490, 497), (498, 503), (504, 516), (516, 517), (518, 527), (528, 535), (536, 546), (547, 548), (548, 553), (553, 554), (555, 562), (563, 564), (564, 565), (566, 570), (571, 581), (582, 588), (589, 596), (597, 598), (599, 604), (605, 609), (610, 611), (611, 616), (616, 617), (618, 625), (626, 627), (627, 628), (629, 636), (637, 639), (640, 647), (648, 650), (651, 658), (659, 661), (662, 671), (672, 674), (675, 687), (688, 690), (691, 700), (701, 703), (704, 708), (709, 710), (711, 719), (719, 720), (721, 727), (728, 732), (732, 733), (734, 736), (737, 740), (741, 752), (753, 756), (757, 763), (764, 766), (767, 772), (773, 779), (780, 782), (783, 790), (791, 793), (794, 800), (800, 801), (802, 804), (805, 812), (813, 815), (816, 826), (827, 828), (829, 839), (839, 840), (841, 844), (845, 846), (847, 857), (858, 867), (868, 871), (872, 881), (882, 885), (886, 888), (889, 893), (893, 894)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = []\n",
    "token_positions = []\n",
    "for idx, sentence in annotated_sentences['text'].items():\n",
    "    tokenized_sentences.append(TreebankWordTokenizer().tokenize(sentence))\n",
    "    token_positions.append(list(TreebankWordTokenizer().span_tokenize(sentence)))\n",
    "\n",
    "\n",
    "print(tokenized_sentences[0])\n",
    "print(token_positions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence_position = []\n",
    "for sentence, positions in zip(tokenized_sentences, token_positions):\n",
    "    tokenized_sentence_position.append(list(zip(sentence, positions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37, 49, 'MISCELLANEOUS'], [79, 120, 'ORGANIZATION'], [214, 230, 'PERSON'], [232, 264, 'MISCELLANEOUS'], [276, 309, 'ORGANIZATION'], [329, 370, 'ORGANIZATION'], [410, 431, 'ORGANIZATION'], [436, 441, 'LOCATION'], [443, 451, 'LOCATION'], [536, 541, 'ORGANIZATION']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1/1', (0, 3)),\n",
       " ('PORTARIA', (4, 12)),\n",
       " ('Nº', (13, 15)),\n",
       " ('9641', (16, 20)),\n",
       " ('de', (21, 23)),\n",
       " ('29/11/2018', (24, 34)),\n",
       " ('A', (35, 36)),\n",
       " ('VICE-REITORA', (37, 49)),\n",
       " (',', (49, 50)),\n",
       " ('NO', (51, 53)),\n",
       " ('EXERCÍCIO', (54, 63)),\n",
       " ('DA', (64, 66)),\n",
       " ('REITORIA', (67, 75)),\n",
       " ('DA', (76, 78)),\n",
       " ('UNIVERSIDADE', (79, 91)),\n",
       " ('FEDERAL', (92, 99)),\n",
       " ('DO', (100, 102)),\n",
       " ('RIO', (103, 106)),\n",
       " ('GRANDE', (107, 113)),\n",
       " ('DO', (114, 116)),\n",
       " ('SUL', (117, 120)),\n",
       " (',', (120, 121)),\n",
       " ('no', (122, 124)),\n",
       " ('uso', (125, 128)),\n",
       " ('de', (129, 131)),\n",
       " ('suas', (132, 136)),\n",
       " ('atribuições', (137, 148)),\n",
       " ('legais', (149, 155)),\n",
       " ('e', (156, 157)),\n",
       " ('estatutárias', (158, 170)),\n",
       " ('RESOLVE', (171, 178)),\n",
       " ('Autorizar', (179, 188)),\n",
       " ('o', (189, 190)),\n",
       " ('afastamento', (191, 202)),\n",
       " ('do', (203, 205)),\n",
       " ('País', (206, 210)),\n",
       " ('de', (211, 213)),\n",
       " ('LUCIANE', (214, 221)),\n",
       " ('INES', (222, 226)),\n",
       " ('ELY', (227, 230)),\n",
       " (',', (230, 231)),\n",
       " ('Técnico', (232, 239)),\n",
       " ('em', (240, 242)),\n",
       " ('Assuntos', (243, 251)),\n",
       " ('Educacionais', (252, 264)),\n",
       " (',', (264, 265)),\n",
       " ('lotada', (266, 272)),\n",
       " ('na', (273, 275)),\n",
       " ('Pró-Reitoria', (276, 288)),\n",
       " ('de', (289, 291)),\n",
       " ('Gestão', (292, 298)),\n",
       " ('de', (299, 301)),\n",
       " ('Pessoas', (302, 309)),\n",
       " ('e', (310, 311)),\n",
       " ('com', (312, 315)),\n",
       " ('exercício', (316, 325)),\n",
       " ('na', (326, 328)),\n",
       " ('Divisão', (329, 336)),\n",
       " ('de', (337, 339)),\n",
       " ('Qualificação', (340, 352)),\n",
       " ('e', (353, 354)),\n",
       " ('Aperfeiçoamento', (355, 370)),\n",
       " (',', (370, 371)),\n",
       " ('com', (372, 375)),\n",
       " ('a', (376, 377)),\n",
       " ('finalidade', (378, 388)),\n",
       " ('de', (389, 391)),\n",
       " ('realizar', (392, 400)),\n",
       " ('visita', (401, 407)),\n",
       " ('à', (408, 409)),\n",
       " ('Universidade', (410, 422)),\n",
       " ('do', (423, 425)),\n",
       " ('Porto', (426, 431)),\n",
       " (',', (431, 432)),\n",
       " ('em', (433, 435)),\n",
       " ('Porto', (436, 441)),\n",
       " (',', (441, 442)),\n",
       " ('Portugal', (443, 451)),\n",
       " (',', (451, 452)),\n",
       " ('no', (453, 455)),\n",
       " ('período', (456, 463)),\n",
       " ('compreendido', (464, 476)),\n",
       " ('entre', (477, 482)),\n",
       " ('26/01/2019', (483, 493)),\n",
       " ('e', (494, 495)),\n",
       " ('14/02/2019', (496, 506)),\n",
       " (',', (506, 507)),\n",
       " ('incluído', (508, 516)),\n",
       " ('trânsito', (517, 525)),\n",
       " (',', (525, 526)),\n",
       " ('com', (527, 530)),\n",
       " ('ônus', (531, 535)),\n",
       " ('UFRGS', (536, 541)),\n",
       " ('.', (541, 542))]"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(annotated_sentences['label'].loc[43])\n",
    "tokenized_sentence_position[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a list of dataframes. Each dataframe is a sentence and will have the columns 'token', 'start' and 'end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_token_df = []\n",
    "for list_of_words in tokenized_sentence_position:\n",
    "    tokens_df = pd.DataFrame(list_of_words, columns = ['token', 'pos'])\n",
    "    tokens_df[['start', 'end']] = tokens_df['pos'].to_list()\n",
    "    tokens_df = tokens_df.drop(columns='pos')\n",
    "    list_of_token_df.append(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PORTARIA</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nº</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9641</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29/11/2018</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VICE-REITORA</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>,</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NO</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EXERCÍCIO</td>\n",
       "      <td>54</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DA</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>REITORIA</td>\n",
       "      <td>67</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DA</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UNIVERSIDADE</td>\n",
       "      <td>79</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FEDERAL</td>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DO</td>\n",
       "      <td>100</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RIO</td>\n",
       "      <td>103</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GRANDE</td>\n",
       "      <td>107</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DO</td>\n",
       "      <td>114</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SUL</td>\n",
       "      <td>117</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>,</td>\n",
       "      <td>120</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>no</td>\n",
       "      <td>122</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>uso</td>\n",
       "      <td>125</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>de</td>\n",
       "      <td>129</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>suas</td>\n",
       "      <td>132</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>atribuições</td>\n",
       "      <td>137</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>legais</td>\n",
       "      <td>149</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>e</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>estatutárias</td>\n",
       "      <td>158</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RESOLVE</td>\n",
       "      <td>171</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Autorizar</td>\n",
       "      <td>179</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>o</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>afastamento</td>\n",
       "      <td>191</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>do</td>\n",
       "      <td>203</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>País</td>\n",
       "      <td>206</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>de</td>\n",
       "      <td>211</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LUCIANE</td>\n",
       "      <td>214</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>INES</td>\n",
       "      <td>222</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ELY</td>\n",
       "      <td>227</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>,</td>\n",
       "      <td>230</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Técnico</td>\n",
       "      <td>232</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>em</td>\n",
       "      <td>240</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Assuntos</td>\n",
       "      <td>243</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Educacionais</td>\n",
       "      <td>252</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>,</td>\n",
       "      <td>264</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>lotada</td>\n",
       "      <td>266</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>na</td>\n",
       "      <td>273</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Pró-Reitoria</td>\n",
       "      <td>276</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>de</td>\n",
       "      <td>289</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Gestão</td>\n",
       "      <td>292</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>de</td>\n",
       "      <td>299</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Pessoas</td>\n",
       "      <td>302</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>e</td>\n",
       "      <td>310</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>com</td>\n",
       "      <td>312</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>exercício</td>\n",
       "      <td>316</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>na</td>\n",
       "      <td>326</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Divisão</td>\n",
       "      <td>329</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>de</td>\n",
       "      <td>337</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Qualificação</td>\n",
       "      <td>340</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>e</td>\n",
       "      <td>353</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Aperfeiçoamento</td>\n",
       "      <td>355</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>,</td>\n",
       "      <td>370</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>com</td>\n",
       "      <td>372</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>a</td>\n",
       "      <td>376</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>finalidade</td>\n",
       "      <td>378</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>de</td>\n",
       "      <td>389</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>realizar</td>\n",
       "      <td>392</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>visita</td>\n",
       "      <td>401</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>à</td>\n",
       "      <td>408</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Universidade</td>\n",
       "      <td>410</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>do</td>\n",
       "      <td>423</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Porto</td>\n",
       "      <td>426</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>,</td>\n",
       "      <td>431</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>em</td>\n",
       "      <td>433</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Porto</td>\n",
       "      <td>436</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>,</td>\n",
       "      <td>441</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>443</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>,</td>\n",
       "      <td>451</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>no</td>\n",
       "      <td>453</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>período</td>\n",
       "      <td>456</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>compreendido</td>\n",
       "      <td>464</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>entre</td>\n",
       "      <td>477</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>26/01/2019</td>\n",
       "      <td>483</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>e</td>\n",
       "      <td>494</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>14/02/2019</td>\n",
       "      <td>496</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>,</td>\n",
       "      <td>506</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>incluído</td>\n",
       "      <td>508</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>trânsito</td>\n",
       "      <td>517</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>,</td>\n",
       "      <td>525</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>com</td>\n",
       "      <td>527</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>ônus</td>\n",
       "      <td>531</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>UFRGS</td>\n",
       "      <td>536</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>.</td>\n",
       "      <td>541</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              token  start  end\n",
       "0               1/1      0    3\n",
       "1          PORTARIA      4   12\n",
       "2                Nº     13   15\n",
       "3              9641     16   20\n",
       "4                de     21   23\n",
       "5        29/11/2018     24   34\n",
       "6                 A     35   36\n",
       "7      VICE-REITORA     37   49\n",
       "8                 ,     49   50\n",
       "9                NO     51   53\n",
       "10        EXERCÍCIO     54   63\n",
       "11               DA     64   66\n",
       "12         REITORIA     67   75\n",
       "13               DA     76   78\n",
       "14     UNIVERSIDADE     79   91\n",
       "15          FEDERAL     92   99\n",
       "16               DO    100  102\n",
       "17              RIO    103  106\n",
       "18           GRANDE    107  113\n",
       "19               DO    114  116\n",
       "20              SUL    117  120\n",
       "21                ,    120  121\n",
       "22               no    122  124\n",
       "23              uso    125  128\n",
       "24               de    129  131\n",
       "25             suas    132  136\n",
       "26      atribuições    137  148\n",
       "27           legais    149  155\n",
       "28                e    156  157\n",
       "29     estatutárias    158  170\n",
       "30          RESOLVE    171  178\n",
       "31        Autorizar    179  188\n",
       "32                o    189  190\n",
       "33      afastamento    191  202\n",
       "34               do    203  205\n",
       "35             País    206  210\n",
       "36               de    211  213\n",
       "37          LUCIANE    214  221\n",
       "38             INES    222  226\n",
       "39              ELY    227  230\n",
       "40                ,    230  231\n",
       "41          Técnico    232  239\n",
       "42               em    240  242\n",
       "43         Assuntos    243  251\n",
       "44     Educacionais    252  264\n",
       "45                ,    264  265\n",
       "46           lotada    266  272\n",
       "47               na    273  275\n",
       "48     Pró-Reitoria    276  288\n",
       "49               de    289  291\n",
       "50           Gestão    292  298\n",
       "51               de    299  301\n",
       "52          Pessoas    302  309\n",
       "53                e    310  311\n",
       "54              com    312  315\n",
       "55        exercício    316  325\n",
       "56               na    326  328\n",
       "57          Divisão    329  336\n",
       "58               de    337  339\n",
       "59     Qualificação    340  352\n",
       "60                e    353  354\n",
       "61  Aperfeiçoamento    355  370\n",
       "62                ,    370  371\n",
       "63              com    372  375\n",
       "64                a    376  377\n",
       "65       finalidade    378  388\n",
       "66               de    389  391\n",
       "67         realizar    392  400\n",
       "68           visita    401  407\n",
       "69                à    408  409\n",
       "70     Universidade    410  422\n",
       "71               do    423  425\n",
       "72            Porto    426  431\n",
       "73                ,    431  432\n",
       "74               em    433  435\n",
       "75            Porto    436  441\n",
       "76                ,    441  442\n",
       "77         Portugal    443  451\n",
       "78                ,    451  452\n",
       "79               no    453  455\n",
       "80          período    456  463\n",
       "81     compreendido    464  476\n",
       "82            entre    477  482\n",
       "83       26/01/2019    483  493\n",
       "84                e    494  495\n",
       "85       14/02/2019    496  506\n",
       "86                ,    506  507\n",
       "87         incluído    508  516\n",
       "88         trânsito    517  525\n",
       "89                ,    525  526\n",
       "90              com    527  530\n",
       "91             ônus    531  535\n",
       "92            UFRGS    536  541\n",
       "93                .    541  542"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_token_df[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have, for each sentences, a dataframe of all the word and a list with all the labels.\n",
    "What needs to be done is assign the correct label to each word of the tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token_df in enumerate(list_of_token_df):\n",
    "    labels = []\n",
    "    is_entity = False\n",
    "    for idx, token in token_df.iterrows():\n",
    "        for label in annotated_sentences['label'].loc[index]:\n",
    "            if token['start'] == label[0]:\n",
    "                labels.append('B-' + label[2])\n",
    "                is_entity = True\n",
    "                break\n",
    "            if token['start'] >= label[0] and token['end'] <= label[1]:\n",
    "                labels.append('I-' + label[2])\n",
    "                is_entity = True\n",
    "                break\n",
    "            is_entity = False\n",
    "        if is_entity == False:\n",
    "            labels.append('O')\n",
    "    token_df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 2) \n",
      " 57\n"
     ]
    }
   ],
   "source": [
    "print(annotated_sentences.shape,'\\n',\n",
    "      len(list_of_token_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the tokens are correctly assigned to each label.\n",
    "#for index in range(0, 89): \n",
    "    #print(annotated_sentences['label'].loc[index])\n",
    "    #print(list_of_token_df[index][list_of_token_df[index]['label'] != 'O'])\n",
    "    #print(list_of_token_df[index]['label'].value_counts())\n",
    "    #print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = ['O', 'B-PERSON', 'I-PERSON', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-LOCATION', 'I-LOCATION', 'B-MISCELLANEOUS', 'I-MISCELLANEOUS']\n",
    "\n",
    "# Convert label from name to id\n",
    "label_to_id = {\n",
    "    'O': 0, \n",
    "    'B-PERSON': 1, \n",
    "    'I-PERSON': 2, \n",
    "    'B-ORGANIZATION': 3, \n",
    "    'I-ORGANIZATION': 4, \n",
    "    'B-LOCATION': 5, \n",
    "    'I-LOCATION': 6, \n",
    "    'B-MISCELLANEOUS': 7, \n",
    "    'I-MISCELLANEOUS': 8\n",
    "}\n",
    "\n",
    "id_to_label = {\n",
    "    0: 'O', \n",
    "    1: 'B-PERSON', \n",
    "    2: 'I-PERSON', \n",
    "    3: 'B-ORGANIZATION', \n",
    "    4: 'I-ORGANIZATION', \n",
    "    5: 'B-LOCATION', \n",
    "    6: 'I-LOCATION', \n",
    "    7: 'B-MISCELLANEOUS', \n",
    "    8: 'I-MISCELLANEOUS'\n",
    "}\n",
    "for sentence in list_of_token_df:\n",
    "    sentence['label'] = sentence['label'].map(label_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PERSON',\n",
       " 2: 'I-PERSON',\n",
       " 3: 'B-ORGANIZATION',\n",
       " 4: 'I-ORGANIZATION',\n",
       " 5: 'B-LOCATION',\n",
       " 6: 'I-LOCATION',\n",
       " 7: 'B-MISCELLANEOUS',\n",
       " 8: 'I-MISCELLANEOUS'}"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PORTARIA</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nº</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1958</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>05/03/2020</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PRÓ-REITOR</td>\n",
       "      <td>37</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DE</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GESTÃO</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DE</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PESSOAS</td>\n",
       "      <td>61</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DA</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>UNIVERSIDADE</td>\n",
       "      <td>72</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FEDERAL</td>\n",
       "      <td>85</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DO</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RIO</td>\n",
       "      <td>96</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GRANDE</td>\n",
       "      <td>100</td>\n",
       "      <td>106</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DO</td>\n",
       "      <td>107</td>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SUL</td>\n",
       "      <td>110</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>,</td>\n",
       "      <td>113</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>no</td>\n",
       "      <td>115</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>uso</td>\n",
       "      <td>118</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>de</td>\n",
       "      <td>122</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>suas</td>\n",
       "      <td>125</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>atribuições</td>\n",
       "      <td>130</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>que</td>\n",
       "      <td>142</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lhe</td>\n",
       "      <td>146</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>foram</td>\n",
       "      <td>150</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>conferidas</td>\n",
       "      <td>156</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>pela</td>\n",
       "      <td>167</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Portaria</td>\n",
       "      <td>172</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>nº</td>\n",
       "      <td>181</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7684</td>\n",
       "      <td>184</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>,</td>\n",
       "      <td>188</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>de</td>\n",
       "      <td>190</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>03</td>\n",
       "      <td>193</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>de</td>\n",
       "      <td>196</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>outubro</td>\n",
       "      <td>199</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>de</td>\n",
       "      <td>207</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2016</td>\n",
       "      <td>210</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>,</td>\n",
       "      <td>214</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>do</td>\n",
       "      <td>216</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Magnífico</td>\n",
       "      <td>219</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Reitor</td>\n",
       "      <td>229</td>\n",
       "      <td>235</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>,</td>\n",
       "      <td>235</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>e</td>\n",
       "      <td>237</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>conforme</td>\n",
       "      <td>239</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>o</td>\n",
       "      <td>248</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Laudo</td>\n",
       "      <td>250</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Médico</td>\n",
       "      <td>256</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>n°60131</td>\n",
       "      <td>263</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>,</td>\n",
       "      <td>270</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>RESOLVE</td>\n",
       "      <td>272</td>\n",
       "      <td>279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Designar</td>\n",
       "      <td>280</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>,</td>\n",
       "      <td>288</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>temporariamente</td>\n",
       "      <td>290</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>,</td>\n",
       "      <td>305</td>\n",
       "      <td>306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>nos</td>\n",
       "      <td>307</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>termos</td>\n",
       "      <td>311</td>\n",
       "      <td>317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>da</td>\n",
       "      <td>318</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Lei</td>\n",
       "      <td>321</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>nº</td>\n",
       "      <td>325</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>8.112</td>\n",
       "      <td>328</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>,</td>\n",
       "      <td>333</td>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>de</td>\n",
       "      <td>335</td>\n",
       "      <td>337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>11</td>\n",
       "      <td>338</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>de</td>\n",
       "      <td>341</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>dezembro</td>\n",
       "      <td>344</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>de</td>\n",
       "      <td>353</td>\n",
       "      <td>355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1990</td>\n",
       "      <td>356</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>,</td>\n",
       "      <td>360</td>\n",
       "      <td>361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>com</td>\n",
       "      <td>362</td>\n",
       "      <td>365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>redação</td>\n",
       "      <td>366</td>\n",
       "      <td>373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>dada</td>\n",
       "      <td>374</td>\n",
       "      <td>378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>pela</td>\n",
       "      <td>379</td>\n",
       "      <td>383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Lei</td>\n",
       "      <td>384</td>\n",
       "      <td>387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>nº</td>\n",
       "      <td>388</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>9.527</td>\n",
       "      <td>391</td>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>,</td>\n",
       "      <td>396</td>\n",
       "      <td>397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>de</td>\n",
       "      <td>398</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>10</td>\n",
       "      <td>401</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>de</td>\n",
       "      <td>404</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>dezembro</td>\n",
       "      <td>407</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>de</td>\n",
       "      <td>416</td>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1997</td>\n",
       "      <td>419</td>\n",
       "      <td>423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>,</td>\n",
       "      <td>423</td>\n",
       "      <td>424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>o</td>\n",
       "      <td>425</td>\n",
       "      <td>426</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>ocupante</td>\n",
       "      <td>427</td>\n",
       "      <td>435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>do</td>\n",
       "      <td>436</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>cargo</td>\n",
       "      <td>439</td>\n",
       "      <td>444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>de</td>\n",
       "      <td>445</td>\n",
       "      <td>447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>ADMINISTRADOR</td>\n",
       "      <td>448</td>\n",
       "      <td>461</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>,</td>\n",
       "      <td>461</td>\n",
       "      <td>462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>do</td>\n",
       "      <td>463</td>\n",
       "      <td>465</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Quadro</td>\n",
       "      <td>466</td>\n",
       "      <td>472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>de</td>\n",
       "      <td>473</td>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Pessoal</td>\n",
       "      <td>476</td>\n",
       "      <td>483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>desta</td>\n",
       "      <td>484</td>\n",
       "      <td>489</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Universidade</td>\n",
       "      <td>490</td>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>,</td>\n",
       "      <td>502</td>\n",
       "      <td>503</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>MARCOS</td>\n",
       "      <td>504</td>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>JOSE</td>\n",
       "      <td>511</td>\n",
       "      <td>515</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>DA</td>\n",
       "      <td>516</td>\n",
       "      <td>518</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>SILVA</td>\n",
       "      <td>519</td>\n",
       "      <td>524</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(</td>\n",
       "      <td>525</td>\n",
       "      <td>526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Siape</td>\n",
       "      <td>526</td>\n",
       "      <td>531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>:</td>\n",
       "      <td>531</td>\n",
       "      <td>532</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1778855</td>\n",
       "      <td>533</td>\n",
       "      <td>540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>)</td>\n",
       "      <td>541</td>\n",
       "      <td>542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>,</td>\n",
       "      <td>542</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>para</td>\n",
       "      <td>544</td>\n",
       "      <td>548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>substituir</td>\n",
       "      <td>549</td>\n",
       "      <td>559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>TURENE</td>\n",
       "      <td>560</td>\n",
       "      <td>566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>ANDRADE</td>\n",
       "      <td>567</td>\n",
       "      <td>574</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>E</td>\n",
       "      <td>575</td>\n",
       "      <td>576</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>SILVA</td>\n",
       "      <td>577</td>\n",
       "      <td>582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>NETO</td>\n",
       "      <td>583</td>\n",
       "      <td>587</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>(</td>\n",
       "      <td>588</td>\n",
       "      <td>589</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Siape</td>\n",
       "      <td>589</td>\n",
       "      <td>594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>:</td>\n",
       "      <td>594</td>\n",
       "      <td>595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0356721</td>\n",
       "      <td>596</td>\n",
       "      <td>603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>)</td>\n",
       "      <td>604</td>\n",
       "      <td>605</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>,</td>\n",
       "      <td>605</td>\n",
       "      <td>606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Diretor</td>\n",
       "      <td>607</td>\n",
       "      <td>614</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>da</td>\n",
       "      <td>615</td>\n",
       "      <td>617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Divisão</td>\n",
       "      <td>618</td>\n",
       "      <td>625</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>de</td>\n",
       "      <td>626</td>\n",
       "      <td>628</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Preparo</td>\n",
       "      <td>629</td>\n",
       "      <td>636</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>da</td>\n",
       "      <td>637</td>\n",
       "      <td>639</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Licitação</td>\n",
       "      <td>640</td>\n",
       "      <td>649</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>do</td>\n",
       "      <td>650</td>\n",
       "      <td>652</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Departamento</td>\n",
       "      <td>653</td>\n",
       "      <td>665</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>de</td>\n",
       "      <td>666</td>\n",
       "      <td>668</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Aquisição</td>\n",
       "      <td>669</td>\n",
       "      <td>678</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>de</td>\n",
       "      <td>679</td>\n",
       "      <td>681</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Bens</td>\n",
       "      <td>682</td>\n",
       "      <td>686</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>e</td>\n",
       "      <td>687</td>\n",
       "      <td>688</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Serviços</td>\n",
       "      <td>689</td>\n",
       "      <td>697</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>,</td>\n",
       "      <td>697</td>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Código</td>\n",
       "      <td>699</td>\n",
       "      <td>705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>FG-4</td>\n",
       "      <td>706</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>,</td>\n",
       "      <td>710</td>\n",
       "      <td>711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>em</td>\n",
       "      <td>712</td>\n",
       "      <td>714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>seu</td>\n",
       "      <td>715</td>\n",
       "      <td>718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>afastamento</td>\n",
       "      <td>719</td>\n",
       "      <td>730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>por</td>\n",
       "      <td>731</td>\n",
       "      <td>734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>motivo</td>\n",
       "      <td>735</td>\n",
       "      <td>741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>de</td>\n",
       "      <td>742</td>\n",
       "      <td>744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Laudo</td>\n",
       "      <td>745</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Médico</td>\n",
       "      <td>751</td>\n",
       "      <td>757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>do</td>\n",
       "      <td>758</td>\n",
       "      <td>760</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>titular</td>\n",
       "      <td>761</td>\n",
       "      <td>768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>da</td>\n",
       "      <td>769</td>\n",
       "      <td>771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Função</td>\n",
       "      <td>772</td>\n",
       "      <td>778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>,</td>\n",
       "      <td>778</td>\n",
       "      <td>779</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>no</td>\n",
       "      <td>780</td>\n",
       "      <td>782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>período</td>\n",
       "      <td>783</td>\n",
       "      <td>790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>de</td>\n",
       "      <td>791</td>\n",
       "      <td>793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>16/03/2020</td>\n",
       "      <td>794</td>\n",
       "      <td>804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>a</td>\n",
       "      <td>805</td>\n",
       "      <td>806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>28/04/2020</td>\n",
       "      <td>807</td>\n",
       "      <td>817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>,</td>\n",
       "      <td>817</td>\n",
       "      <td>818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>com</td>\n",
       "      <td>819</td>\n",
       "      <td>822</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>o</td>\n",
       "      <td>823</td>\n",
       "      <td>824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>decorrente</td>\n",
       "      <td>825</td>\n",
       "      <td>835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>pagamento</td>\n",
       "      <td>836</td>\n",
       "      <td>845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>das</td>\n",
       "      <td>846</td>\n",
       "      <td>849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>vantagens</td>\n",
       "      <td>850</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>por</td>\n",
       "      <td>860</td>\n",
       "      <td>863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>44</td>\n",
       "      <td>864</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>dias</td>\n",
       "      <td>867</td>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>.</td>\n",
       "      <td>871</td>\n",
       "      <td>872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               token  start  end  label\n",
       "0                1/1      0    3      0\n",
       "1           PORTARIA      4   12      0\n",
       "2                 Nº     13   15      0\n",
       "3               1958     16   20      0\n",
       "4                 de     21   23      0\n",
       "5         05/03/2020     24   34      0\n",
       "6                  O     35   36      0\n",
       "7         PRÓ-REITOR     37   47      0\n",
       "8                 DE     48   50      0\n",
       "9             GESTÃO     51   57      0\n",
       "10                DE     58   60      0\n",
       "11           PESSOAS     61   68      0\n",
       "12                DA     69   71      0\n",
       "13      UNIVERSIDADE     72   84      3\n",
       "14           FEDERAL     85   92      4\n",
       "15                DO     93   95      4\n",
       "16               RIO     96   99      4\n",
       "17            GRANDE    100  106      4\n",
       "18                DO    107  109      4\n",
       "19               SUL    110  113      4\n",
       "20                 ,    113  114      0\n",
       "21                no    115  117      0\n",
       "22               uso    118  121      0\n",
       "23                de    122  124      0\n",
       "24              suas    125  129      0\n",
       "25       atribuições    130  141      0\n",
       "26               que    142  145      0\n",
       "27               lhe    146  149      0\n",
       "28             foram    150  155      0\n",
       "29        conferidas    156  166      0\n",
       "30              pela    167  171      0\n",
       "31          Portaria    172  180      0\n",
       "32                nº    181  183      0\n",
       "33              7684    184  188      0\n",
       "34                 ,    188  189      0\n",
       "35                de    190  192      0\n",
       "36                03    193  195      0\n",
       "37                de    196  198      0\n",
       "38           outubro    199  206      0\n",
       "39                de    207  209      0\n",
       "40              2016    210  214      0\n",
       "41                 ,    214  215      0\n",
       "42                do    216  218      0\n",
       "43         Magnífico    219  228      0\n",
       "44            Reitor    229  235      7\n",
       "45                 ,    235  236      0\n",
       "46                 e    237  238      0\n",
       "47          conforme    239  247      0\n",
       "48                 o    248  249      0\n",
       "49             Laudo    250  255      0\n",
       "50            Médico    256  262      0\n",
       "51           n°60131    263  270      0\n",
       "52                 ,    270  271      0\n",
       "53           RESOLVE    272  279      0\n",
       "54          Designar    280  288      0\n",
       "55                 ,    288  289      0\n",
       "56   temporariamente    290  305      0\n",
       "57                 ,    305  306      0\n",
       "58               nos    307  310      0\n",
       "59            termos    311  317      0\n",
       "60                da    318  320      0\n",
       "61               Lei    321  324      0\n",
       "62                nº    325  327      0\n",
       "63             8.112    328  333      0\n",
       "64                 ,    333  334      0\n",
       "65                de    335  337      0\n",
       "66                11    338  340      0\n",
       "67                de    341  343      0\n",
       "68          dezembro    344  352      0\n",
       "69                de    353  355      0\n",
       "70              1990    356  360      0\n",
       "71                 ,    360  361      0\n",
       "72               com    362  365      0\n",
       "73           redação    366  373      0\n",
       "74              dada    374  378      0\n",
       "75              pela    379  383      0\n",
       "76               Lei    384  387      0\n",
       "77                nº    388  390      0\n",
       "78             9.527    391  396      0\n",
       "79                 ,    396  397      0\n",
       "80                de    398  400      0\n",
       "81                10    401  403      0\n",
       "82                de    404  406      0\n",
       "83          dezembro    407  415      0\n",
       "84                de    416  418      0\n",
       "85              1997    419  423      0\n",
       "86                 ,    423  424      0\n",
       "87                 o    425  426      0\n",
       "88          ocupante    427  435      0\n",
       "89                do    436  438      0\n",
       "90             cargo    439  444      0\n",
       "91                de    445  447      0\n",
       "92     ADMINISTRADOR    448  461      7\n",
       "93                 ,    461  462      0\n",
       "94                do    463  465      0\n",
       "95            Quadro    466  472      0\n",
       "96                de    473  475      0\n",
       "97           Pessoal    476  483      0\n",
       "98             desta    484  489      0\n",
       "99      Universidade    490  502      0\n",
       "100                ,    502  503      0\n",
       "101           MARCOS    504  510      1\n",
       "102             JOSE    511  515      2\n",
       "103               DA    516  518      2\n",
       "104            SILVA    519  524      2\n",
       "105                (    525  526      0\n",
       "106            Siape    526  531      0\n",
       "107                :    531  532      0\n",
       "108          1778855    533  540      0\n",
       "109                )    541  542      0\n",
       "110                ,    542  543      0\n",
       "111             para    544  548      0\n",
       "112       substituir    549  559      0\n",
       "113           TURENE    560  566      1\n",
       "114          ANDRADE    567  574      2\n",
       "115                E    575  576      2\n",
       "116            SILVA    577  582      2\n",
       "117             NETO    583  587      2\n",
       "118                (    588  589      0\n",
       "119            Siape    589  594      0\n",
       "120                :    594  595      0\n",
       "121          0356721    596  603      0\n",
       "122                )    604  605      0\n",
       "123                ,    605  606      0\n",
       "124          Diretor    607  614      7\n",
       "125               da    615  617      0\n",
       "126          Divisão    618  625      3\n",
       "127               de    626  628      4\n",
       "128          Preparo    629  636      4\n",
       "129               da    637  639      4\n",
       "130        Licitação    640  649      4\n",
       "131               do    650  652      0\n",
       "132     Departamento    653  665      3\n",
       "133               de    666  668      4\n",
       "134        Aquisição    669  678      4\n",
       "135               de    679  681      4\n",
       "136             Bens    682  686      4\n",
       "137                e    687  688      4\n",
       "138         Serviços    689  697      4\n",
       "139                ,    697  698      0\n",
       "140           Código    699  705      0\n",
       "141             FG-4    706  710      0\n",
       "142                ,    710  711      0\n",
       "143               em    712  714      0\n",
       "144              seu    715  718      0\n",
       "145      afastamento    719  730      0\n",
       "146              por    731  734      0\n",
       "147           motivo    735  741      0\n",
       "148               de    742  744      0\n",
       "149            Laudo    745  750      0\n",
       "150           Médico    751  757      0\n",
       "151               do    758  760      0\n",
       "152          titular    761  768      0\n",
       "153               da    769  771      0\n",
       "154           Função    772  778      0\n",
       "155                ,    778  779      0\n",
       "156               no    780  782      0\n",
       "157          período    783  790      0\n",
       "158               de    791  793      0\n",
       "159       16/03/2020    794  804      0\n",
       "160                a    805  806      0\n",
       "161       28/04/2020    807  817      0\n",
       "162                ,    817  818      0\n",
       "163              com    819  822      0\n",
       "164                o    823  824      0\n",
       "165       decorrente    825  835      0\n",
       "166        pagamento    836  845      0\n",
       "167              das    846  849      0\n",
       "168        vantagens    850  859      0\n",
       "169              por    860  863      0\n",
       "170               44    864  866      0\n",
       "171             dias    867  871      0\n",
       "172                .    871  872      0"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_token_df[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a dataset from the datasets library\n",
    "The easiest way to use our data with HuggingFace is to use the datasets library. It allows us to import our own data and it will format it into a Dataset Object that is ready to be used by the NER model.\n",
    "\n",
    "To understand how our data need to be formatted let's explore a sample dataset that already exists inside the datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (C:/Users/arthu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe23f0fd7a94e09a5718e93f9a60426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = load_dataset('conll2003')\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of DatasetDict, which we'll want to use, is a list of three arrow Datasets: train, test and validation. \n",
    "Each Dataset is composed of two main object: features and num_rows. We need to make sure our JSON has the features 'tokens' and 'ner_tags'\n",
    "The sample data uses the following dictionary to convert each label to an int:\n",
    "\n",
    "**{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}**\n",
    "\n",
    "Since we are using the exact same labels we can utilize this dictionary as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
       " ['Peter', 'Blackburn'],\n",
       " ['BRUSSELS', '1996-08-22']]"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data['train']['tokens'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 0, 7, 0, 0, 0, 7, 0, 0], [1, 2], [5, 0]]"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data['train']['ner_tags'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset into train and test\n",
    "We'll split our DataFrame into to lists of lists. One for the input tokens and another for the labels.\n",
    "After that we'll use the scklearn train_test_split method to get both our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide inputs and tokens into X and y lists.\n",
    "sample_X = []\n",
    "sample_y = []\n",
    "for sentence in list_of_token_df:\n",
    "    sample_X.append(list(sentence['token']))\n",
    "    sample_y.append(list(sentence['label']))\n",
    "\n",
    "# Split X and y into train and test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_X, sample_y, test_size=0.33, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_distribution(seq_labels):\n",
    "    label_count = {}\n",
    "    for label in target_labels:\n",
    "        label_count[label] = 0\n",
    "    for seq in seq_labels:\n",
    "        for target_id in seq:\n",
    "            label = id_to_label[target_id]\n",
    "            label_count[label] += 1\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 2495,\n",
       " 'B-PERSON': 52,\n",
       " 'I-PERSON': 138,\n",
       " 'B-ORGANIZATION': 68,\n",
       " 'I-ORGANIZATION': 233,\n",
       " 'B-LOCATION': 7,\n",
       " 'I-LOCATION': 4,\n",
       " 'B-MISCELLANEOUS': 78,\n",
       " 'I-MISCELLANEOUS': 132}"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1089,\n",
       " 'B-PERSON': 22,\n",
       " 'I-PERSON': 53,\n",
       " 'B-ORGANIZATION': 30,\n",
       " 'I-ORGANIZATION': 126,\n",
       " 'B-LOCATION': 2,\n",
       " 'I-LOCATION': 0,\n",
       " 'B-MISCELLANEOUS': 33,\n",
       " 'I-MISCELLANEOUS': 52}"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_distribution(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train and test dictionary\n",
    "train_data = {'inputs': X_train, 'targets': y_train}\n",
    "test_data = {'inputs': X_test, 'targets': y_test}\n",
    "\n",
    "#Convert dictionary into DataFrame\n",
    "#Needed as intermediary step because DataFrames support convertion into the json record format we need.\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "#Convert DataFrame into json\n",
    "train_json = train_df.to_json(orient='records')\n",
    "test_json = test_df.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save train and test jsons in the auto_data directory.\n",
    "#This directory will serve as the repository of our auto labeled data and we'll use it to import the data with the datasets library.\n",
    "import os\n",
    "os.path\n",
    "file_path = \"C:\\\\Users\\\\arthu\\\\Desktop\\\\ner-using-bert\\BERT_Experiment\\\\auto_data\\\\\"\n",
    "\n",
    "with open(file_path+'train.json', 'w') as outfile:\n",
    "    outfile.write(train_json)\n",
    "\n",
    "with open(file_path+'test.json', 'w') as outfile:\n",
    "    outfile.write(test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and validate\n",
    "We load the dataset that we saved previously. We'll use the load_dataset method from the datasets library, which will allow us to easily use hugging face models with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/arthu/.cache/huggingface/datasets/json/default-b4d3e01485de334b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d16d97e89134fb6bea177aab4b49d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfaa6d842ac4d078d15dd4013129d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa816cde17f4334948e1f1137af1658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5baeb266f34ba3bf56c25f96941f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/arthu/.cache/huggingface/datasets/json/default-b4d3e01485de334b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc0035bfa3641c89e84d67619ad25fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ufrgs_data = load_dataset('json', data_dir = file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we get a DatasetDict object with two Datasets, one for train and one for test.\n",
    "ufrgs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Documento', 'gerado', 'sob', 'autenticação', 'Nº', 'QON.500.984.BHA', ',', 'disponível', 'no', 'endereço', 'http', ':', '//www.ufrgs.br/autenticacao', '1/1', 'PORTARIA', 'Nº', '1184', 'de', '18/02/2016', 'O', 'PRÓ-REITOR', 'DE', 'GESTÃO', 'DE', 'PESSOAS', 'DA', 'UNIVERSIDADE', 'FEDERAL', 'DO', 'RIO', 'GRANDE', 'DO', 'SUL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', 'que', 'lhe', 'foram', 'conferidas', 'pela', 'Portaria', 'nº', '5469', ',', 'de', '04', 'de', 'outubro', 'de', '2012', ',', 'do', 'Magnífico', 'Reitor', ',', 'e', 'conforme', 'o', 'Laudo', 'Médico', 'n°37308', ',', 'RESOLVE', ':', 'Designar', ',', 'temporariamente', ',', 'nos', 'termos', 'da', 'Lei', 'nº', '8.112', ',', 'de', '11', 'de', 'dezembro', 'de', '1990', ',', 'com', 'redação', 'dada', 'pela', 'Lei', 'nº', '9.527', ',', 'de', '10', 'de', 'dezembro', 'de', '1997', ',', 'a', 'ocupante', 'do', 'cargo', 'de', 'PORTEIRO', ',', 'do', 'Quadro', 'de', 'Pessoal', 'desta', 'Universidade', ',', 'ELIANE', 'RICARDO', 'IRANCO', '(', 'Siape', ':', '0359359', ')', ',', 'para', 'substituir', 'SULAMAR', 'FIGUEIRA', 'MARCELINO', '(', 'Siape', ':', '0357487', ')', ',', 'Chefe', 'do', 'Setor', 'de', 'Infraestrutura', 'e', 'Patrimônio', 'da', 'Gerência', 'Administrativa', 'do', 'Instituto', 'de', 'Informática', ',', 'em', 'seu', 'afastamento', 'por', 'motivo', 'de', 'Laudo', 'Médico', 'do', 'titular', 'da', 'Função', ',', 'no', 'período', 'de', '18/01/2016', 'a', '10/02/2016', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 8, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(ufrgs_data['train'][12]['inputs'])\n",
    "print(ufrgs_data['train'][12]['targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "We create a tokenizer to convert our inputs into sub-word ids. We need to use a tokenizer that is compatible with the model we'll use.\n",
    "HuggingFace makes that easy through the AutoTokenizer, which allows us to specify which model will be used and it already makes sure that our tokenizer will work with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'neuralmind/bert-base-portuguese-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 19816, 310, 14928, 425, 16782, 874, 100, 5226, 12234, 119, 5047, 119, 15405, 22336, 119, 241, 18394, 117, 5656, 202, 14441, 14305, 131, 120, 120, 2702, 11740, 119, 169, 2527, 6891, 119, 235, 22282, 120, 16782, 232, 304, 22280, 205, 120, 205, 212, 8718, 5118, 21748, 22301, 100, 17263, 22336, 125, 542, 120, 16956, 120, 4284, 231, 11635, 22369, 118, 257, 18469, 15349, 22322, 10836, 278, 3341, 22321, 16484, 10836, 212, 3341, 19715, 4089, 250, 22301, 7281, 9846, 5054, 22308, 6392, 11836, 22309, 263, 12002, 5054, 9369, 15040, 257, 15749, 278, 5650, 22320, 7545, 15040, 200, 18199, 117, 202, 1700, 125, 675, 20215, 179, 2036, 506, 7940, 649, 412, 14120, 322, 100, 11365, 10852, 117, 125, 16720, 125, 1511, 125, 3470, 117, 171, 13128, 3313, 3501, 428, 117, 122, 4762, 146, 11706, 243, 21491, 149, 22359, 9330, 3708, 22330, 117, 257, 3341, 7918, 22339, 22309, 131, 15945, 159, 117, 12885, 117, 538, 3401, 180, 2502, 100, 1015, 119, 21950, 117, 125, 1433, 125, 1512, 125, 5737, 117, 170, 16517, 5180, 412, 2502, 100, 1117, 119, 10596, 22337, 117, 125, 1193, 125, 1512, 125, 6827, 117, 123, 3045, 175, 171, 2466, 125, 212, 8718, 16017, 15710, 22317, 117, 171, 12106, 157, 125, 10304, 22290, 1014, 1582, 117, 192, 22327, 5234, 22320, 22309, 257, 6162, 6765, 18504, 290, 5650, 22320, 5218, 113, 9278, 7904, 22279, 131, 19148, 22334, 21509, 22334, 22315, 114, 117, 221, 8281, 200, 18199, 8314, 6765, 5769, 22328, 22341, 18469, 5650, 18471, 18532, 9008, 7073, 22317, 113, 9278, 7904, 22279, 131, 19148, 21906, 8706, 22337, 114, 117, 11566, 171, 530, 428, 125, 5027, 124, 5706, 122, 12995, 180, 5685, 399, 5862, 8865, 171, 2900, 125, 5027, 19560, 1443, 117, 173, 347, 18529, 240, 5607, 125, 11706, 243, 21491, 171, 5621, 180, 11513, 182, 117, 202, 1254, 125, 542, 120, 13778, 120, 4284, 123, 1193, 120, 16956, 120, 4284, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(ufrgs_data['train'][12]['inputs'], is_split_into_words=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The tokenizer return looks like a dictionary but it is actually an object called BatchEncoding\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Docu',\n",
       " '##mento',\n",
       " 'gerado',\n",
       " 'sob',\n",
       " 'autent',\n",
       " '##icação',\n",
       " '[UNK]',\n",
       " 'Q',\n",
       " '##ON',\n",
       " '.',\n",
       " '500',\n",
       " '.',\n",
       " '98',\n",
       " '##4',\n",
       " '.',\n",
       " 'B',\n",
       " '##HA',\n",
       " ',',\n",
       " 'disponível',\n",
       " 'no',\n",
       " 'endereço',\n",
       " 'http',\n",
       " ':',\n",
       " '/',\n",
       " '/',\n",
       " 'w',\n",
       " '##ww',\n",
       " '.',\n",
       " 'u',\n",
       " '##fr',\n",
       " '##gs',\n",
       " '.',\n",
       " 'b',\n",
       " '##r',\n",
       " '/',\n",
       " 'autent',\n",
       " '##ica',\n",
       " '##ca',\n",
       " '##o',\n",
       " '1',\n",
       " '/',\n",
       " '1',\n",
       " 'P',\n",
       " '##OR',\n",
       " '##TA',\n",
       " '##RI',\n",
       " '##A',\n",
       " '[UNK]',\n",
       " '118',\n",
       " '##4',\n",
       " 'de',\n",
       " '18',\n",
       " '/',\n",
       " '02',\n",
       " '/',\n",
       " '2016',\n",
       " 'O',\n",
       " 'PR',\n",
       " '##Ó',\n",
       " '-',\n",
       " 'R',\n",
       " '##EI',\n",
       " '##TO',\n",
       " '##R',\n",
       " 'DE',\n",
       " 'G',\n",
       " '##ES',\n",
       " '##T',\n",
       " '##ÃO',\n",
       " 'DE',\n",
       " 'P',\n",
       " '##ES',\n",
       " '##SO',\n",
       " '##AS',\n",
       " 'D',\n",
       " '##A',\n",
       " 'UN',\n",
       " '##IV',\n",
       " '##ER',\n",
       " '##S',\n",
       " '##ID',\n",
       " '##AD',\n",
       " '##E',\n",
       " 'F',\n",
       " '##ED',\n",
       " '##ER',\n",
       " '##AL',\n",
       " 'DO',\n",
       " 'R',\n",
       " '##IO',\n",
       " 'G',\n",
       " '##RA',\n",
       " '##N',\n",
       " '##DE',\n",
       " 'DO',\n",
       " 'S',\n",
       " '##UL',\n",
       " ',',\n",
       " 'no',\n",
       " 'uso',\n",
       " 'de',\n",
       " 'suas',\n",
       " 'atribuições',\n",
       " 'que',\n",
       " 'lhe',\n",
       " 'foram',\n",
       " 'confer',\n",
       " '##idas',\n",
       " 'pela',\n",
       " 'Porta',\n",
       " '##ria',\n",
       " '[UNK]',\n",
       " '54',\n",
       " '##69',\n",
       " ',',\n",
       " 'de',\n",
       " '04',\n",
       " 'de',\n",
       " 'outubro',\n",
       " 'de',\n",
       " '2012',\n",
       " ',',\n",
       " 'do',\n",
       " 'Magn',\n",
       " '##ífico',\n",
       " 'Rei',\n",
       " '##tor',\n",
       " ',',\n",
       " 'e',\n",
       " 'conforme',\n",
       " 'o',\n",
       " 'Lau',\n",
       " '##do',\n",
       " 'Médico',\n",
       " 'n',\n",
       " '##°',\n",
       " '##37',\n",
       " '##30',\n",
       " '##8',\n",
       " ',',\n",
       " 'R',\n",
       " '##ES',\n",
       " '##OL',\n",
       " '##V',\n",
       " '##E',\n",
       " ':',\n",
       " 'Design',\n",
       " '##ar',\n",
       " ',',\n",
       " 'temporariamente',\n",
       " ',',\n",
       " 'nos',\n",
       " 'termos',\n",
       " 'da',\n",
       " 'Lei',\n",
       " '[UNK]',\n",
       " '8',\n",
       " '.',\n",
       " '112',\n",
       " ',',\n",
       " 'de',\n",
       " '11',\n",
       " 'de',\n",
       " 'dezembro',\n",
       " 'de',\n",
       " '1990',\n",
       " ',',\n",
       " 'com',\n",
       " 'redação',\n",
       " 'dada',\n",
       " 'pela',\n",
       " 'Lei',\n",
       " '[UNK]',\n",
       " '9',\n",
       " '.',\n",
       " '52',\n",
       " '##7',\n",
       " ',',\n",
       " 'de',\n",
       " '10',\n",
       " 'de',\n",
       " 'dezembro',\n",
       " 'de',\n",
       " '1997',\n",
       " ',',\n",
       " 'a',\n",
       " 'ocupa',\n",
       " '##nte',\n",
       " 'do',\n",
       " 'cargo',\n",
       " 'de',\n",
       " 'P',\n",
       " '##OR',\n",
       " '##TE',\n",
       " '##IR',\n",
       " '##O',\n",
       " ',',\n",
       " 'do',\n",
       " 'Quad',\n",
       " '##ro',\n",
       " 'de',\n",
       " 'Pessoa',\n",
       " '##l',\n",
       " 'desta',\n",
       " 'Universidade',\n",
       " ',',\n",
       " 'E',\n",
       " '##L',\n",
       " '##IA',\n",
       " '##N',\n",
       " '##E',\n",
       " 'R',\n",
       " '##IC',\n",
       " '##AR',\n",
       " '##DO',\n",
       " 'I',\n",
       " '##RA',\n",
       " '##N',\n",
       " '##CO',\n",
       " '(',\n",
       " 'Si',\n",
       " '##ap',\n",
       " '##e',\n",
       " ':',\n",
       " '03',\n",
       " '##5',\n",
       " '##93',\n",
       " '##5',\n",
       " '##9',\n",
       " ')',\n",
       " ',',\n",
       " 'para',\n",
       " 'substituir',\n",
       " 'S',\n",
       " '##UL',\n",
       " '##AM',\n",
       " '##AR',\n",
       " 'FI',\n",
       " '##G',\n",
       " '##U',\n",
       " '##EI',\n",
       " '##RA',\n",
       " 'MA',\n",
       " '##RC',\n",
       " '##EL',\n",
       " '##IN',\n",
       " '##O',\n",
       " '(',\n",
       " 'Si',\n",
       " '##ap',\n",
       " '##e',\n",
       " ':',\n",
       " '03',\n",
       " '##57',\n",
       " '##48',\n",
       " '##7',\n",
       " ')',\n",
       " ',',\n",
       " 'Chefe',\n",
       " 'do',\n",
       " 'Se',\n",
       " '##tor',\n",
       " 'de',\n",
       " 'Inf',\n",
       " '##ra',\n",
       " '##estrutura',\n",
       " 'e',\n",
       " 'Patrimônio',\n",
       " 'da',\n",
       " 'Ger',\n",
       " '##ência',\n",
       " 'Administ',\n",
       " '##rativa',\n",
       " 'do',\n",
       " 'Instituto',\n",
       " 'de',\n",
       " 'Inf',\n",
       " '##orm',\n",
       " '##ática',\n",
       " ',',\n",
       " 'em',\n",
       " 'seu',\n",
       " 'afastamento',\n",
       " 'por',\n",
       " 'motivo',\n",
       " 'de',\n",
       " 'Lau',\n",
       " '##do',\n",
       " 'Médico',\n",
       " 'do',\n",
       " 'titular',\n",
       " 'da',\n",
       " 'Fun',\n",
       " '##ção',\n",
       " ',',\n",
       " 'no',\n",
       " 'período',\n",
       " 'de',\n",
       " '18',\n",
       " '/',\n",
       " '01',\n",
       " '/',\n",
       " '2016',\n",
       " 'a',\n",
       " '10',\n",
       " '/',\n",
       " '02',\n",
       " '/',\n",
       " '2016',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The object has a tokens method that returns the original tokens before transforming them into integers\n",
    "t.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 56,\n",
       " 57,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 143,\n",
       " 144,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 169,\n",
       " None]"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The word_ids methods returns a list with the ID that maps each sub-word to the original word it was tokenized from.\n",
    "t.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Alignment\n",
    "Now that our input is composed of sub-words, we need to make sure that we have one target per sub-word. To do this we will use the align_targets function and map targets from each word to its sub-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define relationship between B and I tags\n",
    "#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "begin2inside = {\n",
    "    1:2,\n",
    "    3:4,\n",
    "    5:6,\n",
    "    7:8\n",
    "}\n",
    "\n",
    "# Function that aligns the labels to be correctly associated with each sub-word.\n",
    "def align_targets(labels, word_ids):\n",
    "    aligned_labels = []\n",
    "    previous_word = None\n",
    "\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            # Tokens like [CLS] and [SEP]\n",
    "            label = -100 #This value is used by Hugging Face to ignore the tokens during training\n",
    "        elif word != previous_word:\n",
    "            # New word in the list\n",
    "            label = labels[word]\n",
    "        else:\n",
    "            #Repeated word (Would be the next sub-word)\n",
    "            if labels[word] in begin2inside:\n",
    "                #Change B- to I-\n",
    "                label = begin2inside[labels[word]]\n",
    "            else:\n",
    "                # Sub-word of a word classified as \"O\" gets the same label \"O\"\n",
    "                label = labels[word]\n",
    "\n",
    "        aligned_labels.append(label)\n",
    "        previous_word = word #update last word\n",
    " \n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Data: {'input_ids': [101, 205, 120, 205, 212, 8718, 5118, 21748, 22301, 100, 8103, 5752, 125, 16720, 120, 16899, 120, 5096, 177, 5427, 9617, 118, 257, 18469, 15349, 5650, 250, 22301, 7281, 9846, 5054, 22308, 6392, 11836, 22309, 263, 12002, 5054, 9369, 15040, 257, 15749, 278, 5650, 22320, 7545, 15040, 200, 18199, 117, 202, 1700, 125, 675, 20215, 117, 9319, 146, 16620, 229, 14120, 322, 100, 16444, 8510, 117, 125, 2939, 125, 1544, 125, 4284, 257, 3341, 7918, 22339, 22309, 3928, 15802, 6939, 150, 8346, 117, 240, 7887, 125, 4576, 117, 202, 12106, 157, 1014, 1582, 117, 320, 8922, 299, 6072, 22309, 17715, 22322, 22055, 22308, 278, 18178, 3341, 15040, 22308, 16288, 22333, 6072, 117, 12600, 14188, 200, 5234, 8214, 149, 22359, 22222, 4649, 22330, 22330, 117, 1340, 487, 122, 173, 8750, 202, 5985, 125, 8153, 1262, 171, 2900, 125, 9682, 122, 5503, 7023, 138, 117, 180, 3383, 250, 125, 8922, 8688, 243, 117, 2270, 19148, 117, 221, 123, 3383, 250, 125, 8922, 8688, 243, 117, 2270, 16720, 117, 15762, 320, 558, 7485, 826, 125, 542, 120, 1242, 120, 3618, 123, 1040, 120, 1242, 120, 4155, 117, 170, 3459, 399, 7500, 123, 1018, 125, 13778, 120, 16394, 120, 4284, 117, 125, 1365, 170, 146, 179, 13058, 123, 2502, 100, 1242, 119, 14332, 22313, 117, 125, 2593, 125, 1512, 125, 3470, 117, 170, 675, 6860, 122, 123, 4534, 758, 100, 889, 120, 3847, 118, 16273, 22308, 11964, 117, 16072, 412, 4534, 758, 100, 3467, 22302, 120, 3757, 118, 16273, 22308, 11964, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Word Tokens: ['1/1', 'PORTARIA', 'Nº', '3812', 'de', '04/05/2017', 'A', 'VICE-REITORA', 'DA', 'UNIVERSIDADE', 'FEDERAL', 'DO', 'RIO', 'GRANDE', 'DO', 'SUL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', ',', 'considerando', 'o', 'disposto', 'na', 'Portaria', 'nº', '7624', ',', 'de', '29', 'de', 'setembro', 'de', '2016', 'RESOLVE', 'Conceder', 'progressão', 'funcional', ',', 'por', 'avaliação', 'de', 'desempenho', ',', 'no', 'Quadro', 'desta', 'Universidade', ',', 'ao', 'Professor', 'JOSE', 'CARLOS', 'GOMES', 'DOS', 'ANJOS', ',', 'matrícula', 'SIAPE', 'n°', '1296088', ',', 'lotado', 'e', 'em', 'exercício', 'no', 'Departamento', 'de', 'Sociologia', 'do', 'Instituto', 'de', 'Filosofia', 'e', 'Ciências', 'Humanas', ',', 'da', 'classe', 'D', 'de', 'Professor', 'Associado', ',', 'nível', '03', ',', 'para', 'a', 'classe', 'D', 'de', 'Professor', 'Associado', ',', 'nível', '04', ',', 'referente', 'ao', 'interstício', 'de', '18/12/2011', 'a', '17/12/2015', ',', 'com', 'vigência', 'financeira', 'a', 'partir', 'de', '01/08/2016', ',', 'de', 'acordo', 'com', 'o', 'que', 'dispõe', 'a', 'Lei', 'nº', '12.772', ',', 'de', '28', 'de', 'dezembro', 'de', '2012', ',', 'com', 'suas', 'alterações', 'e', 'a', 'Decisão', 'nº', '197/2006-CONSUN', ',', 'alterada', 'pela', 'Decisão', 'nº', '401/2013-CONSUN', '.']\n",
      "Word Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 3, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Word IDs: [None, 0, 0, 0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 11, 12, 12, 13, 13, 13, 13, 14, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 27, 28, 29, 29, 30, 31, 32, 33, 34, 35, 36, 37, 37, 37, 37, 37, 38, 38, 39, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 48, 49, 50, 51, 52, 53, 54, 54, 54, 55, 55, 55, 55, 56, 56, 56, 57, 57, 58, 58, 58, 59, 60, 60, 61, 61, 61, 62, 62, 63, 63, 63, 63, 64, 65, 65, 66, 67, 68, 69, 70, 71, 72, 72, 73, 74, 75, 76, 77, 78, 79, 79, 80, 81, 82, 83, 84, 85, 86, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 97, 98, 99, 100, 101, 102, 103, 104, 104, 104, 105, 106, 106, 106, 106, 106, 107, 108, 108, 108, 108, 108, 109, 110, 111, 111, 112, 113, 114, 115, 116, 116, 116, 116, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 127, 127, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 141, 142, 143, 143, 143, 143, 143, 143, 143, 144, 145, 146, 147, 147, 148, 149, 149, 149, 149, 149, 149, 149, 149, 150, None]\n",
      "Sub-Word Labels: [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "Sub-Word Tokens: ['[CLS]', '1', '/', '1', 'P', '##OR', '##TA', '##RI', '##A', '[UNK]', '38', '##12', 'de', '04', '/', '05', '/', '2017', 'A', 'VI', '##CE', '-', 'R', '##EI', '##TO', '##RA', 'D', '##A', 'UN', '##IV', '##ER', '##S', '##ID', '##AD', '##E', 'F', '##ED', '##ER', '##AL', 'DO', 'R', '##IO', 'G', '##RA', '##N', '##DE', 'DO', 'S', '##UL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', ',', 'considerando', 'o', 'disposto', 'na', 'Porta', '##ria', '[UNK]', '76', '##24', ',', 'de', '29', 'de', 'setembro', 'de', '2016', 'R', '##ES', '##OL', '##V', '##E', 'Conc', '##eder', 'progress', '##ão', 'funcional', ',', 'por', 'avaliação', 'de', 'desempenho', ',', 'no', 'Quad', '##ro', 'desta', 'Universidade', ',', 'ao', 'Professor', 'J', '##OS', '##E', 'CA', '##R', '##LO', '##S', 'G', '##OM', '##ES', 'DO', '##S', 'AN', '##J', '##OS', ',', 'matr', '##ícula', 'S', '##IA', '##PE', 'n', '##°', '129', '##60', '##8', '##8', ',', 'lo', '##tado', 'e', 'em', 'exercício', 'no', 'Departamento', 'de', 'Soci', '##ologia', 'do', 'Instituto', 'de', 'Filosofia', 'e', 'Ciências', 'Human', '##as', ',', 'da', 'classe', 'D', 'de', 'Professor', 'Associa', '##do', ',', 'nível', '03', ',', 'para', 'a', 'classe', 'D', 'de', 'Professor', 'Associa', '##do', ',', 'nível', '04', ',', 'referente', 'ao', 'inter', '##st', '##ício', 'de', '18', '/', '12', '/', '2011', 'a', '17', '/', '12', '/', '2015', ',', 'com', 'vig', '##ência', 'financeira', 'a', 'partir', 'de', '01', '/', '08', '/', '2016', ',', 'de', 'acordo', 'com', 'o', 'que', 'dispõe', 'a', 'Lei', '[UNK]', '12', '.', '77', '##2', ',', 'de', '28', 'de', 'dezembro', 'de', '2012', ',', 'com', 'suas', 'alterações', 'e', 'a', 'Dec', '##isão', '[UNK]', '197', '/', '2006', '-', 'CON', '##S', '##UN', ',', 'alterada', 'pela', 'Dec', '##isão', '[UNK]', '40', '##1', '/', '2013', '-', 'CON', '##S', '##UN', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "## Label-Token Alignment Test\n",
    "idx = 6\n",
    "test_data = tokenizer(ufrgs_data['train'][idx]['inputs'], is_split_into_words=True)\n",
    "print(\"Tokenized Data:\", test_data)\n",
    "print(\"Word Tokens:\", ufrgs_data['train'][idx]['inputs'])\n",
    "test_labels = ufrgs_data['train'][idx]['targets']\n",
    "print(\"Word Labels:\", test_labels)\n",
    "print(\"Word IDs:\", test_data.word_ids())\n",
    "aligned_targets = align_targets(test_labels, test_data.word_ids())\n",
    "print(\"Sub-Word Labels:\", aligned_targets)\n",
    "print(\"Sub-Word Tokens:\", test_data.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "1\tO\n",
      "/\tO\n",
      "1\tO\n",
      "P\tO\n",
      "##OR\tO\n",
      "##TA\tO\n",
      "##RI\tO\n",
      "##A\tO\n",
      "[UNK]\tO\n",
      "38\tO\n",
      "##12\tO\n",
      "de\tO\n",
      "04\tO\n",
      "/\tO\n",
      "05\tO\n",
      "/\tO\n",
      "2017\tO\n",
      "A\tO\n",
      "VI\tO\n",
      "##CE\tO\n",
      "-\tO\n",
      "R\tO\n",
      "##EI\tO\n",
      "##TO\tO\n",
      "##RA\tO\n",
      "D\tO\n",
      "##A\tO\n",
      "UN\tB-ORGANIZATION\n",
      "##IV\tI-ORGANIZATION\n",
      "##ER\tI-ORGANIZATION\n",
      "##S\tI-ORGANIZATION\n",
      "##ID\tI-ORGANIZATION\n",
      "##AD\tI-ORGANIZATION\n",
      "##E\tI-ORGANIZATION\n",
      "F\tI-ORGANIZATION\n",
      "##ED\tI-ORGANIZATION\n",
      "##ER\tI-ORGANIZATION\n",
      "##AL\tI-ORGANIZATION\n",
      "DO\tI-ORGANIZATION\n",
      "R\tI-ORGANIZATION\n",
      "##IO\tI-ORGANIZATION\n",
      "G\tI-ORGANIZATION\n",
      "##RA\tI-ORGANIZATION\n",
      "##N\tI-ORGANIZATION\n",
      "##DE\tI-ORGANIZATION\n",
      "DO\tI-ORGANIZATION\n",
      "S\tI-ORGANIZATION\n",
      "##UL\tI-ORGANIZATION\n",
      ",\tO\n",
      "no\tO\n",
      "uso\tO\n",
      "de\tO\n",
      "suas\tO\n",
      "atribuições\tO\n",
      ",\tO\n",
      "considerando\tO\n",
      "o\tO\n",
      "disposto\tO\n",
      "na\tO\n",
      "Porta\tO\n",
      "##ria\tO\n",
      "[UNK]\tO\n",
      "76\tO\n",
      "##24\tO\n",
      ",\tO\n",
      "de\tO\n",
      "29\tO\n",
      "de\tO\n",
      "setembro\tO\n",
      "de\tO\n",
      "2016\tO\n",
      "R\tO\n",
      "##ES\tO\n",
      "##OL\tO\n",
      "##V\tO\n",
      "##E\tO\n",
      "Conc\tO\n",
      "##eder\tO\n",
      "progress\tO\n",
      "##ão\tO\n",
      "funcional\tO\n",
      ",\tO\n",
      "por\tO\n",
      "avaliação\tO\n",
      "de\tO\n",
      "desempenho\tO\n",
      ",\tO\n",
      "no\tO\n",
      "Quad\tO\n",
      "##ro\tO\n",
      "desta\tO\n",
      "Universidade\tO\n",
      ",\tO\n",
      "ao\tO\n",
      "Professor\tB-MISCELLANEOUS\n",
      "J\tB-PERSON\n",
      "##OS\tI-PERSON\n",
      "##E\tI-PERSON\n",
      "CA\tI-PERSON\n",
      "##R\tI-PERSON\n",
      "##LO\tI-PERSON\n",
      "##S\tI-PERSON\n",
      "G\tI-PERSON\n",
      "##OM\tI-PERSON\n",
      "##ES\tI-PERSON\n",
      "DO\tI-PERSON\n",
      "##S\tI-PERSON\n",
      "AN\tI-PERSON\n",
      "##J\tI-PERSON\n",
      "##OS\tI-PERSON\n",
      ",\tO\n",
      "matr\tO\n",
      "##ícula\tO\n",
      "S\tO\n",
      "##IA\tO\n",
      "##PE\tO\n",
      "n\tO\n",
      "##°\tO\n",
      "129\tO\n",
      "##60\tO\n",
      "##8\tO\n",
      "##8\tO\n",
      ",\tO\n",
      "lo\tO\n",
      "##tado\tO\n",
      "e\tO\n",
      "em\tO\n",
      "exercício\tO\n",
      "no\tO\n",
      "Departamento\tB-ORGANIZATION\n",
      "de\tI-ORGANIZATION\n",
      "Soci\tI-ORGANIZATION\n",
      "##ologia\tI-ORGANIZATION\n",
      "do\tO\n",
      "Instituto\tB-ORGANIZATION\n",
      "de\tI-ORGANIZATION\n",
      "Filosofia\tI-ORGANIZATION\n",
      "e\tI-ORGANIZATION\n",
      "Ciências\tI-ORGANIZATION\n",
      "Human\tI-ORGANIZATION\n",
      "##as\tI-ORGANIZATION\n",
      ",\tO\n",
      "da\tO\n",
      "classe\tO\n",
      "D\tO\n",
      "de\tO\n",
      "Professor\tB-MISCELLANEOUS\n",
      "Associa\tI-MISCELLANEOUS\n",
      "##do\tI-MISCELLANEOUS\n",
      ",\tO\n",
      "nível\tO\n",
      "03\tO\n",
      ",\tO\n",
      "para\tO\n",
      "a\tO\n",
      "classe\tO\n",
      "D\tO\n",
      "de\tO\n",
      "Professor\tB-MISCELLANEOUS\n",
      "Associa\tI-MISCELLANEOUS\n",
      "##do\tI-MISCELLANEOUS\n",
      ",\tO\n",
      "nível\tO\n",
      "04\tO\n",
      ",\tO\n",
      "referente\tO\n",
      "ao\tO\n",
      "inter\tO\n",
      "##st\tO\n",
      "##ício\tO\n",
      "de\tO\n",
      "18\tO\n",
      "/\tO\n",
      "12\tO\n",
      "/\tO\n",
      "2011\tO\n",
      "a\tO\n",
      "17\tO\n",
      "/\tO\n",
      "12\tO\n",
      "/\tO\n",
      "2015\tO\n",
      ",\tO\n",
      "com\tO\n",
      "vig\tO\n",
      "##ência\tO\n",
      "financeira\tO\n",
      "a\tO\n",
      "partir\tO\n",
      "de\tO\n",
      "01\tO\n",
      "/\tO\n",
      "08\tO\n",
      "/\tO\n",
      "2016\tO\n",
      ",\tO\n",
      "de\tO\n",
      "acordo\tO\n",
      "com\tO\n",
      "o\tO\n",
      "que\tO\n",
      "dispõe\tO\n",
      "a\tO\n",
      "Lei\tO\n",
      "[UNK]\tO\n",
      "12\tO\n",
      ".\tO\n",
      "77\tO\n",
      "##2\tO\n",
      ",\tO\n",
      "de\tO\n",
      "28\tO\n",
      "de\tO\n",
      "dezembro\tO\n",
      "de\tO\n",
      "2012\tO\n",
      ",\tO\n",
      "com\tO\n",
      "suas\tO\n",
      "alterações\tO\n",
      "e\tO\n",
      "a\tO\n",
      "Dec\tO\n",
      "##isão\tO\n",
      "[UNK]\tO\n",
      "197\tO\n",
      "/\tO\n",
      "2006\tO\n",
      "-\tO\n",
      "CON\tO\n",
      "##S\tO\n",
      "##UN\tO\n",
      ",\tO\n",
      "alterada\tO\n",
      "pela\tO\n",
      "Dec\tO\n",
      "##isão\tO\n",
      "[UNK]\tO\n",
      "40\tO\n",
      "##1\tO\n",
      "/\tO\n",
      "2013\tO\n",
      "-\tO\n",
      "CON\tO\n",
      "##S\tO\n",
      "##UN\tO\n",
      ".\tO\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = [target_labels[t] if t>=0 else None for t in aligned_targets]\n",
    "for x, y in zip(test_data.tokens(), aligned_labels):\n",
    "    print(f\"{x}\\t{y}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize inputs\n",
    "We take the sub-word inputs and labels and pass it to the 'tokenize_fn' function to generate the tokens we'll feed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize both inputs and targets\n",
    "def tokenize_fn(batch):\n",
    "    # Tokenize the input sequence first\n",
    "    tokenized_inputs = tokenizer(batch['inputs'], truncation=True, is_split_into_words=True)\n",
    "    labels_batch = batch['targets'] # The original targets word-by-word\n",
    "    aligned_labels_batch = [] # The aligned targets sub-word by sub-word\n",
    "    # Loop through each label sequence in the batch\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_ids = tokenized_inputs.word_ids(i) # Get word IDs for the sequence\n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids)) # Align sequence labels\n",
    "    \n",
    "    # Save final aligned labels in a column called 'labels' which is the required name for the hugging face models\n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'targets'],\n",
       "    num_rows: 38\n",
       "})"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufrgs_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cd81f375644bea9e3032cf2067d0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea9db79ded44a40864d72ea905c9f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the datasets method 'map' to apply the tokenize function to the train and test datasets.\n",
    "# We'll use the batched parameter to improve the eficiency of the tokenization.\n",
    "tokenized_datasets = ufrgs_data.map(\n",
    "\ttokenize_fn,\n",
    "\tbatched=True,\n",
    "\tremove_columns=ufrgs_data[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the DataCollator\n",
    "There are several required steps before passing the text into the model: padding, truncate, converting to tensors, etc. When we use the tokenizer method we are not doing most of these steps because the ‘data collator’ in the trainer is taking care of it implicitly when we train the model.\n",
    "\n",
    "The Data Collator is built into the trainer and is defined as such:\n",
    "\n",
    "> • **data_collator** (`DataCollator`, *optional*) — The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will default to **[default_data_collator()](https://huggingface.co/docs/transformers/v4.22.1/en/main_classes/data_collator#transformers.default_data_collator)** if no `tokenizer` is provided, an instance of **[DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.22.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding)** otherwise.\n",
    "> \n",
    "\n",
    "#### For Token Classification\n",
    "\n",
    "The Trainer object in Hugging Face is not capable of recognizing which task we are trying to execute and therefore is not able to automatically select the correct Data Collator for us. Since the default DataCollatorWithPadding does not support tasks of the token classification type, we’ll need to manually define the one we want by importing the token classification data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    3,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    7,    8,    8,    8,    8,\n",
       "            8,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    3,    4,    4,    4,    4,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    3,    4,    4,    4,    0,    5,\n",
       "            6,    6,    6,    6,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0, -100],\n",
       "        [-100,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    7,    8,    8,    8,    8,    8,    8,    8,    8,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the data collator. It should return both inputs as tensor of the same size (including the padding).\n",
    "collator_testset = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = data_collator(collator_testset)\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for validation\n",
    "Computing metrics isn’t as straight forward when we deal with multiple targets per sample(sentence). Usually when calculating accuracy with a single target we can do #correct_samples/#total_samples. This is not possible when each sample has several targets.\n",
    "One solution would be flattening all predictions to calculate #correct_targets/#total_targets.\n",
    "#### Seqeval\n",
    "This library is the standard method to calculate metrics in hugging face as its sole purpose is to compute metrics for NLP tasks with sequence targets.\\\n",
    "https://huggingface.co/spaces/evaluate-metric/seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seqeval will not work with a single input single label task. It will only be usable in tasks that require mulitple labels for multiple inputs\n",
    "# Single input single label example:\n",
    "#metric.compute(predictions=[0, 0, 0], references=[0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.8333333333333334}"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multilple inputs multiple labels example:\n",
    "metric.compute(\n",
    "    predictions=[[0, 0, 0], [1, 0, 1]], \n",
    "    references=[[0, 0, 1], [1, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 0.14285714285714285,\n",
       " 'overall_recall': 0.25,\n",
       " 'overall_f1': 0.18181818181818182,\n",
       " 'overall_accuracy': 0.5}"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seqeval supports sequence labeling evalutation with the IOB formats. So to get rid of the warning and correctly compute the metrics we need to follow this formatting standard.\n",
    "metric.compute(\n",
    "    predictions=[['O', 'I-ORG', 'B-ORG', 'B-ORG', 'B-LOC'], ['B-MISC', 'O', 'B-PER', 'I-PER', \"I-MISC\"]], \n",
    "    references=[['O', 'B-LOC', 'B-ORG', 'I-ORG', 'I-ORG'], ['B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'O']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # remove -100, convert the label ids to label names\n",
    "    str_labels = [[target_labels[t] for t in label if t != -100] for label in labels]\n",
    "\n",
    "    # do the same for predictions whenever true label is -100\n",
    "    str_preds = [[target_labels[p] for p, t in zip(pred, targ) if t != -100] for pred, targ in zip(preds, labels)]\n",
    "\n",
    "    the_metrics = metric.compute(predictions=str_preds, references=str_labels)\n",
    "    return {\n",
    "        'precision': the_metrics['overall_precision'],\n",
    "        'recall': the_metrics['overall_recall'],\n",
    "        'f1': the_metrics['overall_f1'],\n",
    "        'accuracy': the_metrics['overall_accuracy']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PERSON',\n",
       " 2: 'I-PERSON',\n",
       " 3: 'B-ORGANIZATION',\n",
       " 4: 'I-ORGANIZATION',\n",
       " 5: 'B-LOCATION',\n",
       " 6: 'I-LOCATION',\n",
       " 7: 'B-MISCELLANEOUS',\n",
       " 8: 'I-MISCELLANEOUS'}"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {k: v for k, v in enumerate(target_labels)} #Get label IDs\n",
    "label2id = {v: k for k, v in id2label.items()} #Get label names from IDs\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pre-trained model\n",
    "\n",
    "We use the AutoModelForTokenClassification.from_pretrained method to load the BERT model from huggingface.\\\n",
    "The model loades will be the one defined by 'checkpoint'.\\\n",
    "We pass the id2label and label2id parameters for the model to understand the targets we are using for the prediction and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PERSON\",\n",
      "    \"2\": \"I-PERSON\",\n",
      "    \"3\": \"B-ORGANIZATION\",\n",
      "    \"4\": \"I-ORGANIZATION\",\n",
      "    \"5\": \"B-LOCATION\",\n",
      "    \"6\": \"I-LOCATION\",\n",
      "    \"7\": \"B-MISCELLANEOUS\",\n",
      "    \"8\": \"I-MISCELLANEOUS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOCATION\": 5,\n",
      "    \"B-MISCELLANEOUS\": 7,\n",
      "    \"B-ORGANIZATION\": 3,\n",
      "    \"B-PERSON\": 1,\n",
      "    \"I-LOCATION\": 6,\n",
      "    \"I-MISCELLANEOUS\": 8,\n",
      "    \"I-ORGANIZATION\": 4,\n",
      "    \"I-PERSON\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\arthu/.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased\\snapshots\\94d69c95f98f7d5b2a8700c420230ae10def0baa\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Training Arguments\n",
    "The training arguments define several parameters of how the training of the model will happen. Some argument define where the outputs will be save, how often during training we want to compute metric, how many epochs we will use for training, define the learning rate and many others. There are a several arguments which can all be found in the documentation of the function:\n",
    "\n",
    "[https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "\n",
    "The trainer uses AdamW for the backpropagation optimization. We can use a custom one if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"bert-base-portuguese-cased-ner-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "The trainer object is what we’ll use to run the training process. The arguments are fairly simple:\n",
    "\n",
    "- The pre-trained model we will use\n",
    "- The training arguments\n",
    "- The train dataset (already tokenized)\n",
    "- The validation dataset\n",
    "- The data collator\n",
    "- The metrics we will use for validation\n",
    "- The tokenizer\n",
    "\n",
    "The training is done by calling the train method as trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "GPU memory occupied: 8076 MB.\n",
      "Mon Apr  3 04:31:11 2023       "
     ]
    }
   ],
   "source": [
    "print(trainer.args.device)\n",
    "print_gpu_utilization()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 38\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25\n",
      "  Number of trainable parameters = 108339465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.02       Driver Version: 528.02       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   37C    P5    14W / 180W |   7957MiB /  8192MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2900    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      3396    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n",
      "|    0   N/A  N/A      4428    C+G   ...\\app-1.0.9011\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      4472    C+G   ...\\app-1.0.9011\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      5908    C+G   ...3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10652    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     12164    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12316    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     12400    C+G   ... Download Manager\\fdm.exe    N/A      |\n",
      "|    0   N/A  N/A     12700    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     13112    C+G   ...werToys.PowerLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A     13352    C+G   ...ray\\lghub_system_tray.exe    N/A      |\n",
      "|    0   N/A  N/A     14260    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15748    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     16356    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     18336    C+G   ...aming\\Spotify\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     18476    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18684    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     19476    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     19768    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     20296    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     22372      C   ...da3\\envs\\NLP22\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     25012    C+G   ...661.54\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     26012    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55b9b399e4646f981b2d2d0e018cf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6bcabaa2e640709efc383984a6a221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\n",
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4058964252471924, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.7182601880877743, 'eval_runtime': 0.6931, 'eval_samples_per_second': 27.411, 'eval_steps_per_second': 4.328, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-5\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e551386a7a844125bde98c040b975935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\n",
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9357545971870422, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.7394200626959248, 'eval_runtime': 0.6373, 'eval_samples_per_second': 29.812, 'eval_steps_per_second': 4.707, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-10\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4305b2f6b25e40cdb6b7556c45eee909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\n",
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7065246105194092, 'eval_precision': 0.1, 'eval_recall': 0.04597701149425287, 'eval_f1': 0.06299212598425197, 'eval_accuracy': 0.832680250783699, 'eval_runtime': 0.7271, 'eval_samples_per_second': 26.133, 'eval_steps_per_second': 4.126, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-15\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1d7cb6e92445c1977dbf97c9837b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5992717146873474, 'eval_precision': 0.8285714285714286, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.47540983606557374, 'eval_accuracy': 0.850705329153605, 'eval_runtime': 0.6647, 'eval_samples_per_second': 28.584, 'eval_steps_per_second': 4.513, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\\config.json\n",
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-20\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23361e7cc0254ed4b62da7efa3dc51ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\envs\\NLP22\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\n",
      "Configuration saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5647721290588379, 'eval_precision': 0.7111111111111111, 'eval_recall': 0.367816091954023, 'eval_f1': 0.48484848484848486, 'eval_accuracy': 0.8557993730407524, 'eval_runtime': 0.6702, 'eval_samples_per_second': 28.351, 'eval_steps_per_second': 4.476, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-portuguese-cased-ner-finetuned\\checkpoint-25\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 131.4418, 'train_samples_per_second': 1.446, 'train_steps_per_second': 0.19, 'train_loss': 1.0373165130615234, 'epoch': 5.0}\n",
      "Time: 131.44\n",
      "Samples/second: 1.45\n",
      "GPU memory occupied: 8079 MB.\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 8082 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainer.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to auto_tagger\n",
      "Configuration saved in auto_tagger\\config.json\n",
      "Model weights saved in auto_tagger\\pytorch_model.bin\n",
      "tokenizer config file saved in auto_tagger\\tokenizer_config.json\n",
      "Special tokens file saved in auto_tagger\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('auto_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file auto_tagger\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"auto_tagger\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PERSON\",\n",
      "    \"2\": \"I-PERSON\",\n",
      "    \"3\": \"B-ORGANIZATION\",\n",
      "    \"4\": \"I-ORGANIZATION\",\n",
      "    \"5\": \"B-LOCATION\",\n",
      "    \"6\": \"I-LOCATION\",\n",
      "    \"7\": \"B-MISCELLANEOUS\",\n",
      "    \"8\": \"I-MISCELLANEOUS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOCATION\": 5,\n",
      "    \"B-MISCELLANEOUS\": 7,\n",
      "    \"B-ORGANIZATION\": 3,\n",
      "    \"B-PERSON\": 1,\n",
      "    \"I-LOCATION\": 6,\n",
      "    \"I-MISCELLANEOUS\": 8,\n",
      "    \"I-ORGANIZATION\": 4,\n",
      "    \"I-PERSON\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading configuration file auto_tagger\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"auto_tagger\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PERSON\",\n",
      "    \"2\": \"I-PERSON\",\n",
      "    \"3\": \"B-ORGANIZATION\",\n",
      "    \"4\": \"I-ORGANIZATION\",\n",
      "    \"5\": \"B-LOCATION\",\n",
      "    \"6\": \"I-LOCATION\",\n",
      "    \"7\": \"B-MISCELLANEOUS\",\n",
      "    \"8\": \"I-MISCELLANEOUS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOCATION\": 5,\n",
      "    \"B-MISCELLANEOUS\": 7,\n",
      "    \"B-ORGANIZATION\": 3,\n",
      "    \"B-PERSON\": 1,\n",
      "    \"I-LOCATION\": 6,\n",
      "    \"I-MISCELLANEOUS\": 8,\n",
      "    \"I-ORGANIZATION\": 4,\n",
      "    \"I-PERSON\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading weights file auto_tagger\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at auto_tagger.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\n",
    "    \"token-classification\",\n",
    "    model='auto_tagger',\n",
    "    aggregation_strategy=\"average\",\n",
    "    ignore_labels=[\"\"],\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento gerado sob autenticação Nº NIK.843.557.VM6, disponível no endereço http://www.ufrgs.br/autenticacao 1/1 PORTARIA Nº 1181 de 18/02/2016 O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no uso de suas atribuições que lhe foram conferidas pela Portaria nº 5469, de 04 de outubro de 2012, do Magnífico Reitor, e conforme o Laudo Médico n°37564, RESOLVE: Designar, temporariamente, nos termos da Lei nº 8.112, de 11 de dezembro de 1990, com redação dada pela Lei nº 9.527, de 10 de dezembro de 1997, a ocupante do cargo de ASSISTENTE EM ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade, DENISE SCHROEDER (Siape: 0358763 ), para substituir MARILDA SANTOS DA ROCHA (Siape: 1044125 ), Secretário do Depto de Plantas Forrageiras e Agrometeorologia da Faculdade de Agronomia, Código FG-7, em seu afastamento por motivo de Laudo Médico do titular da Função, no período de 03/02/2016 a 10/02/2016, com o decorrente pagamento das vantagens por 8 dias. MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas\n",
      "['[CLS]', 'Docu', '##mento', 'gerado', 'sob', 'autent', '##icação', '[UNK]', 'N', '##I', '##K', '.', '84', '##3', '.', '55', '##7', '.', 'V', '##M', '##6', ',', 'disponível', 'no', 'endereço', 'http', ':', '/', '/', 'w', '##ww', '.', 'u', '##fr', '##gs', '.', 'b', '##r', '/', 'autent', '##ica', '##ca', '##o', '1', '/', '1', 'P', '##OR', '##TA', '##RI', '##A', '[UNK]', '118', '##1', 'de', '18', '/', '02', '/', '2016', 'O', 'PR', '##Ó', '-', 'R', '##EI', '##TO', '##R', 'DE', 'G', '##ES', '##T', '##ÃO', 'DE', 'P', '##ES', '##SO', '##AS', 'D', '##A', 'UN', '##IV', '##ER', '##S', '##ID', '##AD', '##E', 'F', '##ED', '##ER', '##AL', 'DO', 'R', '##IO', 'G', '##RA', '##N', '##DE', 'DO', 'S', '##UL', ',', 'no', 'uso', 'de', 'suas', 'atribuições', 'que', 'lhe', 'foram', 'confer', '##idas', 'pela', 'Porta', '##ria', '[UNK]', '54', '##69', ',', 'de', '04', 'de', 'outubro', 'de', '2012', ',', 'do', 'Magn', '##ífico', 'Rei', '##tor', ',', 'e', 'conforme', 'o', 'Lau', '##do', 'Médico', 'n', '##°', '##37', '##56', '##4', ',', 'R', '##ES', '##OL', '##V', '##E', ':', 'Design', '##ar', ',', 'temporariamente', ',', 'nos', 'termos', 'da', 'Lei', '[UNK]', '8', '.', '112', ',', 'de', '11', 'de', 'dezembro', 'de', '1990', ',', 'com', 'redação', 'dada', 'pela', 'Lei', '[UNK]', '9', '.', '52', '##7', ',', 'de', '10', 'de', 'dezembro', 'de', '1997', ',', 'a', 'ocupa', '##nte', 'do', 'cargo', 'de', 'AS', '##S', '##IS', '##TE', '##NT', '##E', 'E', '##M', 'A', '##D', '##MI', '##N', '##IS', '##T', '##RA', '##Ç', '##ÃO', ',', 'do', 'Quad', '##ro', 'de', 'Pessoa', '##l', 'desta', 'Universidade', ',', 'DE', '##N', '##IS', '##E', 'SC', '##H', '##RO', '##ED', '##ER', '(', 'Si', '##ap', '##e', ':', '03', '##58', '##7', '##6', '##3', ')', ',', 'para', 'substituir', 'MA', '##RI', '##L', '##DA', 'SA', '##NT', '##OS', 'D', '##A', 'R', '##OC', '##HA', '(', 'Si', '##ap', '##e', ':', '10', '##44', '##12', '##5', ')', ',', 'Secretário', 'do', 'De', '##pto', 'de', 'Plan', '##tas', 'For', '##rag', '##eiras', 'e', 'Agro', '##met', '##e', '##oro', '##logia', 'da', 'Faculdade', 'de', 'Agro', '##nomia', ',', 'Código', 'F', '##G', '-', '7', ',', 'em', 'seu', 'afastamento', 'por', 'motivo', 'de', 'Lau', '##do', 'Médico', 'do', 'titular', 'da', 'Fun', '##ção', ',', 'no', 'período', 'de', '03', '/', '02', '/', '2016', 'a', '10', '/', '02', '/', '2016', ',', 'com', 'o', 'decorre', '##nte', 'pagamento', 'das', 'vantagens', 'por', '8', 'dias', '.', 'MA', '##UR', '##Í', '##CI', '##O', 'VI', '##É', '##GA', '##S', 'D', '##A', 'S', '##IL', '##VA', 'Pr', '##ó', '-', 'Rei', '##tor', 'de', 'Gestão', 'de', 'Pessoas', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "test_sample = \"\"\"Documento gerado sob autenticação Nº NIK.843.557.VM6, disponível no endereço http://www.ufrgs.br/autenticacao\n",
    "1/1\n",
    "PORTARIA Nº             1181                  de  18/02/2016\n",
    "O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no\n",
    "uso de suas atribuições que lhe foram conferidas pela Portaria nº.5469, de 04 de outubro de 2012, do\n",
    "Magnífico Reitor, e conforme o Laudo Médico n°37564,\n",
    "RESOLVE:\n",
    "Designar, temporariamente, nos termos da Lei nº. 8.112, de 11 de dezembro de 1990, com redação\n",
    "dada  pela  Lei  nº.9.527,  de  10  de  dezembro  de  1997,  a  ocupante  do  cargo  de  ASSISTENTE  EM\n",
    "ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade, DENISE SCHROEDER (Siape: 0358763 ),  para\n",
    "substituir   MARILDA SANTOS DA ROCHA (Siape: 1044125 ), Secretário do Depto de Plantas Forrageiras e\n",
    "Agrometeorologia da Faculdade de Agronomia, Código FG-7, em seu afastamento por motivo de Laudo\n",
    "Médico do titular da Função, no período de 03/02/2016 a 10/02/2016, com o decorrente pagamento das\n",
    "vantagens por 8 dias.\n",
    "MAURÍCIO VIÉGAS DA SILVA\n",
    "Pró-Reitor de Gestão de Pessoas\"\"\" \n",
    "clean_sample = data_processing.clear_text(test_sample)\n",
    "print(clean_sample)\n",
    "token_sample = tokenizer(clean_sample)\n",
    "print(token_sample.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.token_classification.TokenClassificationPipeline"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'O',\n",
       "  'score': 0.82746357,\n",
       "  'word': 'Documento gerado sob autenticação Nº NIK. 843. 557. VM6, disponível no endereço http : / / www. ufrgs. br / autenticacao 1 / 1 PORTARIA Nº 1181 de 18 / 02 / 2016 O PRÓ - REITOR DE',\n",
       "  'start': 0,\n",
       "  'end': 160},\n",
       " {'entity_group': 'MISCELLANEOUS',\n",
       "  'score': 0.3277286,\n",
       "  'word': 'GESTÃO',\n",
       "  'start': 161,\n",
       "  'end': 167},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.4662776,\n",
       "  'word': 'DE PESSOAS DA',\n",
       "  'start': 168,\n",
       "  'end': 181},\n",
       " {'entity_group': 'ORGANIZATION',\n",
       "  'score': 0.669568,\n",
       "  'word': 'UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL',\n",
       "  'start': 182,\n",
       "  'end': 223},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.8813147,\n",
       "  'word': ', no uso de suas atribuições que lhe foram conferidas pela Portaria nº 5469, de 04 de outubro de 2012, do Magnífico Reitor, e conforme o Laudo Médico n°37564, RESOLVE : Designar, temporariamente, nos termos da Lei nº 8. 112, de 11 de dezembro de 1990, com redação dada pela Lei nº 9. 527, de 10 de dezembro de 1997, a ocupante do cargo de ASSISTENTE EM ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade,',\n",
       "  'start': 223,\n",
       "  'end': 628},\n",
       " {'entity_group': 'PERSON',\n",
       "  'score': 0.53349257,\n",
       "  'word': 'DENISE SCHROEDER',\n",
       "  'start': 629,\n",
       "  'end': 645},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.9132752,\n",
       "  'word': '( Siape : 0358763 ), para substituir',\n",
       "  'start': 646,\n",
       "  'end': 680},\n",
       " {'entity_group': 'PERSON',\n",
       "  'score': 0.5948445,\n",
       "  'word': 'MARILDA SANTOS DA ROCHA',\n",
       "  'start': 681,\n",
       "  'end': 704},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.82912296,\n",
       "  'word': '( Siape : 1044125 ), Secretário do Depto de Plantas Forrageiras e Agrometeorologia da Faculdade de Agronomia, Código FG - 7, em seu afastamento por motivo de Laudo Médico do titular da Função, no período de 03 / 02 / 2016 a 10 / 02 / 2016, com o decorrente pagamento das vantagens por 8 dias.',\n",
       "  'start': 705,\n",
       "  'end': 985},\n",
       " {'entity_group': 'PERSON',\n",
       "  'score': 0.63545716,\n",
       "  'word': 'MAURÍCIO VIÉGAS DA SILVA',\n",
       "  'start': 986,\n",
       "  'end': 1010},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.5815567,\n",
       "  'word': 'Pró - Reitor de Gestão de Pessoas',\n",
       "  'start': 1011,\n",
       "  'end': 1042}]"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ner(clean_sample)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_sentence(original_sentence, tagged_results):\n",
    "    og_sentence_pos = 0\n",
    "    tagged_sentence_pos = 0\n",
    "    formatted_results = []\n",
    "    for entity in tagged_results:\n",
    "        formatted_entity = \"\"\n",
    "        #print(entity['word'])\n",
    "        for index_char, char in enumerate(entity['word']):\n",
    "            #print(char, original_sentence[og_sentence_pos])\n",
    "            if char == original_sentence[og_sentence_pos]:\n",
    "                og_sentence_pos += 1\n",
    "                formatted_entity = formatted_entity + char\n",
    "            else:\n",
    "                #print(\"Bad char:\", char)\n",
    "                # #Look ahead and see if next char is equal to original\n",
    "                if entity['word'][index_char+1] == original_sentence[og_sentence_pos]:\n",
    "                    #print(\"Found char in next pos\")\n",
    "                    pass\n",
    "                # Else add the character to the formatted entity\n",
    "                else:\n",
    "                    og_sentence_pos += 2\n",
    "                    formatted_entity = formatted_entity + char\n",
    "\n",
    "        #print(\"Formatted:\", formatted_entity)\n",
    "        formatted_result = entity.copy()\n",
    "        formatted_result['word'] = formatted_entity\n",
    "        formatted_results.append(formatted_result)\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_sentence = reformat_sentence(clean_sample, results)\n",
    "sentence_for_relabelling = \"\"\n",
    "for entity in reformatted_sentence:\n",
    "    sentence_for_relabelling = sentence_for_relabelling + entity['word'] + ' '\n",
    "sentence_for_relabelling = re.sub(' ,', ',', sentence_for_relabelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento gerado sob autenticação Nº NIK.843.557.VM6, disponível no endereço http://www.ufrgs.br/autenticacao 1/1 PORTARIA Nº 1181 de 18/02/2016 O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no uso de suas atribuições que lhe foram conferidas pela Portaria nº 5469, de 04 de outubro de 2012, do Magnífico Reitor, e conforme o Laudo Médico n°37564, RESOLVE: Designar, temporariamente, nos termos da Lei nº 8.112, de 11 de dezembro de 1990, com redação dada pela Lei nº 9.527, de 10 de dezembro de 1997, a ocupante do cargo de ASSISTENTE EM ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade, DENISE SCHROEDER  Siape: 0358763 ), para substituir MARILDA SANTOS DA ROCHA  Siape: 1044125 ), Secretário do Depto de Plantas Forrageiras e Agrometeorologia da Faculdade de Agronomia, Código FG-7, em seu afastamento por motivo de Laudo Médico do titular da Função, no período de 03/02/2016 a 10/02/2016, com o decorrente pagamento das vantagens por 8 dias. MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas \n",
      "Documento gerado sob autenticação Nº NIK.843.557.VM6, disponível no endereço http://www.ufrgs.br/autenticacao 1/1 PORTARIA Nº 1181 de 18/02/2016 O PRÓ-REITOR DE GESTÃO DE PESSOAS DA UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL, no uso de suas atribuições que lhe foram conferidas pela Portaria nº 5469, de 04 de outubro de 2012, do Magnífico Reitor, e conforme o Laudo Médico n°37564, RESOLVE: Designar, temporariamente, nos termos da Lei nº 8.112, de 11 de dezembro de 1990, com redação dada pela Lei nº 9.527, de 10 de dezembro de 1997, a ocupante do cargo de ASSISTENTE EM ADMINISTRAÇÃO, do Quadro de Pessoal desta Universidade, DENISE SCHROEDER (Siape: 0358763 ), para substituir MARILDA SANTOS DA ROCHA (Siape: 1044125 ), Secretário do Depto de Plantas Forrageiras e Agrometeorologia da Faculdade de Agronomia, Código FG-7, em seu afastamento por motivo de Laudo Médico do titular da Função, no período de 03/02/2016 a 10/02/2016, com o decorrente pagamento das vantagens por 8 dias. MAURÍCIO VIÉGAS DA SILVA Pró-Reitor de Gestão de Pessoas\n"
     ]
    }
   ],
   "source": [
    "print(sentence_for_relabelling)\n",
    "print(clean_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sentence(tagged_sentence):\n",
    "    list_of_words = []\n",
    "    for token in tokens:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('NLP22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "228764ee21566ea5d9101d690a7d226c86c8b64251894d53614cd4c162a99691"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
