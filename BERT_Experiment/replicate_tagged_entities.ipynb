{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate Tagged Entities\n",
    "We'll take a dataset that was partially annotated in doccano and fill the rest of the documents in the dataset with the entities that were already tagged.\\\n",
    "Steps:\n",
    "1. Load the partially tagged dataset\n",
    "2. Create a dictionary of the tagged entities\n",
    "3. Loop through the unannotated documents searching for possible entities in the dictionary.\n",
    "4. Save the resulting dataset and load it in Doccano to finish manual tagging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_test = \"acerpi_dataset/test/\"\n",
    "path_to_train = \"acerpi_dataset/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = pd.read_json(os.path.join(path_to_train, 'annotated_10_percent.jsonl'), orient='record', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = ner_dataset.iloc[0:80].copy()\n",
    "untagged_sentences = ner_dataset.iloc[80:].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create dictionary of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_entities = {}\n",
    "\n",
    "for index, row in tagged_sentences.iterrows():\n",
    "    for entity in row['label']:\n",
    "        start_pos = int(entity[0])\n",
    "        end_pos = int(entity[1])\n",
    "        label = entity[2]\n",
    "        entity_text = row['text'][start_pos:end_pos]\n",
    "        known_entities[entity_text] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_entities(sentence, dictionary):\n",
    "    matches = []\n",
    "    for entity in dictionary:\n",
    "        index = sentence.find(entity)\n",
    "        if index != -1:\n",
    "            matches.append([index, index + len(entity), dictionary[entity]])\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in untagged_sentences.iterrows():\n",
    "    row['label'].extend(find_matching_entities(row['text'], known_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 7) + (649, 7) = (729, 7)\n"
     ]
    }
   ],
   "source": [
    "replicated_data = pd.concat([tagged_sentences, untagged_sentences])\n",
    "print(\n",
    "    tagged_sentences.shape,\n",
    "    '+', untagged_sentences.shape,\n",
    "    '=', replicated_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicated_data.to_json(os.path.join(path_to_train, 'unique_sentences_replicated.jsonl'),lines=True, orient = 'records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
